{
  "module": {
    "id": "07",
    "title": "Principal Component Analysis",
    "subtitle": "CMSC 173 - Machine Learning",
    "course": "CMSC 173",
    "institution": "University of the Philippines - Cebu",
    "totalSlides": 41,
    "estimatedDuration": "82 minutes"
  },
  "slides": [
    {
      "id": 1,
      "title": "Outline",
      "readingTime": "1 min",
      "content": "\\tableofcontents",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 2,
      "title": "What is Principal Component Analysis?",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4><strong>Principal Component Analysis (PCA)</strong> is a statistical technique that transforms high-dimensional data into a lower-dimensional representation while preserving as much variance as possible.</div>\n\n\n\n\\begin{exampleblock}{Key Characteristics}\n<ul>\n\n<li>Unsupervised learning method\n</li>\n<li>Linear dimensionality reduction\n</li>\n<li>Orthogonal transformation\n</li>\n<li>Variance maximization\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n\n<div class=\"column\">\n\n\n<div class=\"figure\"><p><em>[Figure: ../figures/dimensionality_reduction_concept.png]</em></p></div>\n\n\n\n<strong>Applications:</strong>\n<ul>\n\n<li>Data visualization\n</li>\n<li>Feature extraction\n</li>\n<li>Noise reduction\n</li>\n<li>Image compression\n</li>\n</ul>\n</div>\n</div>\n\n\n\n<div class=\"warning\"><h4>Goal</h4>Find new axes (principal components) that maximize variance and are uncorrelated with each other.</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 3,
      "title": "The Curse of Dimensionality",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Problem</h4>As the number of features increases, data becomes increasingly sparse, making analysis difficult and computationally expensive.</div>\n\n\n\n<strong>Challenges in High Dimensions:</strong>\n<ul>\n\n<li><strong>Data sparsity:</strong> Points become isolated\n</li>\n<li><strong>Distance measures:</strong> Lose meaning\n</li>\n<li><strong>Overfitting:</strong> Models fit noise\n</li>\n<li><strong>Computation:</strong> Time/memory grows exponentially\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Hughes Phenomenon</h4>Model performance initially improves with more features, then degrades beyond an optimal point.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/curse_of_dimensionality.png]</em></p></div>\n\n\n\n<strong>Example Impact:</strong>\n<ul>\n\n<li>10 features: $10^2 = 100$ parameter pairs\n</li>\n<li>100 features: $100^2 = 10,000$ pairs\n</li>\n<li>1000 features: $1,000,000$ pairs!\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Solution: PCA}\nReduce dimensions while retaining information.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 4,
      "title": "Why Dimensionality Reduction?",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Motivation for PCA:</strong>\n\n\n<ul>\n\n<li><strong>Visualization:</strong> Reduce to 2D/3D for plotting\n</li>\n<li><strong>Storage:</strong> Compress data efficiently\n</li>\n<li><strong>Speed:</strong> Faster training and inference\n</li>\n<li><strong>Noise removal:</strong> Filter out low-variance components\n</li>\n<li><strong>Feature extraction:</strong> Create meaningful features\n</li>\n<li><strong>Collinearity:</strong> Remove redundant features\n</li>\n</ul>\n\n\n\n<div class=\"highlight\"><h4>Core Principle</h4>Most real-world data has intrinsic dimensionality much lower than its ambient dimensionality.</div>\n\n\n\n<strong>Example:</strong> Images of faces\n<ul>\n\n<li>Ambient: 10,000 pixels\n</li>\n<li>Intrinsic: 100-200 dimensions\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Benefits vs Trade-offs:</strong>\n\n\n\\begin{exampleblock}{Advantages}\n<ul>\n\n<li>Reduced computational cost\n</li>\n<li>Easier visualization\n</li>\n<li>Noise reduction\n</li>\n<li>Better generalization\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<div class=\"warning\"><h4>Trade-offs</h4><ul>\n\n<li>Information loss\n</li>\n<li>Interpretability reduced\n</li>\n<li>Linear assumptions\n</li>\n<li>Sensitive to scaling\n</li>\n</ul></div>\n\n\n\n<div class=\"definition\"><h4>When to Use PCA</h4><ul>\n\n<li>Many correlated features\n</li>\n<li>Need for visualization\n</li>\n<li>Computational constraints\n</li>\n<li>Preprocessing for ML\n</li>\n</ul></div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 5,
      "title": "Real-World Applications",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Computer Vision:</strong>\n<ul>\n\n<li><strong>Face recognition:</strong> Eigenfaces\n</li>\n<li><strong>Image compression:</strong> Reduce storage\n</li>\n<li><strong>Object detection:</strong> Feature extraction\n</li>\n<li><strong>Image denoising:</strong> Remove noise\n</li>\n</ul>\n\n\n\n<strong>Finance:</strong>\n<ul>\n\n<li>Portfolio optimization\n</li>\n<li>Risk assessment\n</li>\n<li>Market trend analysis\n</li>\n<li>Fraud detection\n</li>\n</ul>\n\n\n\n<strong>Biology \\& Medicine:</strong>\n<ul>\n\n<li>Gene expression analysis\n</li>\n<li>Medical imaging\n</li>\n<li>Drug discovery\n</li>\n<li>Disease classification\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Natural Language Processing:</strong>\n<ul>\n\n<li>Document clustering\n</li>\n<li>Topic modeling\n</li>\n<li>Sentiment analysis\n</li>\n<li>Information retrieval\n</li>\n</ul>\n\n\n\n<strong>Signal Processing:</strong>\n<ul>\n\n<li>Audio compression\n</li>\n<li>Speech recognition\n</li>\n<li>Sensor data analysis\n</li>\n<li>Anomaly detection\n</li>\n</ul>\n\n\n\n<strong>Recommendation Systems:</strong>\n<ul>\n\n<li>User preference modeling\n</li>\n<li>Content filtering\n</li>\n<li>Collaborative filtering\n</li>\n<li>Feature engineering\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Success Story}\nNetflix Prize: Teams used PCA for feature reduction and achieved significant performance gains.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 6,
      "title": "Variance: Measuring Spread",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4><strong>Variance</strong> measures how far data points spread from their mean value.</div>\n\n\n\n<strong>For a single variable:</strong>\n$$\\begin{aligned}\\text{Var}(X) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\\\ = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\end{aligned}$$\n\n\n\n<strong>Properties:</strong>\n<ul>\n\n<li>Always non-negative: $\\text{Var}(X) \\geq 0$\n</li>\n<li>Zero variance: constant values\n</li>\n<li>Units: squared original units\n</li>\n<li><strong>Standard deviation:</strong> $\\sigma = \\sqrt{\\text{Var}(X)}$\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>PCA Goal</h4>Find directions of <strong>maximum variance</strong> in the data.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/variance_visualization.png]</em></p></div>\n\n\n\n<strong>Example Calculation:</strong>\n\nData: $[2, 4, 6, 8, 10]$\n$$\\begin{aligned}\\bar{x} = \\frac{2+4+6+8+10}{5} = 6 \\\\ \\text{Var}(X) = \\frac{(2-6)^2 + (4-6)^2 + ⋯}{5} \\\\ = \\frac{16 + 4 + 0 + 4 + 16}{5} = 8\\end{aligned}$$\n\n\n\n\\begin{exampleblock}{Intuition}\nHigh variance direction contains more information than low variance direction.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 7,
      "title": "Covariance: Measuring Linear Relationship",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4><strong>Covariance</strong> measures the joint variability of two variables.</div>\n\n\n\n<strong>Formula:</strong>\n$$\\begin{aligned}\\text{Cov}(X, Y) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y}) \\\\ = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\end{aligned}$$\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>$\\text{Cov}(X, Y) > 0$: Positive relationship\n</li>\n<li>$\\text{Cov}(X, Y) < 0$: Negative relationship\n</li>\n<li>$\\text{Cov}(X, Y) = 0$: No linear relationship\n</li>\n<li>$\\text{Cov}(X, X) = \\text{Var}(X)$\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Correlation}\nNormalized covariance:\n$$\\rho_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\in [-1, 1]$$\n\\end{exampleblock}\n</div>\n\n<div class=\"column\">\n<strong>Covariance Matrix:</strong>\n\nFor data $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$:\n$$\\mathbf{\\Sigma} = \\frac{1}{n}\\mathbf{X}^T\\mathbf{X} \\in \\mathbb{R}^{d \\times d}$$\n\n\n\n<strong>Structure:</strong>\n$$\\mathbf{\\Sigma} = \\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1,X_2) & ⋯ \\\\\n\\text{Cov}(X_2,X_1) & \\text{Var}(X_2) & ⋯ \\\\\n⋮ & ⋮ & \\ddots\n\\end{bmatrix}$$\n\n\n\n<strong>Properties:</strong>\n<ul>\n\n<li>Symmetric: $\\Sigma_{ij} = \\Sigma_{ji}$\n</li>\n<li>Positive semi-definite\n</li>\n<li>Diagonal: variances\n</li>\n<li>Off-diagonal: covariances\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>PCA Key</h4>Diagonalize covariance matrix to find uncorrelated components.</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 8,
      "title": "Eigendecomposition: The Core of PCA",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4>For a square matrix $\\mathbf{A}$, <strong>eigendecomposition</strong> finds vectors and scalars such that:\n$$\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$$</div>\n\n\n\n<strong>Components:</strong>\n<ul>\n\n<li>$\\mathbf{v}$: <strong>Eigenvector</strong> (direction)\n</li>\n<li>$\\lambda$: <strong>Eigenvalue</strong> (scaling factor)\n</li>\n<li>Matrix only changes <em>magnitude</em>, not direction\n</li>\n</ul>\n\n\n\n<strong>For Covariance Matrix:</strong>\n$$\\mathbf{\\Sigma} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$$\n\nwhere:\n<ul>\n\n<li>$\\mathbf{V}$: Eigenvector matrix\n</li>\n<li>$\\mathbf{\\Lambda}$: Diagonal eigenvalue matrix\n</li>\n<li>$\\mathbf{V}^T\\mathbf{V} = \\mathbf{I}$ (orthonormal)\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/covariance_eigenvectors.png]</em></p></div>\n\n\n\n<strong>Geometric Interpretation:</strong>\n<ul>\n\n<li>Eigenvectors: Principal component directions\n</li>\n<li>Eigenvalues: Variance along each PC\n</li>\n<li>Largest eigenvalue: Most variance\n</li>\n<li>Smallest eigenvalue: Least variance\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{PCA Connection}\n<ul>\n\n<li>PC directions = Eigenvectors\n</li>\n<li>PC importance = Eigenvalues\n</li>\n<li>Sorted by decreasing eigenvalue\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 9,
      "title": "Eigendecomposition Example",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Problem:</strong> Find eigenvalues and eigenvectors\n\n$$\\mathbf{\\Sigma} = \\begin{bmatrix} 4 & 2 \\\\ 2 & 3 \\end{bmatrix}$$\n\n\n\n<strong>Step 1: Characteristic equation</strong>\n$$\\det(\\mathbf{\\Sigma} - \\lambda \\mathbf{I}) = 0$$\n\n$$\\det\\begin{bmatrix} 4-\\lambda & 2 \\\\ 2 & 3-\\lambda \\end{bmatrix} = 0$$\n\n$$(4-\\lambda)(3-\\lambda) - 4 = 0$$\n$$\\lambda^2 - 7\\lambda + 8 = 0$$\n\n\n\n<strong>Step 2: Solve for eigenvalues</strong>\n$$\\lambda_1 = 5.56,   \\lambda_2 = 1.44$$\n</div>\n\n<div class=\"column\">\n<strong>Step 3: Find eigenvectors</strong>\n\nFor $\\lambda_1 = 5.56$:\n$$(\\mathbf{\\Sigma} - 5.56\\mathbf{I})\\mathbf{v}_1 = 0$$\n$$\\mathbf{v}_1 = \\begin{bmatrix} 0.79 \\\\ 0.61 \\end{bmatrix}$$\n\nFor $\\lambda_2 = 1.44$:\n$$\\mathbf{v}_2 = \\begin{bmatrix} -0.61 \\\\ 0.79 \\end{bmatrix}$$\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>PC1 direction: $(0.79, 0.61)$\n</li>\n<li>PC1 variance: $5.56$\n</li>\n<li>PC2 direction: $(-0.61, 0.79)$\n</li>\n<li>PC2 variance: $1.44$\n</li>\n<li>Total variance: $5.56 + 1.44 = 7$\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Note</h4>Eigenvectors are orthogonal: $\\mathbf{v}_1 \\perp \\mathbf{v}_2$</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 10,
      "title": "Singular Value Decomposition (SVD)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4>Any matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ can be decomposed as:\n$$\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$</div>\n\n\n\n<strong>Components:</strong>\n<ul>\n\n<li>$\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$: Left singular vectors\n</li>\n<li>$\\mathbf{\\Sigma} \\in \\mathbb{R}^{n \\times d}$: Singular values (diagonal)\n</li>\n<li>$\\mathbf{V} \\in \\mathbb{R}^{d \\times d}$: Right singular vectors\n</li>\n<li>$\\mathbf{U}^T\\mathbf{U} = \\mathbf{I}$, $\\mathbf{V}^T\\mathbf{V} = \\mathbf{I}$\n</li>\n</ul>\n\n\n\n<strong>Relationship to PCA:</strong>\n$$\\mathbf{X}^T\\mathbf{X} = \\mathbf{V}\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}\\mathbf{V}^T$$\n\nTherefore:\n<ul>\n\n<li>PC directions: Columns of $\\mathbf{V}$\n</li>\n<li>PC variances: $\\sigma_i^2 / n$\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/svd_decomposition.png]</em></p></div>\n\n\n\n<strong>Advantages of SVD for PCA:</strong>\n<ul>\n\n<li>More numerically stable\n</li>\n<li>Works for $n < d$ or $n > d$\n</li>\n<li>No need to form $\\mathbf{X}^T\\mathbf{X}$\n</li>\n<li>Efficient algorithms available\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Truncated SVD}\nKeep only top $k$ components:\n$$\\mathbf{X}_k = \\mathbf{U}_k\\mathbf{\\Sigma}_k\\mathbf{V}_k^T$$\nBest rank-$k$ approximation in Frobenius norm.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 11,
      "title": "SVD vs Eigendecomposition",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Eigendecomposition Approach</h4><strong>Steps:</strong>\n<ol>\n\n<li>Center data: $\\tilde{\\mathbf{X}} = \\mathbf{X} - \\bar{\\mathbf{X}}$\n</li>\n<li>Compute covariance: $\\mathbf{\\Sigma} = \\frac{1}{n}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$\n</li>\n<li>Eigendecomposition: $\\mathbf{\\Sigma} = \\mathbf{V}\\mathbf{\\Lambda}\\mathbf{V}^T$\n</li>\n<li>Sort by eigenvalues\n</li>\n</ol>\n\n\n\n<strong>Complexity:</strong> $\\mathcal{O}(d^2n + d^3)$\n\n\n\n<strong>Issues:</strong>\n<ul>\n\n<li>Numerical instability for $\\mathbf{X}^T\\mathbf{X}$\n</li>\n<li>Squaring condition number\n</li>\n<li>Memory: storing $d \\times d$ matrix\n</li>\n</ul></div>\n</div>\n\n<div class=\"column\">\n<div class=\"highlight\"><h4>SVD Approach</h4><strong>Steps:</strong>\n<ol>\n\n<li>Center data: $\\tilde{\\mathbf{X}} = \\mathbf{X} - \\bar{\\mathbf{X}}$\n</li>\n<li>SVD: $\\tilde{\\mathbf{X}} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$\n</li>\n<li>PCs: columns of $\\mathbf{V}$\n</li>\n<li>Variance: $\\sigma_i^2 / n$\n</li>\n</ol>\n\n\n\n<strong>Complexity:</strong> $\\mathcal{O}(\\min(nd^2, n^2d))$\n\n\n\n<strong>Advantages:</strong>\n<ul>\n\n<li>Better numerical stability\n</li>\n<li>Works directly on $\\mathbf{X}$\n</li>\n<li>Better for $n \\ll d$ or $n \\gg d$\n</li>\n<li>Modern implementations optimized\n</li>\n</ul></div>\n\n\n\n<div class=\"warning\"><h4>Recommendation</h4>Use SVD for PCA in practice.</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 12,
      "title": "Projection and Reconstruction",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Projection onto PCs:</strong>\n\nTransform to $k$ principal components:\n$$\\mathbf{Z} = \\mathbf{X}\\mathbf{V}_k$$\n\nwhere $\\mathbf{V}_k \\in \\mathbb{R}^{d \\times k}$ are first $k$ PCs.\n\n\n\n<strong>Properties:</strong>\n<ul>\n\n<li>$\\mathbf{Z} \\in \\mathbb{R}^{n \\times k}$: Reduced representation\n</li>\n<li>Columns of $\\mathbf{Z}$ are uncorrelated\n</li>\n<li>Maximum variance preserved\n</li>\n<li>Linear transformation\n</li>\n</ul>\n\n\n\n<strong>Reconstruction:</strong>\n$$\\hat{\\mathbf{X}} = \\mathbf{Z}\\mathbf{V}_k^T = \\mathbf{X}\\mathbf{V}_k\\mathbf{V}_k^T$$\n\n\n\n<div class=\"warning\"><h4>Reconstruction Error</h4>$$\\|\\mathbf{X} - \\hat{\\mathbf{X}}\\|_F^2 = \\sum_{i=k+1}^{d}\\lambda_i$$</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/pca_reconstruction.png]</em></p></div>\n\n\n\n<strong>Example:</strong>\n\nOriginal: $\\mathbf{x} = [5, 3]$\n\nPCs: $\\mathbf{v}_1 = [0.8, 0.6]$\n\nProjection:\n$$z_1 = \\mathbf{x}^T\\mathbf{v}_1 = 5(0.8) + 3(0.6) = 5.8$$\n\nReconstruction:\n$$\\hat{\\mathbf{x}} = z_1\\mathbf{v}_1 = 5.8[0.8, 0.6] = [4.64, 3.48]$$\n\nError:\n$$\\|\\mathbf{x} - \\hat{\\mathbf{x}}\\| = \\|[0.36, -0.48]\\| = 0.6$$\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 13,
      "title": "Data Centering and Standardization",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Centering (Required):</strong>\n\nRemove mean from each feature:\n$$\\tilde{x}_{ij} = x_{ij} - \\bar{x}_j$$\n\n\n\n<strong>Why?</strong>\n<ul>\n\n<li>PCA finds directions of variance\n</li>\n<li>Variance computed about mean\n</li>\n<li>Ensures PC1 passes through origin\n</li>\n<li>Mathematical requirement\n</li>\n</ul>\n\n\n\n<strong>Standardization (Optional):</strong>\n\nScale to unit variance:\n$$\\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{\\sigma_j}$$\n\n\n\n<strong>When to standardize?</strong>\n<ul>\n\n<li>Features have different scales\n</li>\n<li>Different units of measurement\n</li>\n<li>Want equal feature importance\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/standardization_importance.png]</em></p></div>\n\n\n\n\\begin{exampleblock}{Example Impact}\nWithout standardization:\n<ul>\n\n<li>Income: \\$20,000-\\$200,000\n</li>\n<li>Age: 20-80 years\n</li>\n<li>PC1 dominated by income scale\n</li>\n</ul>\n\nWith standardization:\n<ul>\n\n<li>Both scaled to mean 0, std 1\n</li>\n<li>Equal contribution opportunity\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<div class=\"warning\"><h4>Best Practice</h4><strong>Always center.</strong> Standardize if features have different scales or units.</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 14,
      "title": "PCA Algorithm: Step-by-Step",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n\\begin{algorithm}[H]\n\\caption{Principal Component Analysis}\n\\begin{algorithmic}[1]\n\\REQUIRE Data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, number of components $k$\n\\ENSURE Principal components $\\mathbf{V}_k$, transformed data $\\mathbf{Z}$\n\\STATE <strong>Center</strong> the data:\n\\STATE $\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i$\n\\STATE $\\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1}_n\\bar{\\mathbf{x}}^T$\n\\STATE <strong>Optional:</strong> Standardize features\n\\STATE <strong>Compute</strong> covariance matrix:\n\\STATE $\\mathbf{\\Sigma} = \\frac{1}{n}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$\n\\STATE <strong>OR</strong> compute SVD:\n\\STATE $\\tilde{\\mathbf{X}} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^T$\n\\STATE <strong>Extract</strong> top $k$ eigenvectors/singular vectors:\n\\STATE $\\mathbf{V}_k = [\\mathbf{v}_1, …, \\mathbf{v}_k]$\n\\STATE <strong>Project</strong> data onto PCs:\n\\STATE $\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{V}_k$\n\\RETURN $\\mathbf{V}_k$, $\\mathbf{Z}$\n\\end{algorithmic}\n\\end{algorithm}\n</div>\n\n<div class=\"column\">\n<strong>Key Steps Explained:</strong>\n\n\n\n\\begin{exampleblock}{Step 1-2: Centering}\nEssential for computing variance correctly. Shift data so mean is at origin.\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Step 3-4: Covariance/SVD}\nTwo equivalent approaches. SVD more stable numerically.\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Step 5: Top-k Selection}\nSort by eigenvalues (descending) and keep largest $k$ components.\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Step 6: Projection}\nTransform original data to new coordinate system defined by PCs.\n\\end{exampleblock}\n\n\n\n<strong>Complexity:</strong>\n<ul>\n\n<li>Eigendecomposition: $\\mathcal{O}(d^3)$\n</li>\n<li>SVD: $\\mathcal{O}(\\min(nd^2, n^2d))$\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 15,
      "title": "Worked Example: PCA on Toy Dataset (Part 1)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Problem Setup:</strong>\n\nDataset with 5 points in 2D:\n$$\\mathbf{X} = \\begin{bmatrix}\n1 & 2 \\\\\n2 & 3 \\\\\n3 & 4 \\\\\n4 & 5 \\\\\n5 & 6\n\\end{bmatrix}$$\n\n<strong>Goal:</strong> Find principal components\n\n\n\n<strong>Step 1: Center the data</strong>\n\nCompute mean:\n$$\\bar{\\mathbf{x}} = \\frac{1}{5}[15, 20]^T = [3, 4]^T$$\n\nCentered data:\n$$\\tilde{\\mathbf{X}} = \\begin{bmatrix}\n-2 & -2 \\\\\n-1 & -1 \\\\\n0 & 0 \\\\\n1 & 1 \\\\\n2 & 2\n\\end{bmatrix}$$\n</div>\n\n<div class=\"column\">\n<strong>Step 2: Compute covariance matrix</strong>\n\n$$\\mathbf{\\Sigma} = \\frac{1}{5}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$$\n\n$$= \\frac{1}{5}\\begin{bmatrix}\n-2 & -1 & 0 & 1 & 2 \\\\\n-2 & -1 & 0 & 1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n-2 & -2 \\\\\n-1 & -1 \\\\\n0 & 0 \\\\\n1 & 1 \\\\\n2 & 2\n\\end{bmatrix}$$\n\n$$= \\frac{1}{5}\\begin{bmatrix}\n10 & 10 \\\\\n10 & 10\n\\end{bmatrix} = \\begin{bmatrix}\n2 & 2 \\\\\n2 & 2\n\\end{bmatrix}$$\n\n\n\n<div class=\"warning\"><h4>Observation</h4>Perfect correlation: $\\text{Cov}(X_1, X_2) = \\text{Var}(X_1) = \\text{Var}(X_2) = 2$</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 16,
      "title": "Worked Example: PCA on Toy Dataset (Part 2)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Step 3: Find eigenvalues</strong>\n\nSolve: $\\det(\\mathbf{\\Sigma} - \\lambda\\mathbf{I}) = 0$\n\n$$\\det\\begin{bmatrix} 2-\\lambda & 2 \\\\ 2 & 2-\\lambda \\end{bmatrix} = 0$$\n\n$$(2-\\lambda)^2 - 4 = 0$$\n$$\\lambda^2 - 4\\lambda = 0$$\n$$\\lambda(\\lambda - 4) = 0$$\n\n<strong>Eigenvalues:</strong>\n$$\\lambda_1 = 4,   \\lambda_2 = 0$$\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>All variance in first PC: $\\lambda_1 = 4$\n</li>\n<li>Zero variance in second PC: $\\lambda_2 = 0$\n</li>\n<li>Data lies on a line!\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Step 4: Find eigenvectors</strong>\n\nFor $\\lambda_1 = 4$:\n$$(\\mathbf{\\Sigma} - 4\\mathbf{I})\\mathbf{v}_1 = 0$$\n$$\\begin{bmatrix} -2 & 2 \\\\ 2 & -2 \\end{bmatrix}\\mathbf{v}_1 = 0$$\n\nSolution: $\\mathbf{v}_1 = \\frac{1}{\\sqrt{2}}[1, 1]^T$\n\n\n\nFor $\\lambda_2 = 0$:\n$$\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}}[1, -1]^T$$\n\n\n\n<strong>Verification:</strong>\n$$\\mathbf{v}_1^T\\mathbf{v}_2 = \\frac{1}{2}(1 - 1) = 0 \\checkmark$$\n\n\n\n\\begin{exampleblock}{Result}\nPC1: $[0.707, 0.707]$ (diagonal direction)\\\\\nPC2: $[0.707, -0.707]$ (perpendicular)\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 17,
      "title": "Worked Example: PCA on Toy Dataset (Part 3)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Step 5: Project data onto PCs</strong>\n\n$$\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{V}$$\n\n$$= \\begin{bmatrix}\n-2 & -2 \\\\\n-1 & -1 \\\\\n0 & 0 \\\\\n1 & 1 \\\\\n2 & 2\n\\end{bmatrix}\n\\begin{bmatrix}\n0.707 & 0.707 \\\\\n0.707 & -0.707\n\\end{bmatrix}$$\n\n$$= \\begin{bmatrix}\n-2.83 & 0 \\\\\n-1.41 & 0 \\\\\n0 & 0 \\\\\n1.41 & 0 \\\\\n2.83 & 0\n\\end{bmatrix}$$\n\n\n\n<strong>Observation:</strong> All points lie on PC1 axis (PC2 coordinates are zero).\n</div>\n\n<div class=\"column\">\n<strong>Step 6: Explained variance</strong>\n\nTotal variance:\n$$\\text{Total} = \\lambda_1 + \\lambda_2 = 4 + 0 = 4$$\n\nExplained by PC1:\n$$\\frac{\\lambda_1}{\\text{Total}} = \\frac{4}{4} = 100\\%$$\n\nExplained by PC2:\n$$\\frac{\\lambda_2}{\\text{Total}} = \\frac{0}{4} = 0\\%$$\n\n\n\n<strong>Reconstruction (k=1):</strong>\n$$\\hat{\\mathbf{X}} = \\mathbf{Z}_{:,1}\\mathbf{v}_1^T + \\bar{\\mathbf{X}}$$\n\nPerfect reconstruction since all variance in PC1!\n\n\n\n<div class=\"warning\"><h4>Conclusion</h4>This toy example shows perfectly correlated data requiring only 1 dimension.</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 18,
      "title": "Worked Example: Non-Trivial 2D Case (Part 1)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>New Dataset (4 points):</strong>\n\n$$\\mathbf{X} = \\begin{bmatrix}\n1 & 2 \\\\\n2 & 1 \\\\\n3 & 4 \\\\\n4 & 3\n\\end{bmatrix}$$\n\n\n\n<strong>Step 1: Center data</strong>\n\nMean: $\\bar{\\mathbf{x}} = [2.5, 2.5]^T$\n\n$$\\tilde{\\mathbf{X}} = \\begin{bmatrix}\n-1.5 & -0.5 \\\\\n-0.5 & -1.5 \\\\\n0.5 & 1.5 \\\\\n1.5 & 0.5\n\\end{bmatrix}$$\n\n\n\n<strong>Step 2: Covariance</strong>\n\n$$\\mathbf{\\Sigma} = \\frac{1}{4}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$$\n$$= \\frac{1}{4}\\begin{bmatrix}\n5 & 3 \\\\\n3 & 5\n\\end{bmatrix} = \\begin{bmatrix}\n1.25 & 0.75 \\\\\n0.75 & 1.25\n\\end{bmatrix}$$\n</div>\n\n<div class=\"column\">\n<strong>Step 3: Eigenvalues</strong>\n\n$$\\det(\\mathbf{\\Sigma} - \\lambda\\mathbf{I}) = 0$$\n\n$$(1.25-\\lambda)^2 - 0.5625 = 0$$\n$$\\lambda^2 - 2.5\\lambda + 1 = 0$$\n\nUsing quadratic formula:\n$$\\lambda = \\frac{2.5 \\pm \\sqrt{6.25-4}}{2} = \\frac{2.5 \\pm 1.5}{2}$$\n\n<strong>Eigenvalues:</strong>\n$$\\lambda_1 = 2.0,   \\lambda_2 = 0.5$$\n\n\n\n<strong>Variance explained:</strong>\n<ul>\n\n<li>PC1: $\\frac{2.0}{2.5} = 80\\%$\n</li>\n<li>PC2: $\\frac{0.5}{2.5} = 20\\%$\n</li>\n<li>Both components carry information!\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 19,
      "title": "Worked Example: Non-Trivial 2D Case (Part 2)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Step 4: Eigenvectors</strong>\n\nFor $\\lambda_1 = 2.0$:\n$$\\begin{bmatrix} -0.75 & 0.75 \\\\ 0.75 & -0.75 \\end{bmatrix}\\mathbf{v}_1 = 0$$\n\nNormalize: $\\mathbf{v}_1 = \\frac{1}{\\sqrt{2}}[1, 1]^T = [0.707, 0.707]^T$\n\n\n\nFor $\\lambda_2 = 0.5$:\n$$\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}}[1, -1]^T = [0.707, -0.707]^T$$\n\n\n\n<strong>Step 5: Transform data</strong>\n$$\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{V}$$\n\n$$= \\begin{bmatrix}\n-1.414 & -0.707 \\\\\n-1.414 & 0.707 \\\\\n1.414 & -0.707 \\\\\n1.414 & 0.707\n\\end{bmatrix}$$\n</div>\n\n<div class=\"column\">\n<strong>Step 6: Reconstruction (k=1)</strong>\n\nKeep only PC1:\n$$\\hat{\\tilde{\\mathbf{X}}} = \\mathbf{Z}_{:,1}\\mathbf{v}_1^T$$\n\n$$= \\begin{bmatrix}\n-1.414 \\\\\n-1.414 \\\\\n1.414 \\\\\n1.414\n\\end{bmatrix}\n\\begin{bmatrix}\n0.707 & 0.707\n\\end{bmatrix}$$\n\n$$= \\begin{bmatrix}\n-1.0 & -1.0 \\\\\n-1.0 & -1.0 \\\\\n1.0 & 1.0 \\\\\n1.0 & 1.0\n\\end{bmatrix}$$\n\n\n\n<strong>Reconstruction error:</strong>\n$$\\text{Error} = \\lambda_2 = 0.5$$\n\n\n\n\\begin{exampleblock}{Summary}\nReduced from 2D to 1D, preserving 80\\% of variance.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 20,
      "title": "Kernel PCA: Non-linear Extension",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Motivation</h4>Standard PCA finds only <strong>linear</strong> relationships. Many real-world datasets have non-linear structure.</div>\n\n\n\n<strong>Key Idea:</strong>\n<ul>\n\n<li>Map data to high-dimensional space: $\\phi: \\mathbb{R}^d \\to \\mathcal{H}$\n</li>\n<li>Apply PCA in feature space $\\mathcal{H}$\n</li>\n<li>Use kernel trick: $K(x_i, x_j) = \\phi(x_i)^T\\phi(x_j)$\n</li>\n<li>Never explicitly compute $\\phi(x)$\n</li>\n</ul>\n\n\n\n<strong>Algorithm:</strong>\n<ol>\n\n<li>Compute kernel matrix: $\\mathbf{K}_{ij} = K(x_i, x_j)$\n</li>\n<li>Center kernel matrix\n</li>\n<li>Eigendecompose: $\\mathbf{K} = \\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^T$\n</li>\n<li>Project: $z_i = \\mathbf{U}^T\\mathbf{k}(x_i)$\n</li>\n</ol>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/kernel_pca_concept.png]</em></p></div>\n\n\n\n<strong>Common Kernels:</strong>\n<ul>\n\n<li><strong>RBF:</strong> $K(x,y) = \\exp(-\\gamma\\|x-y\\|^2)$\n</li>\n<li><strong>Polynomial:</strong> $K(x,y) = (x^Ty + c)^d$\n</li>\n<li><strong>Sigmoid:</strong> $K(x,y) = \\tanh(\\alpha x^Ty + c)$\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Applications}\n<ul>\n\n<li>Manifold learning\n</li>\n<li>Non-linear dimensionality reduction\n</li>\n<li>Feature extraction for non-linear data\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<div class=\"warning\"><h4>Trade-off</h4>More flexible but computationally expensive: $\\mathcal{O}(n^3)$</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 21,
      "title": "Sparse PCA",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Motivation</h4>Standard PCA produces components that are linear combinations of <strong>all</strong> original features, making interpretation difficult.</div>\n\n\n\n<strong>Goal:</strong> Find PCs with <strong>sparse loadings</strong>\n<ul>\n\n<li>Most loadings are exactly zero\n</li>\n<li>Only few features contribute to each PC\n</li>\n<li>Easier interpretation\n</li>\n<li>Feature selection built-in\n</li>\n</ul>\n\n\n\n<strong>Formulation:</strong>\n$$\\min_{\\mathbf{V}} \\|\\mathbf{X} - \\mathbf{X}\\mathbf{V}\\mathbf{V}^T\\|_F^2 + \\lambda\\sum_{j=1}^{k}\\|\\mathbf{v}_j\\|_1$$\n\n\n\n<ul>\n\n<li>First term: Reconstruction error\n</li>\n<li>Second term: Sparsity penalty (L1 regularization)\n</li>\n<li>$\\lambda$: Controls sparsity level\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/sparse_pca.png]</em></p></div>\n\n\n\n<strong>Comparison:</strong>\n\n\\begin{tabular}{|l|c|c|}\n\\hline\n<strong>Method</strong> & <strong>Variance</strong> & <strong>Sparsity</strong> \\\\\n\\hline\nStandard PCA & 100\\% & 0\\% \\\\\nSparse PCA & 95-98\\% & 70-90\\% \\\\\n\\hline\n\\end{tabular}\n\n\n\n<strong>Applications:</strong>\n<ul>\n\n<li>Gene expression analysis\n</li>\n<li>Financial data analysis\n</li>\n<li>Text mining\n</li>\n<li>Any domain requiring interpretability\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Trade-off</h4>Slight loss of explained variance for much better interpretability.</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 22,
      "title": "Incremental PCA",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Motivation</h4>Standard PCA requires entire dataset in memory. Not feasible for:\n<ul>\n\n<li>Very large datasets\n</li>\n<li>Streaming data\n</li>\n<li>Online learning scenarios\n</li>\n<li>Limited memory systems\n</li>\n</ul></div>\n\n\n\n<strong>Key Idea:</strong>\nProcess data in <strong>mini-batches</strong>:\n<ol>\n\n<li>Initialize with first batch\n</li>\n<li>Update PCs incrementally with new batches\n</li>\n<li>Maintain approximate PCs online\n</li>\n<li>Never load full dataset\n</li>\n</ol>\n\n\n\n<strong>Algorithm Sketch:</strong>\n\\begin{algorithmic}[1]\n\\STATE Initialize: $\\mathbf{V}_0$, $\\mathbf{\\Sigma}_0$, $n_0$\n\\FOR{each batch $\\mathbf{X}_b$}\n\\STATE Update mean: $\\bar{\\mathbf{x}}_{new}$\n\\STATE Update covariance: $\\mathbf{\\Sigma}_{new}$\n\\STATE SVD: Update $\\mathbf{V}$\n\\STATE $n \\gets n + n_b$\n\\ENDFOR\n\\end{algorithmic}\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/incremental_pca.png]</em></p></div>\n\n\n\n<strong>Advantages:</strong>\n<ul>\n\n<li>Memory efficient: Constant memory\n</li>\n<li>Scalable: Handles large data\n</li>\n<li>Online: Updates with new data\n</li>\n<li>Streaming: Real-time processing\n</li>\n</ul>\n\n\n\n<strong>Complexity:</strong>\n<ul>\n\n<li>Per batch: $\\mathcal{O}(bd^2)$ where $b$ = batch size\n</li>\n<li>Memory: $\\mathcal{O}(d^2)$ instead of $\\mathcal{O}(nd)$\n</li>\n<li>Nearly identical results to batch PCA\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Use Cases}\n<ul>\n\n<li>Large-scale image processing\n</li>\n<li>Sensor data streams\n</li>\n<li>Online recommendation systems\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 23,
      "title": "Probabilistic PCA",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Motivation</h4>Standard PCA is deterministic. Probabilistic PCA provides:\n<ul>\n\n<li>Generative model of data\n</li>\n<li>Handling of missing values\n</li>\n<li>Uncertainty quantification\n</li>\n<li>Bayesian extensions\n</li>\n</ul></div>\n\n\n\n<strong>Model:</strong>\n$$\\begin{aligned}\\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I}_k) \\\\ \\mathbf{x}|\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2\\mathbf{I}_d)\\end{aligned}$$\n\nwhere:\n<ul>\n\n<li>$\\mathbf{z} \\in \\mathbb{R}^k$: Latent variables\n</li>\n<li>$\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$: Loading matrix\n</li>\n<li>$\\sigma^2$: Isotropic noise\n</li>\n</ul>\n\n\n\n<strong>Maximum Likelihood Solution:</strong>\n$$\\mathbf{W}_{ML} = \\mathbf{V}_k(\\mathbf{\\Lambda}_k - \\sigma^2\\mathbf{I})^{1/2}\\mathbf{R}$$\n\nwhere $\\mathbf{R}$ is arbitrary rotation.\n</div>\n\n<div class=\"column\">\n<strong>EM Algorithm:</strong>\n\n\\begin{algorithmic}[1]\n\\STATE Initialize $\\mathbf{W}$, $\\sigma^2$\n\\REPEAT\n\\STATE <strong>E-step:</strong> Compute $\\mathbb{E}[\\mathbf{z}|\\mathbf{x}]$\n\\STATE <strong>M-step:</strong> Update $\\mathbf{W}$, $\\sigma^2$\n\\UNTIL{convergence}\n\\end{algorithmic}\n\n\n\n<strong>Advantages:</strong>\n<ul>\n\n<li>Handle missing data naturally\n</li>\n<li>Mixture of PPCAs for clustering\n</li>\n<li>Bayesian inference possible\n</li>\n<li>Uncertainty quantification\n</li>\n</ul>\n\n\n\n<strong>Relationship to Standard PCA:</strong>\n<ul>\n\n<li>As $\\sigma^2 \\to 0$: Recovers standard PCA\n</li>\n<li>Same subspace in limit\n</li>\n<li>Additional noise model\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Extensions}\n<ul>\n\n<li>Factor Analysis: Non-isotropic noise\n</li>\n<li>Variational PCA: Variational inference\n</li>\n<li>Sparse PPCA: Sparse loadings\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 24,
      "title": "Robust PCA",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Motivation</h4>Standard PCA is sensitive to:\n<ul>\n\n<li>Outliers: Anomalous data points\n</li>\n<li>Corrupted entries: Missing or noisy values\n</li>\n<li>Sparse errors: Localized corruption\n</li>\n</ul></div>\n\n\n\n<strong>Problem Formulation:</strong>\n\nDecompose $\\mathbf{X}$ into:\n$$\\mathbf{X} = \\mathbf{L} + \\mathbf{S}$$\n\nwhere:\n<ul>\n\n<li>$\\mathbf{L}$: Low-rank (clean data)\n</li>\n<li>$\\mathbf{S}$: Sparse (outliers/corruption)\n</li>\n</ul>\n\n\n\n<strong>Optimization:</strong>\n$$\\min_{\\mathbf{L},\\mathbf{S}} \\|\\mathbf{L}\\|_* + \\lambda\\|\\mathbf{S}\\|_1$$\n$$\\text{subject to } \\mathbf{X} = \\mathbf{L} + \\mathbf{S}$$\n\n<ul>\n\n<li>$\\|\\mathbf{L}\\|_*$: Nuclear norm (rank proxy)\n</li>\n<li>$\\|\\mathbf{S}\\|_1$: L1 norm (sparsity)\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Applications:</strong>\n\n\n\n\\begin{exampleblock}{Video Surveillance}\n<ul>\n\n<li>$\\mathbf{L}$: Static background\n</li>\n<li>$\\mathbf{S}$: Moving objects\n</li>\n<li>Separates foreground/background\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Face Recognition}\n<ul>\n\n<li>$\\mathbf{L}$: Face structure\n</li>\n<li>$\\mathbf{S}$: Occlusions, shadows\n</li>\n<li>Robust to partial occlusion\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<strong>Solution Methods:</strong>\n<ul>\n\n<li>Principal Component Pursuit (PCP)\n</li>\n<li>Alternating Direction Method (ADMM)\n</li>\n<li>Proximal gradient methods\n</li>\n</ul>\n\n\n\n<strong>Complexity:</strong>\n<ul>\n\n<li>$\\mathcal{O}(nd^2k)$ per iteration\n</li>\n<li>Slower than standard PCA\n</li>\n<li>Worth it for corrupted data\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 25,
      "title": "Scree Plot: Visualizing Variance",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4>A <strong>scree plot</strong> displays eigenvalues (or explained variance) for each principal component in decreasing order.</div>\n\n\n\n<strong>How to Read:</strong>\n<ul>\n\n<li>X-axis: Principal component number\n</li>\n<li>Y-axis: Eigenvalue or explained variance\n</li>\n<li>Look for \"elbow\" point\n</li>\n<li>Steep drop indicates important PCs\n</li>\n<li>Flat tail indicates noise\n</li>\n</ul>\n\n\n\n<strong>Elbow Method:</strong>\n<ul>\n\n<li>Find point where curve bends\n</li>\n<li>Keep PCs before elbow\n</li>\n<li>Remaining PCs contribute little\n</li>\n<li>Subjective but practical\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Rule of Thumb</h4>Keep components before the \"elbow\" where variance drops sharply.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/scree_plot.png]</em></p></div>\n\n\n\n<strong>Example Interpretation:</strong>\n<ul>\n\n<li>PC1: Large eigenvalue (most variance)\n</li>\n<li>PC2-3: Moderate decrease\n</li>\n<li>PC4+: Flat (noise level)\n</li>\n<li><strong>Decision:</strong> Keep 3-4 components\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Kaiser Criterion}\nAlternative rule: Keep components with eigenvalue $> 1$ (for correlation matrix).\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 26,
      "title": "Explained Variance Ratio",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Individual Explained Variance:</strong>\n\nFor component $i$:\n$$\\text{EVR}_i = \\frac{\\lambda_i}{\\sum_{j=1}^{d}\\lambda_j}$$\n\n\n\n<strong>Cumulative Explained Variance:</strong>\n\nFor first $k$ components:\n$$\\text{CEV}_k = \\frac{\\sum_{i=1}^{k}\\lambda_i}{\\sum_{j=1}^{d}\\lambda_j}$$\n\n\n\n<strong>Selection Criteria:</strong>\n<ul>\n\n<li><strong>Fixed threshold:</strong> $\\text{CEV}_k \\geq 0.95$ (95\\%)\n</li>\n<li><strong>Fixed number:</strong> $k = 10$ components\n</li>\n<li><strong>Elbow method:</strong> Visual inspection\n</li>\n<li><strong>Cross-validation:</strong> Best downstream performance\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Common Choice</h4>Keep components explaining 90-95\\% of total variance.</div>\n</div>\n\n<div class=\"column\">\n<strong>Example Calculation:</strong>\n\nEigenvalues: $[5.2, 2.1, 0.8, 0.3, 0.1]$\n\nTotal: $\\sum \\lambda_i = 8.5$\n\n\n\n\\begin{tabular}{|c|c|c|}\n\\hline\n<strong>PC</strong> & <strong>EVR</strong> & <strong>CEV</strong> \\\\\n\\hline\n1 & 61.2\\% & 61.2\\% \\\\\n2 & 24.7\\% & 85.9\\% \\\\\n3 & 9.4\\% & 95.3\\% \\\\\n4 & 3.5\\% & 98.8\\% \\\\\n5 & 1.2\\% & 100.0\\% \\\\\n\\hline\n\\end{tabular}\n\n\n\n<strong>Decision:</strong>\n<ul>\n\n<li>For 85\\% variance: Keep 2 PCs\n</li>\n<li>For 95\\% variance: Keep 3 PCs\n</li>\n<li>For 99\\% variance: Keep 4 PCs\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Trade-off}\nMore components: Better reconstruction, less compression\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 27,
      "title": "Reconstruction Error",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"highlight\"><h4>Definition</h4><strong>Reconstruction error</strong> measures information lost when using only $k < d$ principal components.</div>\n\n\n\n<strong>Formula:</strong>\n$$E_k = \\|\\mathbf{X} - \\hat{\\mathbf{X}}_k\\|_F^2 = \\sum_{i=k+1}^{d}\\lambda_i$$\n\nwhere $\\hat{\\mathbf{X}}_k = \\mathbf{X}\\mathbf{V}_k\\mathbf{V}_k^T$\n\n\n\n<strong>Normalized Error:</strong>\n$$\\text{Relative Error} = \\frac{E_k}{\\|\\mathbf{X}\\|_F^2} = \\frac{\\sum_{i=k+1}^{d}\\lambda_i}{\\sum_{i=1}^{d}\\lambda_i}$$\n\n\n\n<strong>Properties:</strong>\n<ul>\n\n<li>Monotonically decreasing in $k$\n</li>\n<li>Zero when $k = d$ (perfect reconstruction)\n</li>\n<li>Directly related to discarded eigenvalues\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/reconstruction_error.png]</em></p></div>\n\n\n\n<strong>Selection Strategy:</strong>\n\nSet error tolerance $\\epsilon$:\n$$E_k \\leq \\epsilon \\cdot \\|\\mathbf{X}\\|_F^2$$\n\nChoose smallest $k$ satisfying constraint.\n\n\n\n<strong>Example:</strong>\n<ul>\n\n<li>Total variance: 100\n</li>\n<li>Target: 5\\% error tolerance\n</li>\n<li>Need: CEV $\\geq$ 95\\%\n</li>\n<li>$\\sum_{i=1}^{k}\\lambda_i \\geq 95$\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Application}\nImage compression: Balance file size vs quality using reconstruction error.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 28,
      "title": "Cross-Validation for Component Selection",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Task-Specific Selection:</strong>\n\nWhen PCA is used for downstream task:\n<ol>\n\n<li>Apply PCA with different $k$ values\n</li>\n<li>Train model on reduced data\n</li>\n<li>Evaluate on validation set\n</li>\n<li>Choose $k$ with best performance\n</li>\n</ol>\n\n\n\n<strong>Procedure:</strong>\n\\begin{algorithmic}[1]\n\\STATE Split data: Train/Validation\n\\FOR{$k = 1$ to $k_{max}$}\n\\STATE Fit PCA on training set\n\\STATE Transform train \\& validation\n\\STATE Train classifier/regressor\n\\STATE Evaluate performance\n\\ENDFOR\n\\STATE Select $k$ with best validation score\n\\end{algorithmic}\n\n\n\n<div class=\"warning\"><h4>Important</h4>Fit PCA only on training data to avoid data leakage!</div>\n</div>\n\n<div class=\"column\">\n<strong>Example: Classification Task</strong>\n\n\n\\begin{tabular}{|c|c|c|}\n\\hline\n<strong>k</strong> & <strong>Train Acc</strong> & <strong>Val Acc</strong> \\\\\n\\hline\n5 & 0.75 & 0.72 \\\\\n10 & 0.82 & 0.79 \\\\\n20 & 0.89 & 0.85 \\\\\n50 & 0.95 & 0.84 \\\\\n100 & 0.98 & 0.82 \\\\\n\\hline\n\\end{tabular}\n\n\n<strong>Analysis:</strong>\n<ul>\n\n<li>Best validation: $k = 20$\n</li>\n<li>$k = 50, 100$: Overfitting\n</li>\n<li>Task-specific optimum\n</li>\n</ul>\n\n\n\n<strong>Considerations:</strong>\n<ul>\n\n<li>Variance explained vs task performance\n</li>\n<li>Computational cost\n</li>\n<li>Interpretability needs\n</li>\n<li>Domain constraints\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Best Practice}\nUse cross-validation when PCA is preprocessing step for supervised learning.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 29,
      "title": "Component Correlation Analysis",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Goal:</strong> Understand relationships between original features and principal components.\n\n\n\n<strong>Loading Matrix:</strong>\n\n$$\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, …, \\mathbf{v}_d]$$\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>$v_{ij}$: Contribution of feature $j$ to PC $i$\n</li>\n<li>Large $|v_{ij}|$: Feature $j$ important for PC $i$\n</li>\n<li>Sign indicates direction of contribution\n</li>\n<li>Loadings are correlations (if standardized)\n</li>\n</ul>\n\n\n\n<strong>Biplot:</strong>\n<ul>\n\n<li>Visualize data and loadings together\n</li>\n<li>Arrow length: Variable importance\n</li>\n<li>Arrow direction: Correlation with PCs\n</li>\n<li>Angle between arrows: Variable correlation\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Interpretation</h4>Close to parallel arrows indicate highly correlated variables.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/component_correlation.png]</em></p></div>\n\n\n\n<strong>Example Loading Matrix:</strong>\n\n\n\\scriptsize\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n<strong>Feature</strong> & <strong>PC1</strong> & <strong>PC2</strong> & <strong>PC3</strong> \\\\\n\\hline\nHeight & 0.82 & 0.15 & -0.05 \\\\\nWeight & 0.79 & 0.20 & 0.08 \\\\\nAge & 0.12 & 0.91 & 0.10 \\\\\nIncome & -0.05 & 0.08 & 0.95 \\\\\n\\hline\n\\end{tabular}\n\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>PC1: \"Body size\" (height, weight)\n</li>\n<li>PC2: \"Age\" dimension\n</li>\n<li>PC3: \"Income\" dimension\n</li>\n<li>Clear separation of concepts\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 30,
      "title": "Application: Image Compression",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Problem:</strong>\nImages contain redundant information. Can we store them more efficiently?\n\n\n\n<strong>PCA Approach:</strong>\n<ol>\n\n<li>Treat each image as vector (flatten pixels)\n</li>\n<li>Build dataset: $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\n</li>\n<li>Apply PCA: Find eigenvectors\n</li>\n<li>Project: $\\mathbf{Z} = \\mathbf{X}\\mathbf{V}_k$\n</li>\n<li>Store: $\\mathbf{Z}$ and $\\mathbf{V}_k$ instead of $\\mathbf{X}$\n</li>\n</ol>\n\n\n\n<strong>Compression Ratio:</strong>\n\nOriginal: $n \\times d$ values\n\nCompressed: $n \\times k + k \\times d$ values\n\nRatio: $\\frac{nd}{nk + kd} \\approx \\frac{d}{k}$ (for large $n$)\n\n\n\n<strong>Example:</strong>\n<ul>\n\n<li>$d = 10,000$ pixels\n</li>\n<li>$k = 100$ components\n</li>\n<li>Compression: $100:1$\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/image_compression.png]</em></p></div>\n\n\n\n<strong>Quality vs Compression:</strong>\n\n\n\\scriptsize\n\\begin{tabular}{|c|c|c|}\n\\hline\n<strong>PCs</strong> & <strong>Compression</strong> & <strong>Quality</strong> \\\\\n\\hline\n10 & 95\\% & Poor \\\\\n50 & 75\\% & Fair \\\\\n100 & 50\\% & Good \\\\\n200 & 0\\% & Excellent \\\\\n\\hline\n\\end{tabular}\n\n\n\n\n\\begin{exampleblock}{Trade-off}\nBalance between file size and visual quality. Typical choice: 80-90\\% variance retained.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 31,
      "title": "Application: Face Recognition (Eigenfaces)",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Eigenfaces Method:</strong>\n\n<ol>\n\n<li>Collect face images: $n$ images, $d$ pixels each\n</li>\n<li>Apply PCA: Find \"eigenfaces\" (principal components)\n</li>\n<li>Each eigenface captures facial variation\n</li>\n<li>Represent faces in eigenface space\n</li>\n<li>Recognition: Nearest neighbor in PC space\n</li>\n</ol>\n\n\n\n<strong>Advantages:</strong>\n<ul>\n\n<li>Dimensionality reduction: $10,000 \\to 100$\n</li>\n<li>Fast matching in low-dimensional space\n</li>\n<li>Captures important facial features\n</li>\n<li>Robust to minor variations\n</li>\n</ul>\n\n\n\n<strong>Process:</strong>\n<ul>\n\n<li>Training: Build eigenface basis\n</li>\n<li>Enrollment: Project new face to PC space\n</li>\n<li>Recognition: Compare with database\n</li>\n<li>Decision: Threshold distance\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/eigenfaces_application.png]</em></p></div>\n\n\n\n<strong>Typical Eigenfaces:</strong>\n<ul>\n\n<li>PC1: Average lighting\n</li>\n<li>PC2: Left-right contrast\n</li>\n<li>PC3: Facial expression\n</li>\n<li>PC4-10: Detailed features\n</li>\n<li>PC11+: Fine details/noise\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Historical Note}\nEigenfaces pioneered in 1991 by Turk and Pentland. Still used as baseline method.\n\\end{exampleblock}\n\n\n\n<div class=\"warning\"><h4>Limitations</h4>Sensitive to: lighting, pose, facial expression. Modern methods use deep learning.</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 32,
      "title": "Application: Noise Filtering",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Principle:</strong>\n\nNoise typically has:\n<ul>\n\n<li>Low variance\n</li>\n<li>High frequency\n</li>\n<li>Random direction\n</li>\n<li>Captured by small eigenvalues\n</li>\n</ul>\n\nSignal has:\n<ul>\n\n<li>High variance\n</li>\n<li>Low frequency\n</li>\n<li>Structured patterns\n</li>\n<li>Captured by large eigenvalues\n</li>\n</ul>\n\n\n\n<strong>Denoising Procedure:</strong>\n<ol>\n\n<li>Apply PCA to noisy data\n</li>\n<li>Keep only top $k$ components (signal)\n</li>\n<li>Discard remaining components (noise)\n</li>\n<li>Reconstruct: $\\hat{\\mathbf{X}} = \\mathbf{X}\\mathbf{V}_k\\mathbf{V}_k^T$\n</li>\n</ol>\n\n\n\n<div class=\"warning\"><h4>Key Insight</h4>PCA acts as low-pass filter, removing high-frequency noise.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/noise_filtering.png]</em></p></div>\n\n\n\n<strong>Example: Signal Denoising</strong>\n\nOriginal signal + Gaussian noise\n\n<ul>\n\n<li>Noise variance: 0.1\n</li>\n<li>PCA: Keep 5 components\n</li>\n<li>SNR improvement: 15 dB\n</li>\n</ul>\n\n\n\n<strong>Applications:</strong>\n<ul>\n\n<li>Image denoising\n</li>\n<li>Audio signal processing\n</li>\n<li>Sensor data cleaning\n</li>\n<li>Medical imaging\n</li>\n<li>Financial time series\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Challenge}\nChoosing $k$: Too small loses signal, too large keeps noise. Use cross-validation or domain knowledge.\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 33,
      "title": "Application: Data Visualization",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Challenge:</strong>\n\nCannot visualize data in $d > 3$ dimensions.\n\n\n\n<strong>PCA Solution:</strong>\n\nProject to 2D or 3D for visualization:\n<ol>\n\n<li>Apply PCA: Get principal components\n</li>\n<li>Keep PC1 and PC2 (or PC1, PC2, PC3)\n</li>\n<li>Plot data in reduced space\n</li>\n<li>Preserves maximum variance\n</li>\n</ol>\n\n\n\n<strong>Interpretation:</strong>\n<ul>\n\n<li>PC1 (x-axis): Direction of most variance\n</li>\n<li>PC2 (y-axis): Second most variance\n</li>\n<li>Distances approximately preserved\n</li>\n<li>Clusters may emerge\n</li>\n</ul>\n\n\n\n<strong>Use Cases:</strong>\n<ul>\n\n<li>Exploratory data analysis\n</li>\n<li>Cluster visualization\n</li>\n<li>Outlier detection\n</li>\n<li>Pattern discovery\n</li>\n<li>Presentation to stakeholders\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/data_visualization_application.png]</em></p></div>\n\n\n\n\\begin{exampleblock}{Example: Iris Dataset}\n<ul>\n\n<li>Original: 4 dimensions\n</li>\n<li>PCA: 2D projection\n</li>\n<li>Explained variance: 95.8\\%\n</li>\n<li>Clear species separation visible\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<strong>Comparison with Other Methods:</strong>\n\n\n\\scriptsize\n\\begin{tabular}{|l|c|c|}\n\\hline\n<strong>Method</strong> & <strong>Linear</strong> & <strong>Global</strong> \\\\\n\\hline\nPCA & Yes & Yes \\\\\nt-SNE & No & No \\\\\nUMAP & No & Local \\\\\nMDS & Yes/No & Yes \\\\\n\\hline\n\\end{tabular}\n\n\n\n\n<div class=\"warning\"><h4>Limitation</h4>PCA preserves global structure but may miss non-linear patterns.</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 34,
      "title": "Application: Exploratory Data Analysis",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Digits Dataset Visualization:</strong>\n\nHigh-dimensional handwritten digit images (64 dimensions) projected to 2D.\n\n\n\n<strong>Insights from PCA:</strong>\n<ul>\n\n<li>Digit clusters visible in PC space\n</li>\n<li>Similar digits closer together\n</li>\n<li>Outliers easily identified\n</li>\n<li>Confusion patterns apparent\n</li>\n</ul>\n\n\n\n<strong>Practical Workflow:</strong>\n<ol>\n\n<li>Load dataset\n</li>\n<li>Standardize features\n</li>\n<li>Apply PCA (keep 2-3 PCs)\n</li>\n<li>Visualize with scatter plot\n</li>\n<li>Color by labels (if available)\n</li>\n<li>Identify patterns/outliers\n</li>\n</ol>\n\n\n\n<div class=\"warning\"><h4>EDA Benefit</h4>Quick visual check before applying complex ML models.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/digits_visualization.png]</em></p></div>\n\n\n\n<strong>Observations:</strong>\n<ul>\n\n<li>Digits 0 and 1 well-separated\n</li>\n<li>Digits 3, 5, 8 overlap slightly\n</li>\n<li>Some outliers (miswritten digits)\n</li>\n<li>PC1 and PC2 capture 30\\% variance\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Feature Engineering}\nPCA projections can be used as features for downstream classification:\n<ul>\n\n<li>Reduce 64D to 20D\n</li>\n<li>Train classifier on PC scores\n</li>\n<li>Faster training\n</li>\n<li>Often better generalization\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 35,
      "title": "Application: Feature Engineering",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>PCA as Preprocessing:</strong>\n\nUse PCA-transformed features for ML models.\n\n\n\n<strong>Benefits:</strong>\n<ul>\n\n<li><strong>Decorrelation:</strong> Remove multicollinearity\n</li>\n<li><strong>Dimensionality:</strong> Reduce feature count\n</li>\n<li><strong>Noise reduction:</strong> Filter out noisy components\n</li>\n<li><strong>Speed:</strong> Faster model training\n</li>\n<li><strong>Regularization:</strong> Implicit regularization effect\n</li>\n</ul>\n\n\n\n<strong>Pipeline:</strong>\n\\begin{algorithmic}[1]\n\\STATE Train set: Fit PCA\n\\STATE Train set: Transform with PCA\n\\STATE Test set: Transform with same PCA\n\\STATE Train classifier on PC scores\n\\STATE Evaluate on test PC scores\n\\end{algorithmic}\n\n\n\n<div class=\"warning\"><h4>Warning</h4>Never fit PCA on test data! This causes data leakage.</div>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/feature_engineering_application.png]</em></p></div>\n\n\n\n<strong>Example Results:</strong>\n\n\n\\scriptsize\n\\begin{tabular}{|l|c|c|}\n\\hline\n<strong>Features</strong> & <strong>Accuracy</strong> & <strong>Time</strong> \\\\\n\\hline\nOriginal (1000D) & 0.85 & 120s \\\\\nPCA (100D) & 0.87 & 15s \\\\\nPCA (50D) & 0.86 & 8s \\\\\nPCA (10D) & 0.79 & 2s \\\\\n\\hline\n\\end{tabular}\n\n\n\n\n<strong>Observations:</strong>\n<ul>\n\n<li>Sweet spot: 50-100 components\n</li>\n<li>Improved accuracy with PCA\n</li>\n<li>Much faster training\n</li>\n<li>Slight degradation with too few PCs\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Use Case}\nHigh-dimensional datasets where features are correlated (genomics, text, images).\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 36,
      "title": "Best Practices: Standardization",
      "readingTime": "1 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<div class=\"warning\"><h4>Critical Decision</h4>Should you standardize features before PCA?</div>\n\n\n\n<strong>Standardization:</strong>\n$$z_j = \\frac{x_j - \\mu_j}{\\sigma_j}$$\n\n\n\n<strong>When to Standardize:</strong>\n<ul>\n\n<li>Features have different units\n</li>\n<li>Features have different scales\n</li>\n<li>Want equal weight for all features\n</li>\n<li>Domain knowledge suggests equal importance\n</li>\n</ul>\n\n\n\n<strong>When NOT to Standardize:</strong>\n<ul>\n\n<li>Features already on same scale\n</li>\n<li>Scale carries information\n</li>\n<li>Domain knowledge: some features more important\n</li>\n<li>Interpretation requires original scale\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/standardization_importance.png]</em></p></div>\n\n\n\n<strong>Example Impact:</strong>\n\nDataset: Height (cm), Weight (kg), Age (years)\n\n\n\n<em>Without standardization:</em>\n<ul>\n\n<li>Height range: 150-200\n</li>\n<li>Weight range: 50-100\n</li>\n<li>Age range: 20-70\n</li>\n<li>PC1 dominated by height\n</li>\n</ul>\n\n\n\n<em>With standardization:</em>\n<ul>\n\n<li>All ranges: -2 to 2\n</li>\n<li>Equal opportunity for all\n</li>\n<li>More balanced PCs\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Recommendation}\n<strong>Default: Standardize unless you have good reason not to.</strong>\n\\end{exampleblock}\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 37,
      "title": "Computational Complexity",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Standard PCA Complexity:</strong>\n\n\n\n\\begin{tabular}{|l|c|}\n\\hline\n<strong>Operation</strong> & <strong>Complexity</strong> \\\\\n\\hline\nCentering & $\\mathcal{O}(nd)$ \\\\\nCovariance & $\\mathcal{O}(nd^2)$ \\\\\nEigendecomp & $\\mathcal{O}(d^3)$ \\\\\nSVD & $\\mathcal{O}(\\min(nd^2, n^2d))$ \\\\\n\\hline\n<strong>Total</strong> & $\\mathcal{O}(nd^2 + d^3)$ \\\\\n\\hline\n\\end{tabular}\n\n\n\n<strong>Memory Requirements:</strong>\n<ul>\n\n<li>Data: $\\mathcal{O}(nd)$\n</li>\n<li>Covariance: $\\mathcal{O}(d^2)$\n</li>\n<li>Eigenvectors: $\\mathcal{O}(d^2)$\n</li>\n<li><strong>Total:</strong> $\\mathcal{O}(nd + d^2)$\n</li>\n</ul>\n\n\n\n<strong>Scalability Issues:</strong>\n<ul>\n\n<li>Large $d$: Covariance matrix huge\n</li>\n<li>Large $n$: Memory for data matrix\n</li>\n<li>Both large: Computational bottleneck\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n\n<div class=\"figure\"><p><em>[Figure: ../figures/computational_complexity.png]</em></p></div>\n\n\n\n<strong>Solutions for Large-Scale:</strong>\n\n\n\n\\begin{exampleblock}{Incremental PCA}\n<ul>\n\n<li>Process mini-batches\n</li>\n<li>Memory: $\\mathcal{O}(bd + d^2)$\n</li>\n<li>Time: $\\mathcal{O}(ndk)$\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Randomized PCA}\n<ul>\n\n<li>Approximate top-$k$ PCs\n</li>\n<li>Time: $\\mathcal{O}(ndk)$\n</li>\n<li>Much faster for $k \\ll d$\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n\\begin{exampleblock}{Sparse PCA}\n<ul>\n\n<li>Exploit data sparsity\n</li>\n<li>Reduce effective dimensionality\n</li>\n</ul>\n\\end{exampleblock}\n\n\n\n<div class=\"warning\"><h4>Rule of Thumb</h4>Standard PCA: $n, d < 10,000$\\\\\nIncremental: $n > 100,000$\\\\\nRandomized: $d > 10,000$, $k \\ll d$</div>\n</div>\n</div>",
      "hasVisualization": true,
      "knowledgeCheck": null
    },
    {
      "id": 38,
      "title": "Common Pitfalls and How to Avoid Them",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>1. Data Leakage:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Fitting PCA on entire dataset including test set.</div>\n\n<strong>Correct approach:</strong>\n<ul>\n\n<li>Fit PCA only on training set\n</li>\n<li>Transform train and test separately\n</li>\n<li>Use same transformation for both\n</li>\n</ul>\n\n\n\n<strong>2. Forgetting to Center:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Applying PCA without centering data.</div>\n\n<strong>Why it matters:</strong>\n<ul>\n\n<li>PCA assumes zero mean\n</li>\n<li>Results will be incorrect\n</li>\n<li>Always center first!\n</li>\n</ul>\n\n\n\n<strong>3. Wrong Scaling Choice:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Not standardizing when features have different scales.</div>\n\n<strong>Impact:</strong>\n<ul>\n\n<li>PCs dominated by large-scale features\n</li>\n<li>Misleading variance explanation\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>4. Interpreting PCs as Features:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Assuming PCs have same meaning as original features.</div>\n\n<strong>Reality:</strong>\n<ul>\n\n<li>PCs are linear combinations\n</li>\n<li>May not have intuitive interpretation\n</li>\n<li>Use loadings for understanding\n</li>\n</ul>\n\n\n\n<strong>5. Assuming Linearity:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Applying PCA to manifold data with non-linear structure.</div>\n\n<strong>Solution:</strong>\n<ul>\n\n<li>Check for non-linearity first\n</li>\n<li>Consider Kernel PCA or manifold methods\n</li>\n</ul>\n\n\n\n<strong>6. Ignoring Outliers:</strong>\n\n<div class=\"warning\"><h4>Mistake</h4>Not handling outliers before PCA.</div>\n\n<strong>Impact:</strong>\n<ul>\n\n<li>PCs skewed by outliers\n</li>\n<li>Variance misrepresented\n</li>\n<li>Use robust PCA if needed\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 39,
      "title": "Interpreting PCA Results",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>What to Report:</strong>\n\n\n\n<ol>\n\n<li><strong>Number of components:</strong> $k$ chosen\n</li>\n<li><strong>Explained variance:</strong> Per component and cumulative\n</li>\n<li><strong>Scree plot:</strong> Visualize eigenvalue decay\n</li>\n<li><strong>Loading matrix:</strong> Top features per PC\n</li>\n<li><strong>Biplot:</strong> If $d$ is small\n</li>\n<li><strong>Reconstruction error:</strong> If applicable\n</li>\n</ol>\n\n\n\n<strong>Interpreting Loadings:</strong>\n\n<ul>\n\n<li>Examine largest magnitude loadings\n</li>\n<li>Group features by sign\n</li>\n<li>Name PC based on dominant features\n</li>\n<li>Example: \"Size component\", \"Age component\"\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Example}\nPC1 with high loadings on [height, weight, BMI]:\n<ul>\n\n<li>Interpretation: \"Body size\" component\n</li>\n<li>Positive values: Larger individuals\n</li>\n<li>Negative values: Smaller individuals\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n\n<div class=\"column\">\n<strong>Statistical Significance:</strong>\n\n\n\n<strong>Bootstrap approach:</strong>\n<ul>\n\n<li>Resample data multiple times\n</li>\n<li>Compute PCA on each sample\n</li>\n<li>Check stability of components\n</li>\n<li>Report confidence intervals\n</li>\n</ul>\n\n\n\n<strong>Permutation test:</strong>\n<ul>\n\n<li>Randomly permute features\n</li>\n<li>Compare eigenvalues to null distribution\n</li>\n<li>Test if variance is significant\n</li>\n</ul>\n\n\n\n<strong>Practical Checklist:</strong>\n\n<ul>\n\n<li>$\\checkmark$ Data centered/standardized?\n</li>\n<li>$\\checkmark$ Scree plot shows elbow?\n</li>\n<li>$\\checkmark$ Enough variance explained?\n</li>\n<li>$\\checkmark$ PCs interpretable?\n</li>\n<li>$\\checkmark$ No data leakage?\n</li>\n<li>$\\checkmark$ Outliers addressed?\n</li>\n<li>$\\checkmark$ Results validated?\n</li>\n</ul>\n\n\n\n<div class=\"warning\"><h4>Documentation</h4>Always document preprocessing choices and justification for $k$.</div>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 40,
      "title": "Key Takeaways",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Core Concepts:</strong>\n\n\n\n<ul>\n\n<li><strong>PCA:</strong> Linear dimensionality reduction via variance maximization\n</li>\n<li><strong>Principal components:</strong> Orthogonal directions of maximum variance\n</li>\n<li><strong>Eigendecomposition:</strong> Mathematical foundation\n</li>\n<li><strong>SVD:</strong> Practical computation method\n</li>\n<li><strong>Variance explained:</strong> Quantifies information retention\n</li>\n</ul>\n\n\n\n<strong>Key Steps:</strong>\n<ol>\n\n<li>Center (and optionally standardize) data\n</li>\n<li>Compute covariance or apply SVD\n</li>\n<li>Extract eigenvectors/singular vectors\n</li>\n<li>Project data onto top-$k$ components\n</li>\n<li>Evaluate and interpret results\n</li>\n</ol>\n\n\n\n<strong>Variants:</strong>\n<ul>\n\n<li>Kernel PCA: Non-linear extension\n</li>\n<li>Sparse PCA: Interpretable loadings\n</li>\n<li>Incremental PCA: Large-scale data\n</li>\n<li>Robust PCA: Handle outliers\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Applications:</strong>\n\n\n\n<ul>\n\n<li>Data visualization and exploration\n</li>\n<li>Image compression and processing\n</li>\n<li>Face recognition (eigenfaces)\n</li>\n<li>Noise filtering and denoising\n</li>\n<li>Feature engineering for ML\n</li>\n<li>Dimensionality reduction\n</li>\n</ul>\n\n\n\n<strong>Best Practices:</strong>\n<ul>\n\n<li>Always center data, standardize if needed\n</li>\n<li>Use scree plot and explained variance\n</li>\n<li>Avoid data leakage in train/test split\n</li>\n<li>Check for outliers and non-linearity\n</li>\n<li>Validate component selection\n</li>\n<li>Document all preprocessing choices\n</li>\n</ul>\n\n\n\n<strong>Limitations:</strong>\n<ul>\n\n<li>Assumes linear relationships\n</li>\n<li>Sensitive to outliers and scaling\n</li>\n<li>Loses interpretability\n</li>\n<li>May not preserve non-linear structure\n</li>\n</ul>\n</div>\n</div>",
      "hasVisualization": false,
      "knowledgeCheck": null
    },
    {
      "id": 41,
      "title": "Further Reading and Resources",
      "readingTime": "2 min",
      "content": "<div class=\"two-column\">\n<div class=\"column\">\n<strong>Classic Papers:</strong>\n\n\n\n<ul>\n\n<li>Pearson (1901): \"On lines and planes of closest fit\"\n</li>\n<li>Hotelling (1933): \"Analysis of complex statistical variables\"\n</li>\n<li>Turk \\& Pentland (1991): \"Eigenfaces for recognition\"\n</li>\n<li>Jolliffe (2002): \"Principal Component Analysis\" (book)\n</li>\n</ul>\n\n\n\n<strong>Advanced Topics:</strong>\n\n\n\n<ul>\n\n<li>Independent Component Analysis (ICA)\n</li>\n<li>Non-negative Matrix Factorization (NMF)\n</li>\n<li>t-SNE and UMAP for visualization\n</li>\n<li>Autoencoders for non-linear PCA\n</li>\n<li>Gaussian Process Latent Variable Models\n</li>\n</ul>\n\n\n\n<strong>Software Libraries:</strong>\n<ul>\n\n<li>\\texttt{scikit-learn}: PCA, KernelPCA, IncrementalPCA\n</li>\n<li>\\texttt{numpy/scipy}: Low-level linear algebra\n</li>\n<li>\\texttt{statsmodels}: Statistical PCA\n</li>\n</ul>\n</div>\n\n<div class=\"column\">\n<strong>Related Methods:</strong>\n\n\n\n<ul>\n\n<li><strong>Linear Discriminant Analysis (LDA):</strong> Supervised dimensionality reduction\n</li>\n<li><strong>Factor Analysis:</strong> Assumes latent variables\n</li>\n<li><strong>Canonical Correlation Analysis:</strong> Multi-view learning\n</li>\n<li><strong>Isomap:</strong> Geodesic distances\n</li>\n<li><strong>Locally Linear Embedding:</strong> Manifold learning\n</li>\n</ul>\n\n\n\n<strong>When to Use Alternatives:</strong>\n<ul>\n\n<li>Non-linear structure: Kernel PCA, manifold methods\n</li>\n<li>Labeled data: LDA, supervised methods\n</li>\n<li>Visualization only: t-SNE, UMAP\n</li>\n<li>Interpretability: Sparse methods, NMF\n</li>\n<li>Very large data: Random projections, sketching\n</li>\n</ul>\n\n\n\n\\begin{exampleblock}{Next Steps}\n<ul>\n\n<li>Practice on real datasets\n</li>\n<li>Compare with other methods\n</li>\n<li>Explore kernel and sparse variants\n</li>\n<li>Study deep learning autoencoders\n</li>\n</ul>\n\\end{exampleblock}\n</div>\n</div>\n\n\n\n\n<strong>Thank you for your attention!</strong>",
      "hasVisualization": false,
      "knowledgeCheck": null
    }
  ]
}