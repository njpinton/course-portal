{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC 178IP - Digital Image Processing\n",
    "# Final Examination\n",
    "\n",
    "**Student Name:** _______________________\n",
    "\n",
    "**Student Number:** _______________________\n",
    "\n",
    "**Date:** _______________________\n",
    "\n",
    "---\n",
    "\n",
    "## Exam Information\n",
    "\n",
    "| Item | Details |\n",
    "|------|---------|\n",
    "| **Total Points** | 100 points |\n",
    "| **Time Allocation** | 3-4 hours (self-paced) |\n",
    "| **Deadline** | 1 week from release |\n",
    "| **Format** | Jupyter Notebook + PDF export |\n",
    "\n",
    "## Exam Structure\n",
    "\n",
    "| Part | Topic | Points |\n",
    "|------|-------|--------|\n",
    "| Part 1 | Image Fundamentals | 20 |\n",
    "| Part 2 | Image Processing & Filtering | 20 |\n",
    "| Part 3 | Feature Extraction & Segmentation | 20 |\n",
    "| Part 4 | Deep Learning for Computer Vision | 20 |\n",
    "| Part 5 | Generative Models | 20 |\n",
    "| Bonus | Integrated Application | 10 |\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Answer all written questions** in the designated markdown cells\n",
    "2. **Complete all code cells** marked with `# TODO`\n",
    "3. **Run all cells** before submission (outputs must be visible)\n",
    "4. **Export to PDF** and submit both `.ipynb` and `.pdf`\n",
    "5. You may use course materials, textbooks, and online resources\n",
    "6. LLM usage (ChatGPT, Claude, etc.) is permitted - document what you learned\n",
    "7. An **oral examination** (10-15 minutes) will follow to verify understanding\n",
    "\n",
    "## Personalized Parameters\n",
    "\n",
    "Use your **student number as a random seed** for unique values:\n",
    "\n",
    "```python\n",
    "MY_SEED = int(\"YOUR_STUDENT_NUMBER\"[-6:])  # Last 6 digits\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Image processing\n",
    "from skimage import data, color, filters, morphology, measure, exposure, transform\n",
    "from skimage.util import random_noise\n",
    "from skimage.feature import canny\n",
    "from scipy import ndimage\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# ============================================================\n",
    "# PERSONALIZED SEED - REPLACE WITH YOUR STUDENT NUMBER!\n",
    "# ============================================================\n",
    "MY_SEED = int(\"123456\")  # TODO: REPLACE WITH YOUR STUDENT NUMBER!\n",
    "\n",
    "np.random.seed(MY_SEED)\n",
    "\n",
    "# Generate personalized parameters\n",
    "MY_NOISE_VAR = 0.01 + (MY_SEED % 100) / 2000\n",
    "MY_SP_AMOUNT = 0.03 + (MY_SEED % 50) / 1000\n",
    "MY_LATENT_DIM = 16 + (MY_SEED % 32)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"\\nYour personalized parameters:\")\n",
    "print(f\"   Noise variance: {MY_NOISE_VAR:.4f}\")\n",
    "print(f\"   S&P amount: {MY_SP_AMOUNT:.4f}\")\n",
    "print(f\"   Latent dimension: {MY_LATENT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def show_images(images, titles=None, cmap='gray', figsize=(15, 5)):\n",
    "    \"\"\"Display multiple images in a row.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
    "        if img.ndim == 2:\n",
    "            ax.imshow(img, cmap=cmap)\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Image Fundamentals (20 points)\n",
    "\n",
    "This section covers image representation, quantization, sampling, and color spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Image Representation and Quantization (6 points)\n",
    "\n",
    "A grayscale image uses 8-bit quantization (256 gray levels).\n",
    "\n",
    "**Q1.1a (2 pts):** If we reduce the quantization to 4 bits, how many gray levels would be available?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q1.1b (2 pts):** Describe TWO visible artifacts that would appear when reducing from 8-bit to 2-bit quantization.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q1.1c (2 pts):** Why does the human eye perceive these artifacts more strongly in smooth gradient regions than in textured regions?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Sampling and Aliasing (6 points)\n",
    "\n",
    "**Q1.2a (2 pts):** State the Nyquist-Shannon sampling theorem in your own words.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q1.2b (2 pts):** A camera captures an image of a striped shirt. The stripes have a spatial frequency of 50 cycles per cm. If the camera samples at 80 samples per cm, will aliasing occur? Explain your reasoning.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q1.2c (2 pts):** Describe ONE practical method to prevent aliasing in digital cameras.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Color Spaces (8 points)\n",
    "\n",
    "**Q1.3a (2 pts):** Why is YCbCr color space preferred over RGB for image/video compression?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q1.3b (2 pts):** JPEG compression applies 4:2:0 chroma subsampling. Explain what this means and why it's perceptually acceptable.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Color Space Exploration (4 pts)\n",
    "# Load and explore color images\n",
    "\n",
    "color_img = data.astronaut()\n",
    "\n",
    "# TODO: Convert to different color spaces and display\n",
    "gray_img = color.rgb2gray(color_img)\n",
    "hsv_img = color.rgb2hsv(color_img)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original RGB\n",
    "axes[0, 0].imshow(color_img)\n",
    "axes[0, 0].set_title('Original RGB')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Grayscale\n",
    "axes[0, 1].imshow(gray_img, cmap='gray')\n",
    "axes[0, 1].set_title('Grayscale')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# HSV - Hue channel\n",
    "axes[0, 2].imshow(hsv_img[:, :, 0], cmap='hsv')\n",
    "axes[0, 2].set_title('Hue (H)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# TODO: Display Saturation and Value channels\n",
    "# axes[1, 0].imshow(hsv_img[:, :, 1], cmap='gray')\n",
    "# axes[1, 0].set_title('Saturation (S)')\n",
    "\n",
    "# axes[1, 1].imshow(hsv_img[:, :, 2], cmap='gray')\n",
    "# axes[1, 1].set_title('Value (V)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Q: Why is HSV useful for color-based object detection?\n",
    "# Your answer in the markdown cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3c (4 pts):** Based on your exploration above, why is HSV color space useful for image processing tasks like object detection based on color? Give a specific example.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Image Processing & Filtering (20 points)\n",
    "\n",
    "This section covers convolution, frequency domain, histogram operations, and noise filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Convolution and Correlation (6 points)\n",
    "\n",
    "**Q2.1a (2 pts):** What is the fundamental difference between convolution and correlation? When does this difference matter?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2.1b (2 pts):** Given a 3x3 kernel, explain why we need to \"flip\" it for convolution but not for correlation.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2.1c (2 pts):** A separable 5x5 filter can be decomposed into two 1D filters. How many multiplications are saved when applying a separable filter to a 512x512 image compared to a non-separable filter? Show your calculation.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Frequency Domain Processing (6 points)\n",
    "\n",
    "**Q2.2a (2 pts):** What type of image features correspond to LOW frequencies in the Fourier domain? What about HIGH frequencies?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2.2b (2 pts):** A researcher applies an ideal low-pass filter (sharp cutoff) in the frequency domain. They observe \"ringing\" artifacts in the output image. Explain why this occurs.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2.2c (2 pts):** What filter shape would reduce ringing while still removing high frequencies?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Noise Filtering (8 pts)\n",
    "# Different types of noise require different filtering approaches\n",
    "\n",
    "original = data.camera() / 255.0\n",
    "\n",
    "# Create noisy versions using YOUR personalized parameters\n",
    "noisy_gaussian = random_noise(original, mode='gaussian', var=MY_NOISE_VAR)\n",
    "noisy_sp = random_noise(original, mode='s&p', amount=MY_SP_AMOUNT)\n",
    "\n",
    "print(f\"Your personalized noise levels:\")\n",
    "print(f\"   Gaussian variance: {MY_NOISE_VAR:.4f}\")\n",
    "print(f\"   S&P amount: {MY_SP_AMOUNT:.4f}\")\n",
    "\n",
    "show_images([original, noisy_gaussian, noisy_sp], \n",
    "            ['Original', 'Gaussian Noise', 'Salt & Pepper Noise'])\n",
    "\n",
    "# TODO: Apply appropriate filters for each noise type\n",
    "# For Gaussian noise: try Gaussian blur\n",
    "filtered_gaussian = None  # TODO: filters.gaussian(noisy_gaussian, sigma=?)\n",
    "\n",
    "# For Salt & Pepper: try median filter\n",
    "filtered_sp = None  # TODO: filters.median(noisy_sp, morphology.disk(?))\n",
    "\n",
    "# TODO: Display filtered results and compare\n",
    "# show_images([noisy_gaussian, filtered_gaussian, noisy_sp, filtered_sp],\n",
    "#             ['Gaussian Noise', 'Filtered', 'S&P Noise', 'Filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2d (4 pts):** Why does median filter work better for salt-and-pepper noise than Gaussian blur? What would happen if you used Gaussian blur on S&P noise?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2.2e (4 pts):** Why is a bilateral filter often preferred over Gaussian blur for denoising photographs of faces?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Feature Extraction & Segmentation (20 points)\n",
    "\n",
    "This section covers edge detection, feature descriptors, and image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Edge Detection (6 points)\n",
    "\n",
    "**Q3.1a (3 pts):** The Canny edge detector uses non-maximum suppression. What is its purpose and how does it improve edge detection compared to simple thresholding?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3.1b (3 pts):** Compare the Sobel operator and the Laplacian of Gaussian (LoG) for edge detection. When would you choose one over the other?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Edge Detection (6 pts)\n",
    "\n",
    "edge_image = data.camera() / 255.0\n",
    "\n",
    "# TODO: Apply Sobel edge detection\n",
    "sobel_x = filters.sobel_h(edge_image)  # Horizontal edges\n",
    "sobel_y = filters.sobel_v(edge_image)  # Vertical edges\n",
    "sobel_magnitude = None  # TODO: Calculate magnitude = sqrt(sobel_x^2 + sobel_y^2)\n",
    "\n",
    "# TODO: Apply Canny edge detection with different sigma values\n",
    "canny_sigma1 = None  # TODO: canny(edge_image, sigma=1)\n",
    "canny_sigma3 = None  # TODO: canny(edge_image, sigma=3)\n",
    "\n",
    "# Display results\n",
    "# show_images([edge_image, sobel_magnitude, canny_sigma1, canny_sigma3],\n",
    "#             ['Original', 'Sobel Magnitude', 'Canny σ=1', 'Canny σ=3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1c (2 pts):** How does the sigma parameter in Canny affect the results? What happens with σ=1 vs σ=3?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Descriptors (4 points)\n",
    "\n",
    "**Q3.2a (2 pts):** SIFT (Scale-Invariant Feature Transform) is described as \"invariant to scale and rotation.\" Explain how SIFT achieves scale invariance.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3.2b (2 pts):** Why are feature descriptors like SIFT/ORB useful for image stitching (creating panoramas)? What could go wrong if the images have very different lighting conditions?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Segmentation (4 points)\n",
    "\n",
    "**Q3.3a (2 pts):** Otsu's thresholding automatically selects a threshold value. What criterion does it optimize, and why might it fail on images with uneven illumination?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3.3b (2 pts):** Compare region-based segmentation (e.g., region growing) with edge-based segmentation. Give one advantage and one disadvantage of each approach.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Thresholding & Segmentation (6 pts)\n",
    "\n",
    "coins = data.coins()\n",
    "\n",
    "# TODO: Apply different thresholding methods\n",
    "\n",
    "# 1. Manual threshold\n",
    "threshold_manual = 100\n",
    "binary_manual = coins > threshold_manual\n",
    "\n",
    "# 2. Otsu's automatic threshold\n",
    "threshold_otsu = None  # TODO: filters.threshold_otsu(coins)\n",
    "binary_otsu = None  # TODO: coins > threshold_otsu\n",
    "\n",
    "# 3. Adaptive (local) threshold\n",
    "binary_adaptive = None  # TODO: filters.threshold_local(coins, block_size=35)\n",
    "\n",
    "# Display results\n",
    "# show_images([coins, binary_manual, binary_otsu, binary_adaptive],\n",
    "#             ['Original', f'Manual (t={threshold_manual})', \n",
    "#              f'Otsu (t={threshold_otsu})', 'Adaptive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Deep Learning for Computer Vision (20 points)\n",
    "\n",
    "This section covers CNN architectures, training, and object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 CNN Fundamentals (8 points)\n",
    "\n",
    "**Q4.1a (3 pts):** A convolutional layer uses 32 filters of size 3x3 on an input with 3 channels (RGB). How many learnable parameters does this layer have (including biases)? Show your calculation.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q4.1b (3 pts):** Explain the purpose of pooling layers in CNNs. What is the trade-off between using max pooling vs. average pooling?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q4.1c (2 pts):** Why do Conv2D layers have far fewer parameters than Dense layers, even though they process the entire image?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: CNN Parameter Calculation (4 pts)\n",
    "# Consider this simple CNN for CIFAR-10 (32x32x3 input, 10 classes)\n",
    "\n",
    "\"\"\"\n",
    "Layer 1: Conv2D(filters=32, kernel_size=3x3, input_channels=3)\n",
    "Layer 2: MaxPool2D(pool_size=2x2)\n",
    "Layer 3: Conv2D(filters=64, kernel_size=3x3)\n",
    "Layer 4: MaxPool2D(pool_size=2x2)\n",
    "Layer 5: Flatten\n",
    "Layer 6: Dense(128)\n",
    "Layer 7: Dense(10)  # Output layer\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Calculate output shape after each layer\n",
    "print(\"=== Output Shapes ===\")\n",
    "print(\"Input:                32 × 32 × 3\")\n",
    "print(\"After Conv2D(32,3×3): ___ × ___ × ___\")  # TODO\n",
    "print(\"After MaxPool(2×2):   ___ × ___ × ___\")  # TODO\n",
    "print(\"After Conv2D(64,3×3): ___ × ___ × ___\")  # TODO\n",
    "print(\"After MaxPool(2×2):   ___ × ___ × ___\")  # TODO\n",
    "print(\"After Flatten:        ___\")              # TODO\n",
    "\n",
    "# TODO: Calculate parameters for each layer\n",
    "print(\"\\n=== Parameters ===\")\n",
    "print(\"Conv2D(32, 3×3, in=3):  (3 × 3 × 3 + 1) × 32 = ___\")  # TODO\n",
    "print(\"Conv2D(64, 3×3, in=32): (3 × 3 × ___ + 1) × 64 = ___\")  # TODO\n",
    "print(\"Dense(128):             (___ + 1) × 128 = ___\")  # TODO\n",
    "print(\"Dense(10):              (128 + 1) × 10 = ___\")  # TODO\n",
    "print(\"\\nTotal parameters: ___\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training Deep Networks (4 points)\n",
    "\n",
    "**Q4.2a (2 pts):** A student trains a CNN on a small dataset of 500 images. The training accuracy reaches 99% but validation accuracy is only 60%. Diagnose the problem and suggest TWO concrete solutions.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q4.2b (2 pts):** Explain how transfer learning works and why it's especially valuable when you have limited training data.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Object Detection and Segmentation (4 points)\n",
    "\n",
    "**Q4.3a (2 pts):** Compare two-stage detectors (like Faster R-CNN) with one-stage detectors (like YOLO). What is the main trade-off between them?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q4.3b (2 pts):** What is the difference between semantic segmentation and instance segmentation? Give an example scenario where instance segmentation would be necessary but semantic segmentation would be insufficient.\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Generative Models (20 points)\n",
    "\n",
    "This section covers autoencoders, VAEs, and GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Autoencoders and VAEs (10 points)\n",
    "\n",
    "**Q5.1a (3 pts):** A standard autoencoder can compress and reconstruct images, but it's not good for generating NEW images. Explain why.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q5.1b (3 pts):** How does a Variational Autoencoder (VAE) solve this problem? Specifically, explain the role of the KL divergence term in the VAE loss function.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q5.1c (2 pts):** In β-VAE, we use β > 1 to weight the KL term more heavily. What is the trade-off when increasing β?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: Autoencoder Architecture Design (2 pts)\n",
    "# Design an autoencoder for MNIST (28x28 = 784 pixels)\n",
    "\n",
    "print(f\"Your personalized latent dimension: {MY_LATENT_DIM}\")\n",
    "print(f\"Design your autoencoder to compress to {MY_LATENT_DIM} dimensions\\n\")\n",
    "\n",
    "# TODO: Define your architecture and calculate parameters\n",
    "\"\"\"\n",
    "Encoder:\n",
    "    Input(784) \n",
    "    → Dense(?) + ReLU\n",
    "    → Dense(?) + ReLU  \n",
    "    → Dense(MY_LATENT_DIM)\n",
    "\n",
    "Decoder:\n",
    "    Input(MY_LATENT_DIM)\n",
    "    → Dense(?) + ReLU\n",
    "    → Dense(?) + ReLU\n",
    "    → Dense(784) + Sigmoid\n",
    "\"\"\"\n",
    "\n",
    "print(\"Describe your architecture and calculate total parameters:\")\n",
    "# Your calculation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Generative Adversarial Networks (10 points)\n",
    "\n",
    "**Q5.2a (3 pts):** Describe the \"adversarial game\" between the generator and discriminator in a GAN. What is each network trying to optimize?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q5.2b (3 pts):** \"Mode collapse\" is a common problem in GAN training. What is mode collapse and what causes it?\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical: GAN Training Dynamics Analysis (4 pts)\n",
    "# Analyze these simulated training curves\n",
    "\n",
    "np.random.seed(42)\n",
    "epochs = 100\n",
    "\n",
    "# Scenario A: Healthy training\n",
    "d_loss_healthy = 0.7 - 0.2 * (1 - np.exp(-np.arange(epochs)/30)) + 0.05 * np.random.randn(epochs)\n",
    "g_loss_healthy = 2.0 - 1.3 * (1 - np.exp(-np.arange(epochs)/40)) + 0.08 * np.random.randn(epochs)\n",
    "\n",
    "# Scenario B: Mode collapse  \n",
    "d_loss_collapse = np.concatenate([0.7 - 0.3 * np.arange(30)/30, np.ones(70) * 0.1 + 0.02 * np.random.randn(70)])\n",
    "g_loss_collapse = np.concatenate([2.0 - 0.5 * np.arange(30)/30, np.ones(70) * 0.3 + 0.05 * np.random.randn(70)])\n",
    "\n",
    "# Scenario C: Discriminator too strong\n",
    "d_loss_strong_d = 0.7 * np.exp(-np.arange(epochs)/10) + 0.02 * np.random.randn(epochs)\n",
    "g_loss_strong_d = 2.0 + 0.5 * np.log(1 + np.arange(epochs)/20) + 0.1 * np.random.randn(epochs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(d_loss_healthy, 'b-', label='D Loss')\n",
    "axes[0].plot(g_loss_healthy, 'r-', label='G Loss')\n",
    "axes[0].set_title('Scenario A')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(d_loss_collapse, 'b-', label='D Loss')\n",
    "axes[1].plot(g_loss_collapse, 'r-', label='G Loss')\n",
    "axes[1].set_title('Scenario B')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].plot(d_loss_strong_d, 'b-', label='D Loss')\n",
    "axes[2].plot(g_loss_strong_d, 'r-', label='G Loss')\n",
    "axes[2].set_title('Scenario C')\n",
    "axes[2].legend()\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.2c (4 pts):** For each scenario (A, B, C), describe what is happening during training and whether it represents healthy or problematic training.\n",
    "\n",
    "*Scenario A:*\n",
    "\n",
    "*Scenario B:*\n",
    "\n",
    "*Scenario C:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Comparing Generative Models\n",
    "\n",
    "Complete this comparison table:\n",
    "\n",
    "| Aspect | Autoencoder | VAE | GAN |\n",
    "|--------|-------------|-----|-----|\n",
    "| Training stability | | | |\n",
    "| Output quality (sharpness) | | | |\n",
    "| Latent space interpolation | | | |\n",
    "| Can generate new samples? | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: Integrated Application (10 points)\n",
    "\n",
    "You are tasked with building an automated quality control system for a manufacturing line that produces printed circuit boards (PCBs). The system must detect defects such as missing components, misaligned parts, and solder bridges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a complete image processing pipeline that addresses this problem. Your answer should include:\n",
    "\n",
    "**a) Image acquisition considerations (lighting, camera setup) (2 pts)**\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**b) Preprocessing steps to normalize images and reduce noise (2 pts)**\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**c) The main detection approach (traditional CV, deep learning, or hybrid) with justification (4 pts)**\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**d) How you would handle the challenge of limited defect samples for training (2 pts)**\n",
    "\n",
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LLM Usage Log\n",
    "\n",
    "Document all LLM interactions. Be honest - this helps you reflect on your learning.\n",
    "\n",
    "| Question/Task | LLM Used | What I LEARNED |\n",
    "|---------------|----------|----------------|\n",
    "| Example: \"How does KL divergence work\" | ChatGPT | I learned it measures how different two distributions are |\n",
    "| | | |\n",
    "| | | |\n",
    "| | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Submission Checklist\n",
    "\n",
    "- [ ] Student name and ID filled in at the top\n",
    "- [ ] MY_SEED replaced with YOUR student number\n",
    "- [ ] All code cells executed (outputs visible)\n",
    "- [ ] All written questions answered\n",
    "- [ ] All TODO items completed\n",
    "- [ ] LLM usage documented\n",
    "- [ ] Notebook exported as PDF\n",
    "- [ ] Both `.ipynb` and `.pdf` ready for submission\n",
    "\n",
    "**File naming:** `LastName_FirstName_Finals.ipynb` and `.pdf`\n",
    "\n",
    "---\n",
    "\n",
    "## UP Honor Code Statement\n",
    "\n",
    "*\"Honor and Excellence\" (Karangalan at Kahusayan)*\n",
    "\n",
    "By submitting this exam, I affirm that:\n",
    "1. All answers represent my own understanding\n",
    "2. I have documented all external help received\n",
    "3. I can explain and defend any answer I wrote\n",
    "\n",
    "**Student Signature:** ______________________ \n",
    "\n",
    "**Date:** __________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
