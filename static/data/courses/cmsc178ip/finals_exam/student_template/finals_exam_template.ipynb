{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CMSC 178IP - Digital Image Processing\n# Final Examination (Practical)\n\n**Student Name:** _______________________\n\n**Student Number:** _______________________\n\n**Date:** _______________________\n\n---\n\n## Exam Information\n\n| Item | Details |\n|------|---------|\n| **Total Points** | 100 (90 base + 10 bonus) |\n| **Time Allocation** | 2-3 hours (self-paced) |\n| **Deadline** | 1 week from release |\n| **Format** | Jupyter Notebook + PDF export |\n\n## Exam Structure (Ordered by Complexity)\n\n| Part | Topic | Points | Difficulty |\n|------|-------|--------|------------|\n| Part 0 | Image Representation & Basics | 20 | ‚≠ê Easiest |\n| Part 1 | Spatial Operations (Filtering, Edges, Thresholding) | 25 | ‚≠ê‚≠ê Medium |\n| Part 2 | CNN Architecture Analysis | 25 | ‚≠ê‚≠ê‚≠ê Medium-Hard |\n| Part 3 | Generative Models | 20 | ‚≠ê‚≠ê‚≠ê‚≠ê Hardest |\n| Bonus | End-to-End Application | 10 | Applied |\n\n## Instructions\n\n1. **Complete all code cells** marked with `# TODO`\n2. **Answer all analysis questions** in the designated markdown cells\n3. **Complete all COMPARISON requirements** - try multiple approaches where asked\n4. **Answer all REFLECTION questions** honestly - these help you learn!\n5. **Run all cells** before submission (outputs must be visible)\n6. **Export to PDF** and submit both `.ipynb` and `.pdf`\n7. **Document LLM usage** including **what you learned** from each interaction\n8. **Use YOUR student number as seed** for randomization (see below)\n9. **Document your process** - what you tried, what failed, what you learned\n\n## üé≤ Personalized Parameters (REQUIRED)\n\nTo ensure each student has unique values, use your **student number as a random seed**:\n\n```python\nMY_SEED = int(\"YOUR_STUDENT_NUMBER\"[-6:])  # Last 6 digits of your student number\n```\n\n## Grading Breakdown\n\n- **Implementation (40%):** Does your code work correctly?\n- **Analysis (40%):** Do you understand WHY it works?\n- **Reflection & Comparison (20%):** Did you explore alternatives and reflect on your learning?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Image processing\nfrom skimage import data, color, filters, morphology, measure, exposure, transform\nfrom skimage.util import random_noise\nfrom scipy import ndimage\nfrom scipy.signal import convolve2d\n\n# Deep learning (for analysis - no training required)\ntry:\n    import torch\n    import torch.nn as nn\n    TORCH_AVAILABLE = True\nexcept ImportError:\n    print(\"PyTorch not available - some cells will use numpy alternatives\")\n    TORCH_AVAILABLE = False\n\n# Utilities\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Display settings\nplt.rcParams['figure.figsize'] = [10, 6]\nplt.rcParams['figure.dpi'] = 100\n\n# ============================================================\n# üé≤ PERSONALIZED SEED - REPLACE WITH YOUR STUDENT NUMBER!\n# ============================================================\n# Replace \"123456\" with the LAST 6 DIGITS of your student number\n# Example: If your student number is 2020-12345, use \"012345\"\n\nMY_SEED = int(\"123456\")  # TODO: REPLACE WITH YOUR STUDENT NUMBER!\n\nnp.random.seed(MY_SEED)\n\n# Generate your personalized parameters\nMY_NOISE_VAR = 0.01 + (MY_SEED % 100) / 2000  # Noise variance: 0.01-0.06\nMY_SP_AMOUNT = 0.03 + (MY_SEED % 50) / 1000   # S&P amount: 0.03-0.08\nMY_LATENT_DIM = 16 + (MY_SEED % 32)           # Latent dim: 16-48\n\nprint(\"‚úÖ All imports successful!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"\\nüé≤ Your personalized parameters:\")\nprint(f\"   Noise variance: {MY_NOISE_VAR:.4f}\")\nprint(f\"   S&P amount: {MY_SP_AMOUNT:.4f}\")\nprint(f\"   Latent dimension: {MY_LATENT_DIM}\")\nprint(f\"\\n‚ö†Ô∏è  Did you replace MY_SEED with YOUR student number?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for displaying images\n",
    "def show_images(images, titles=None, cmap='gray', figsize=(15, 5)):\n",
    "    \"\"\"Display multiple images in a row.\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
    "        if img.ndim == 2:\n",
    "            ax.imshow(img, cmap=cmap)\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_psnr(original, processed):\n",
    "    \"\"\"Calculate Peak Signal-to-Noise Ratio.\"\"\"\n",
    "    mse = np.mean((original - processed) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0 if original.max() <= 1 else 255.0\n",
    "    return 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 0: Image Representation & Basics (20 points) ‚≠ê\n\n**Time Allocation:** 25-30 minutes | **Difficulty:** Easiest\n\nThis section covers fundamental concepts: how images are represented, color spaces, and basic properties."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0.1 Image Properties & Data Types (10 points)\n\nUnderstanding how images are stored and represented is fundamental to image processing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load sample images\ngrayscale_img = data.camera()  # Grayscale image\ncolor_img = data.astronaut()   # Color (RGB) image\n\n# TODO: Explore image properties\nprint(\"=== Grayscale Image Properties ===\")\nprint(f\"Shape: {grayscale_img.shape}\")\nprint(f\"Data type: {grayscale_img.dtype}\")\nprint(f\"Min value: {grayscale_img.min()}\")\nprint(f\"Max value: {grayscale_img.max()}\")\nprint(f\"Total pixels: {grayscale_img.size}\")\n\nprint(\"\\n=== Color Image Properties ===\")\nprint(f\"Shape: {color_img.shape}\")\nprint(f\"Data type: {color_img.dtype}\")\n# TODO: Print min, max, and explain what the 3rd dimension represents\n\n# Display both images\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(grayscale_img, cmap='gray')\naxes[0].set_title(f'Grayscale: {grayscale_img.shape}')\naxes[0].axis('off')\naxes[1].imshow(color_img)\naxes[1].set_title(f'Color (RGB): {color_img.shape}')\naxes[1].axis('off')\nplt.tight_layout()\nplt.show()\n\n# TODO: Extract and display individual color channels\n# red_channel = color_img[:, :, 0]\n# green_channel = color_img[:, :, 1]\n# blue_channel = color_img[:, :, 2]\n# Display each channel as a grayscale image"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Color Space Conversions\n# Convert the color image to different color spaces\n\n# RGB to Grayscale\ngray_from_rgb = color.rgb2gray(color_img)\n\n# RGB to HSV (Hue, Saturation, Value)\nhsv_img = color.rgb2hsv(color_img)\n\n# TODO: Display the original and converted images\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Original RGB\naxes[0, 0].imshow(color_img)\naxes[0, 0].set_title('Original RGB')\naxes[0, 0].axis('off')\n\n# Grayscale\naxes[0, 1].imshow(gray_from_rgb, cmap='gray')\naxes[0, 1].set_title('Grayscale')\naxes[0, 1].axis('off')\n\n# HSV - Hue channel\naxes[0, 2].imshow(hsv_img[:, :, 0], cmap='hsv')\naxes[0, 2].set_title('Hue (H)')\naxes[0, 2].axis('off')\n\n# TODO: Display Saturation and Value channels\n# axes[1, 0].imshow(hsv_img[:, :, 1], cmap='gray')\n# axes[1, 0].set_title('Saturation (S)')\n\n# axes[1, 1].imshow(hsv_img[:, :, 2], cmap='gray')\n# axes[1, 1].set_title('Value (V)')\n\n# TODO: What happens when you modify just the Hue?\n# modified_hsv = hsv_img.copy()\n# modified_hsv[:, :, 0] = (modified_hsv[:, :, 0] + 0.5) % 1.0  # Shift hue\n# modified_rgb = color.hsv2rgb(modified_hsv)\n# axes[1, 2].imshow(modified_rgb)\n# axes[1, 2].set_title('Hue Shifted')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analysis 0.1 (5 points)\n\n**Q1:** A grayscale image has shape `(512, 512)` and a color image has shape `(512, 512, 3)`. Explain what each dimension represents.\n\n*Your answer:*\n\n\n**Q2:** If an image has dtype `uint8` with values 0-255, and another has dtype `float64` with values 0.0-1.0, are they representing the same information? How would you convert between them?\n\n*Your answer:*\n\n\n**Q3:** Why is HSV color space useful for image processing tasks like object detection based on color? Give a specific example.\n\n*Your answer:*"
  },
  {
   "cell_type": "markdown",
   "source": "## 0.2 Histograms & Intensity Distribution (10 points)\n\nHistograms show the distribution of pixel intensities and are essential for understanding image characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load images with different characteristics\ndark_img = exposure.adjust_gamma(data.camera(), gamma=2.0)  # Darker\nbright_img = exposure.adjust_gamma(data.camera(), gamma=0.5)  # Brighter\nlow_contrast = exposure.rescale_intensity(data.camera(), out_range=(80, 180))\nnormal_img = data.camera()\n\n# TODO: Plot histograms for each image\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\n\nimages = [dark_img, normal_img, bright_img, low_contrast]\ntitles = ['Dark Image', 'Normal Image', 'Bright Image', 'Low Contrast']\n\nfor i, (img, title) in enumerate(zip(images, titles)):\n    # Display image\n    axes[0, i].imshow(img, cmap='gray', vmin=0, vmax=255)\n    axes[0, i].set_title(title)\n    axes[0, i].axis('off')\n    \n    # TODO: Plot histogram\n    # axes[1, i].hist(img.ravel(), bins=256, range=(0, 256), color='gray', alpha=0.7)\n    # axes[1, i].set_xlabel('Pixel Value')\n    # axes[1, i].set_ylabel('Frequency')\n    # axes[1, i].set_xlim(0, 256)\n\nplt.tight_layout()\nplt.show()\n\n# TODO: Calculate basic statistics for each image\nfor img, title in zip(images, titles):\n    print(f\"{title}: Mean={img.mean():.1f}, Std={img.std():.1f}, Min={img.min()}, Max={img.max()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis 0.2 (5 points)\n\n**Q1:** By looking at a histogram, how can you tell if an image is: (a) too dark, (b) too bright, (c) low contrast?\n\n*Your answer:*\n\n\n**Q2:** What does the standard deviation of pixel values tell you about an image? What would a very low standard deviation indicate?\n\n*Your answer:*\n\n\n### Comparison Requirement\n\n**Compare the histograms** of the four images above. For each, describe in 1-2 sentences what the histogram shape tells you about the image.\n\n| Image | Histogram Description |\n|-------|----------------------|\n| Dark Image | |\n| Normal Image | |\n| Bright Image | |\n| Low Contrast | |\n\n---\n\n## Part 0 Reflection (Required)\n\n**What was the MOST intuitive concept in this section?**\n\n*Your answer:*\n\n\n**If you had to explain \"color space\" to a friend, what analogy would you use?**\n\n*Your answer:*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 0.3 Introduction to Kernels (5 points)\n\nA **kernel** (or filter) is a small matrix that slides over an image to produce effects like blurring, sharpening, or edge detection. This is the foundation of convolution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exploring basic kernels - see what different small matrices do to an image\nfrom scipy.signal import convolve2d\n\n# Load a test image\ntest_img = data.camera() / 255.0\n\n# Define some common kernels\nkernels = {\n    'Identity': np.array([[0, 0, 0],\n                          [0, 1, 0],\n                          [0, 0, 0]]),\n    \n    'Box Blur (3x3)': np.array([[1, 1, 1],\n                                 [1, 1, 1],\n                                 [1, 1, 1]]) / 9,\n    \n    'Sharpen': np.array([[0, -1, 0],\n                         [-1, 5, -1],\n                         [0, -1, 0]]),\n    \n    'Edge Detect (Horizontal)': np.array([[-1, -2, -1],\n                                           [0, 0, 0],\n                                           [1, 2, 1]]),\n    \n    'Edge Detect (Vertical)': np.array([[-1, 0, 1],\n                                         [-2, 0, 2],\n                                         [-1, 0, 1]])\n}\n\n# Apply each kernel and display results\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Original image\naxes[0].imshow(test_img, cmap='gray')\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\n# Apply kernels\nfor i, (name, kernel) in enumerate(kernels.items()):\n    if i >= 5:\n        break\n    result = convolve2d(test_img, kernel, mode='same', boundary='symm')\n    # Clip for display (edge detection can have negative values)\n    axes[i+1].imshow(np.clip(result, 0, 1), cmap='gray')\n    axes[i+1].set_title(f'{name}')\n    axes[i+1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# TODO: Print one of the kernels to see its values\nprint(\"Example: Sharpen kernel:\")\nprint(kernels['Sharpen'])\nprint(\"\\nNotice how the center value (5) is larger than the sum of neighbors (-4)\")\nprint(\"This emphasizes the center pixel while subtracting surrounding values.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis 0.3 (2 points)\n\n**Q1:** Looking at the results above, describe what each kernel does to the image:\n- **Box Blur:** \n- **Sharpen:** \n- **Edge Detect (Horizontal):** \n\n*Your answer:*\n\n\n**Q2:** The sharpen kernel has a center value of 5 and surrounding values that sum to -4. Why do these values create a sharpening effect?\n\n*Your answer:*\n\n\n**Q3:** Why do edge detection kernels produce mostly dark images with bright lines where edges exist?\n\n*Your answer:*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 1: Spatial Operations (25 points) ‚≠ê‚≠ê\n\n**Time Allocation:** 40-50 minutes | **Difficulty:** Medium\n\nThis section covers spatial filtering, edge detection, and basic segmentation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.1 Noise Filtering (8 points)\n\nDifferent types of noise require different filtering approaches."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load test image and create noisy versions\noriginal = data.camera() / 255.0\n\n# Create noisy versions using YOUR personalized parameters\nnoisy_gaussian = random_noise(original, mode='gaussian', var=MY_NOISE_VAR)\nnoisy_sp = random_noise(original, mode='s&p', amount=MY_SP_AMOUNT)\n\nprint(f\"üé≤ Your personalized noise levels:\")\nprint(f\"   Gaussian variance: {MY_NOISE_VAR:.4f}\")\nprint(f\"   S&P amount: {MY_SP_AMOUNT:.4f}\")\n\nshow_images([original, noisy_gaussian, noisy_sp], \n            ['Original', 'Gaussian Noise', 'Salt & Pepper Noise'])\n\n# TODO: Apply appropriate filters for each noise type\n# For Gaussian noise: try Gaussian blur or bilateral filter\nfiltered_gaussian = None  # TODO: filters.gaussian(noisy_gaussian, sigma=?)\n\n# For Salt & Pepper: try median filter\nfiltered_sp = None  # TODO: filters.median(noisy_sp, morphology.disk(?))\n\n# TODO: Display filtered results\n# show_images([noisy_gaussian, filtered_gaussian, noisy_sp, filtered_sp],\n#             ['Gaussian Noise', 'Filtered', 'S&P Noise', 'Filtered'],\n#             figsize=(16, 4))"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Analysis 1.1 (3 points)\n\n**Q1:** What type of noise is each (Gaussian vs S&P)? How can you visually identify them?\n\n*Your answer:*\n\n\n**Q2:** Why does median filter work better for S&P noise than Gaussian blur?\n\n*Your answer:*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.2 Edge Detection (8 points)\n\nEdges represent boundaries between regions of different intensities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Edge Detection using Sobel and Canny operators\nfrom skimage.feature import canny\n\nedge_image = data.camera() / 255.0\n\n# TODO: Apply Sobel edge detection\nsobel_x = filters.sobel_h(edge_image)  # Horizontal edges\nsobel_y = filters.sobel_v(edge_image)  # Vertical edges\nsobel_magnitude = None  # TODO: Calculate magnitude = sqrt(sobel_x^2 + sobel_y^2)\n\n# TODO: Apply Canny edge detection with different sigma values\ncanny_sigma1 = None  # TODO: canny(edge_image, sigma=1)\ncanny_sigma3 = None  # TODO: canny(edge_image, sigma=3)\n\n# Display results\n# show_images([edge_image, sobel_magnitude, canny_sigma1, canny_sigma3],\n#             ['Original', 'Sobel Magnitude', 'Canny œÉ=1', 'Canny œÉ=3'],\n#             figsize=(16, 4))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analysis 1.2 (3 points)\n\n**Q1:** What is the difference between Sobel and Canny edge detection?\n\n*Your answer:*\n\n\n**Q2:** How does the sigma parameter in Canny affect the results?\n\n*Your answer:*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1.3 Thresholding & Segmentation (9 points)\n\nThresholding converts grayscale images to binary, separating foreground from background."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load coins image for thresholding\ncoins = data.coins()\n\n# TODO: Apply different thresholding methods\n\n# 1. Manual threshold\nthreshold_manual = 100\nbinary_manual = coins > threshold_manual\n\n# 2. Otsu's automatic threshold\nthreshold_otsu = None  # TODO: filters.threshold_otsu(coins)\nbinary_otsu = None  # TODO: coins > threshold_otsu\n\n# 3. Adaptive (local) threshold\n# TODO: filters.threshold_local(coins, block_size=35)\nbinary_adaptive = None\n\n# Display results\n# show_images([coins, binary_manual, binary_otsu, binary_adaptive],\n#             ['Original', f'Manual (t={threshold_manual})', \n#              f'Otsu (t={threshold_otsu})', 'Adaptive'],\n#             figsize=(16, 4))\n\n# Try different manual thresholds for comparison\nfor t in [80, 100, 120, 140]:\n    binary = coins > t\n    # Count approximate number of \"coin pixels\"\n    # print(f\"Threshold {t}: {np.sum(binary)} foreground pixels\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analysis 1.3 (4 points)\n\n**Q1:** Why does Otsu's method select the threshold it does? What is it optimizing?\n\n*Your answer:*\n\n\n**Q2:** When would adaptive thresholding be preferred over Otsu's?\n\n*Your answer:*\n\n\n### Comparison Requirement\n\nTry **3 different manual thresholds**. Document your observations:\n\n| Threshold | Effect on Coins | Effect on Background |\n|-----------|-----------------|---------------------|\n| 80 | | |\n| 100 | | |\n| 120 | | |\n\n---\n\n## Part 1 Reflection (Required)\n\n**Which technique (filtering, edge detection, or thresholding) was EASIEST to understand?**\n\n*Your answer:*\n\n\n**Which was HARDEST? Why?**\n\n*Your answer:*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 2: CNN Architecture Analysis (25 points) ‚≠ê‚≠ê‚≠ê\n\n**Time Allocation:** 40-50 minutes | **Difficulty:** Medium-Hard\n\nThis section tests your understanding of Convolutional Neural Networks."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Convolution Operation (10 points)\n\nImplement 2D convolution from scratch to understand how CNNs work."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def convolve2d_manual(image, kernel):\n    \"\"\"\n    Implement 2D convolution from scratch.\n    \n    This is how CNN convolution layers work internally!\n    \n    Parameters:\n    -----------\n    image : 2D numpy array\n        Input grayscale image\n    kernel : 2D numpy array\n        Convolution kernel (filter)\n    \n    Returns:\n    --------\n    output : 2D numpy array\n        Convolved image (same size as input)\n    \n    Steps:\n    1. Flip the kernel (required for true convolution)\n    2. Pad the image with zeros\n    3. Slide kernel over padded image\n    4. At each position: element-wise multiply and sum\n    \"\"\"\n    # Get dimensions\n    img_h, img_w = image.shape\n    k_h, k_w = kernel.shape\n    \n    # TODO: Step 1 - Flip the kernel (180 degree rotation)\n    # Hint: np.flip(kernel) or kernel[::-1, ::-1]\n    flipped_kernel = None  # TODO: Flip the kernel\n    \n    # TODO: Step 2 - Calculate padding needed for 'same' output size\n    # For 'same' convolution, we need padding of (kernel_size - 1) / 2\n    pad_h = k_h // 2\n    pad_w = k_w // 2\n    \n    # TODO: Step 3 - Zero-pad the image\n    # Hint: np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n    padded = None  # TODO: Pad the image\n    \n    # TODO: Step 4 - Create output array\n    output = np.zeros((img_h, img_w))\n    \n    # TODO: Step 5 - Perform convolution\n    # For each output pixel position (i, j):\n    #   - Extract the region from padded image\n    #   - Multiply element-wise with flipped kernel\n    #   - Sum all values\n    \n    # for i in range(img_h):\n    #     for j in range(img_w):\n    #         region = padded[i:i+k_h, j:j+k_w]\n    #         output[i, j] = np.sum(region * flipped_kernel)\n    \n    return output\n\n# Test your implementation\ntest_image = data.camera()[:64, :64] / 255.0  # Small test image\ntest_kernel = np.array([[1, 0, -1],\n                        [2, 0, -2],\n                        [1, 0, -1]])  # Sobel-x kernel\n\n# Your implementation\n# my_result = convolve2d_manual(test_image, test_kernel)\n\n# Compare with scipy (ground truth)\nfrom scipy.signal import convolve2d as scipy_convolve2d\nscipy_result = scipy_convolve2d(test_image, test_kernel, mode='same', boundary='fill')\n\n# Validation\n# diff = np.abs(my_result - scipy_result).max()\n# print(f\"Maximum difference from scipy: {diff:.6f}\")\n# print(\"‚úÖ Passed!\" if diff < 1e-5 else \"‚ùå Check your implementation\")\n\n# Display results\n# fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n# axes[0].imshow(test_image, cmap='gray')\n# axes[0].set_title('Original')\n# axes[0].axis('off')\n# axes[1].imshow(my_result, cmap='gray')\n# axes[1].set_title('Your Convolution')\n# axes[1].axis('off')\n# axes[2].imshow(scipy_result, cmap='gray')\n# axes[2].set_title('SciPy Reference')\n# axes[2].axis('off')\n# plt.tight_layout()\n# plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analysis 2.1 (4 points)\n\n**Q1:** Why does convolution require flipping the kernel? What would happen if we didn't flip it?\n\n*Your answer:*\n\n\n**Q2:** Explain why zero-padding is needed for \"same\" convolution. What happens to the output size without padding?\n\n*Your answer:*\n\n\n**Q3:** The nested for-loop approach above is slow. In a real CNN, how is convolution made efficient?\n\n*Your answer:*\n\n\n## 2.2 CNN Parameter Calculation (8 points)\n\nUnderstanding how parameters are calculated is crucial for designing efficient networks."
  },
  {
   "cell_type": "code",
   "source": "# CNN Architecture: Calculate parameters and output shapes\n# Consider this simple CNN for CIFAR-10 (32x32x3 input, 10 classes)\n\n\"\"\"\nLayer 1: Conv2D(filters=32, kernel_size=3x3, input_channels=3)\nLayer 2: MaxPool2D(pool_size=2x2)\nLayer 3: Conv2D(filters=64, kernel_size=3x3)\nLayer 4: MaxPool2D(pool_size=2x2)\nLayer 5: Flatten\nLayer 6: Dense(128)\nLayer 7: Dense(10)  # Output layer\n\"\"\"\n\n# TODO: Calculate output shape after each layer\n# Input: 32x32x3\n\nprint(\"=== Output Shapes ===\")\nprint(\"Input:                32 √ó 32 √ó 3\")\nprint(\"After Conv2D(32,3√ó3): ___ √ó ___ √ó ___\")  # TODO\nprint(\"After MaxPool(2√ó2):   ___ √ó ___ √ó ___\")  # TODO\nprint(\"After Conv2D(64,3√ó3): ___ √ó ___ √ó ___\")  # TODO\nprint(\"After MaxPool(2√ó2):   ___ √ó ___ √ó ___\")  # TODO\nprint(\"After Flatten:        ___\")              # TODO\nprint(\"After Dense(128):     ___\")              # TODO\nprint(\"After Dense(10):      ___\")              # TODO\n\n# TODO: Calculate parameters for each layer\n# Formula for Conv2D: (kernel_h √ó kernel_w √ó input_channels + 1) √ó num_filters\n# Formula for Dense: (input_features + 1) √ó output_features\n\nprint(\"\\n=== Parameters ===\")\nprint(\"Conv2D(32, 3√ó3, in=3):  (3 √ó 3 √ó 3 + 1) √ó 32 = ___\")  # TODO\nprint(\"Conv2D(64, 3√ó3, in=32): (3 √ó 3 √ó ___ + 1) √ó 64 = ___\")  # TODO\nprint(\"Dense(128):             (___ + 1) √ó 128 = ___\")  # TODO (after flatten)\nprint(\"Dense(10):              (128 + 1) √ó 10 = ___\")  # TODO\nprint(\"\\nTotal parameters: ___\")  # TODO",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis 2.2 (3 points)\n\n**Q1:** Why do Conv2D layers have far fewer parameters than Dense layers, even though they process the entire image?\n\n*Your answer:*\n\n\n**Q2:** What is the purpose of MaxPooling? What would happen if we removed all pooling layers?\n\n*Your answer:*\n\n\n**Q3:** If we doubled the number of filters in each Conv2D layer, how would that affect the total parameters?\n\n*Your answer:*\n\n\n## 2.3 Feature Maps (7 points)\n\nVisualize what different convolutional filters detect in an image.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize feature maps from different filters\nfrom scipy.signal import convolve2d\n\n# Load test image\nfeature_img = data.camera() / 255.0\n\n# Define filters that detect different features\nfeature_filters = {\n    'Horizontal Edges': np.array([[-1, -2, -1],\n                                   [0, 0, 0],\n                                   [1, 2, 1]]),\n    \n    'Vertical Edges': np.array([[-1, 0, 1],\n                                 [-2, 0, 2],\n                                 [-1, 0, 1]]),\n    \n    'Diagonal (/)': np.array([[0, 1, 2],\n                               [-1, 0, 1],\n                               [-2, -1, 0]]),\n    \n    'Diagonal (\\\\)': np.array([[2, 1, 0],\n                                [1, 0, -1],\n                                [0, -1, -2]]),\n    \n    'Laplacian (Blob)': np.array([[0, 1, 0],\n                                   [1, -4, 1],\n                                   [0, 1, 0]])\n}\n\n# Apply each filter and display feature maps\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Original\naxes[0].imshow(feature_img, cmap='gray')\naxes[0].set_title('Original Image')\naxes[0].axis('off')\n\n# Feature maps\nfor i, (name, kernel) in enumerate(feature_filters.items()):\n    feature_map = convolve2d(feature_img, kernel, mode='same', boundary='symm')\n    # Take absolute value for visualization\n    axes[i+1].imshow(np.abs(feature_map), cmap='hot')\n    axes[i+1].set_title(f'Feature: {name}')\n    axes[i+1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# TODO: Observe which parts of the image activate each filter\nprint(\"Observe: Different filters 'light up' for different image features!\")\nprint(\"- Horizontal edges filter responds to the tripod legs\")\nprint(\"- Vertical edges filter responds to the camera body edges\")\nprint(\"- This is how early CNN layers learn to detect basic features!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Analysis 2.3 (4 points)\n\n**Q1:** Looking at the feature maps above, which filter responds most strongly to the camera's tripod? Why?\n\n*Your answer:*\n\n\n**Q2:** In a CNN, early layers learn filters similar to what we see above (edges, blobs). What do DEEPER layers learn to detect?\n\n*Your answer:*\n\n\n**Q3:** Why is the concept of \"hierarchical feature learning\" important for image classification?\n\n*Your answer:*\n\n---\n\n## Part 2 Reflection (Required)\n\n**What was the MOST challenging concept in this section (convolution, parameters, or feature maps)?**\n\n*Your answer:*\n\n\n**In your own words, explain why CNNs are better than fully-connected networks for image classification.**\n\n*Your answer:*\n\n\n**If you were designing a CNN to recognize faces, what kinds of features would you expect early vs. late layers to detect?**\n\n*Your answer:*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 3: Generative Models (20 points) ‚≠ê‚≠ê‚≠ê‚≠ê\n\n**Time Allocation:** 35-45 minutes | **Difficulty:** Hardest\n\nThis section tests your understanding of autoencoders, VAEs, and GANs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.1 Autoencoder Architecture (12 points)\n\nDesign and analyze an autoencoder architecture for MNIST digit reconstruction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Define autoencoder architecture in pseudocode or PyTorch-like notation\n# Input: 28x28 = 784 pixels (flattened)\n# Goal: Compress to YOUR personalized latent space dimension, then reconstruct\n\nprint(f\"üé≤ Your personalized latent dimension: {MY_LATENT_DIM}\")\nprint(f\"   Design your autoencoder to compress to {MY_LATENT_DIM} dimensions\\n\")\n\n# Example structure (complete this with YOUR latent dimension):\n\"\"\"\nEncoder:\n    Input(784) \n    ‚Üí Dense(?) + ReLU\n    ‚Üí Dense(?) + ReLU  \n    ‚Üí Dense(MY_LATENT_DIM)  # Your personalized latent space\n\nDecoder:\n    Input(MY_LATENT_DIM)  # From latent space\n    ‚Üí Dense(?) + ReLU\n    ‚Üí Dense(?) + ReLU\n    ‚Üí Dense(784) + Sigmoid  # Reconstruct image\n\"\"\"\n\n# TODO: Calculate total parameters for YOUR architecture\n# The latent dimension affects your parameter count!\n# Write your calculation below:\n\nprint(\"Encoder parameters:\")\nprint(f\"  Layer 1: (784 + 1) √ó ___ = ___\")\nprint(f\"  Layer 2: (___ + 1) √ó ___ = ___\")\nprint(f\"  Layer 3: (___ + 1) √ó {MY_LATENT_DIM} = ___\")\n# TODO: Complete\n\nprint(\"\\nDecoder parameters:\")\nprint(f\"  Layer 1: ({MY_LATENT_DIM} + 1) √ó ___ = ___\")\nprint(f\"  Layer 2: (___ + 1) √ó ___ = ___\")\nprint(f\"  Layer 3: (___ + 1) √ó 784 = ___\")\n# TODO: Complete\n\nprint(\"\\nTotal parameters:\")\n# TODO: Calculate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3.1 (3 points)\n",
    "\n",
    "**Q1:** What is the purpose of the \"bottleneck\" (latent space) in an autoencoder? What happens if it's too small or too large?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2:** An autoencoder trained on digit images achieves good reconstruction loss, but when you sample random points in the latent space, the decoded images look like noise. Why does this happen?\n",
    "\n",
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 VAE Loss Function (9 points)\n\nImplement and understand the VAE loss function components."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_original, x_reconstructed, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate VAE loss = Reconstruction Loss + Œ≤ * KL Divergence\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_original : array\n",
    "        Original input images (batch_size, 784)\n",
    "    x_reconstructed : array\n",
    "        Reconstructed images from decoder (batch_size, 784)\n",
    "    mu : array\n",
    "        Mean of latent distribution (batch_size, latent_dim)\n",
    "    log_var : array\n",
    "        Log variance of latent distribution (batch_size, latent_dim)\n",
    "    beta : float\n",
    "        Weight for KL divergence term\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    total_loss, reconstruction_loss, kl_loss\n",
    "    \"\"\"\n",
    "    # TODO: Implement reconstruction loss (MSE or BCE)\n",
    "    reconstruction_loss = None  # TODO: Mean squared error between original and reconstructed\n",
    "    \n",
    "    # TODO: Implement KL divergence\n",
    "    # KL(q(z|x) || p(z)) = -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "    kl_loss = None  # TODO\n",
    "    \n",
    "    total_loss = reconstruction_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "# Test with dummy data\n",
    "np.random.seed(42)\n",
    "batch_size = 32\n",
    "latent_dim = 16\n",
    "\n",
    "x_orig = np.random.rand(batch_size, 784)\n",
    "x_recon = x_orig + np.random.randn(batch_size, 784) * 0.1  # Noisy reconstruction\n",
    "mu = np.random.randn(batch_size, latent_dim) * 0.5\n",
    "log_var = np.random.randn(batch_size, latent_dim) * 0.5\n",
    "\n",
    "# total, recon, kl = vae_loss(x_orig, x_recon, mu, log_var)\n",
    "# print(f\"Total Loss: {total:.4f}\")\n",
    "# print(f\"Reconstruction Loss: {recon:.4f}\")\n",
    "# print(f\"KL Divergence: {kl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3.2 (3 points)\n",
    "\n",
    "**Q1:** What is the purpose of the KL divergence term in the VAE loss? What distribution are we encouraging the latent space to match?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2:** In Œ≤-VAE, we use Œ≤ > 1 to weight the KL term more heavily. What is the trade-off when increasing Œ≤?\n",
    "\n",
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.3 GAN Training Dynamics (9 points)\n\nAnalyze GAN training behavior and common failure modes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated GAN training curves (DO NOT MODIFY)\n",
    "np.random.seed(42)\n",
    "epochs = 100\n",
    "\n",
    "# Scenario A: Healthy training\n",
    "d_loss_healthy = 0.7 - 0.2 * (1 - np.exp(-np.arange(epochs)/30)) + 0.05 * np.random.randn(epochs)\n",
    "g_loss_healthy = 2.0 - 1.3 * (1 - np.exp(-np.arange(epochs)/40)) + 0.08 * np.random.randn(epochs)\n",
    "\n",
    "# Scenario B: Mode collapse  \n",
    "d_loss_collapse = np.concatenate([0.7 - 0.3 * np.arange(30)/30, np.ones(70) * 0.1 + 0.02 * np.random.randn(70)])\n",
    "g_loss_collapse = np.concatenate([2.0 - 0.5 * np.arange(30)/30, np.ones(70) * 0.3 + 0.05 * np.random.randn(70)])\n",
    "\n",
    "# Scenario C: Discriminator too strong\n",
    "d_loss_strong_d = 0.7 * np.exp(-np.arange(epochs)/10) + 0.02 * np.random.randn(epochs)\n",
    "g_loss_strong_d = 2.0 + 0.5 * np.log(1 + np.arange(epochs)/20) + 0.1 * np.random.randn(epochs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(d_loss_healthy, 'b-', label='D Loss')\n",
    "axes[0].plot(g_loss_healthy, 'r-', label='G Loss')\n",
    "axes[0].set_title('Scenario A')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "\n",
    "axes[1].plot(d_loss_collapse, 'b-', label='D Loss')\n",
    "axes[1].plot(g_loss_collapse, 'r-', label='G Loss')\n",
    "axes[1].set_title('Scenario B')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Epoch')\n",
    "\n",
    "axes[2].plot(d_loss_strong_d, 'b-', label='D Loss')\n",
    "axes[2].plot(g_loss_strong_d, 'r-', label='G Loss')\n",
    "axes[2].set_title('Scenario C')\n",
    "axes[2].legend()\n",
    "axes[2].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Analysis 3.3 (6 points)\n\n**Q1:** For each scenario (A, B, C), describe what is happening during training and whether it represents healthy or problematic training.\n\n*Scenario A:*\n\n*Scenario B:*\n\n*Scenario C:*\n\n\n**Q2:** What is \"mode collapse\" in GANs? Why does it occur, and name ONE technique to mitigate it.\n\n*Your answer:*\n\n\n**Q3:** Compare VAEs and GANs: which typically produces sharper images, and why?\n\n*Your answer:*\n\n\n### Comparison Table (Required)\n\nFill in this comparison table based on your understanding:\n\n| Aspect | Autoencoder | VAE | GAN |\n|--------|-------------|-----|-----|\n| Training stability | | | |\n| Image sharpness | | | |\n| Can generate new samples? | | | |\n| Main loss function | | | |\n\n---\n\n## Part 3 Reflection (Required)\n\n**Before this exam, what did you think \"generative models\" meant?**\n\n*Your answer:*\n\n\n**What is the MOST SURPRISING thing you learned about generative models?**\n\n*Your answer:*\n\n\n**If you had to explain VAE vs GAN to a non-technical friend, how would you describe the difference in ONE sentence each?**\n\n*VAE:*\n\n*GAN:*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus: End-to-End Application (10 points)\n",
    "\n",
    "**Time Allocation:** 30-45 minutes\n",
    "\n",
    "Design and implement a complete image processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Document Enhancement Pipeline\n",
    "\n",
    "You receive a scanned document with multiple degradations:\n",
    "- Uneven illumination\n",
    "- Noise\n",
    "- Low contrast\n",
    "- Slight blur\n",
    "\n",
    "Your task is to design a restoration pipeline that produces a clean, readable document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a degraded document image\n",
    "def create_degraded_document():\n",
    "    \"\"\"Create a synthetic degraded document.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Start with white background\n",
    "    doc = np.ones((300, 400)) * 0.95\n",
    "    \n",
    "    # Add text-like lines\n",
    "    for row in range(40, 260, 20):\n",
    "        line_length = np.random.randint(100, 350)\n",
    "        start_col = np.random.randint(20, 50)\n",
    "        doc[row:row+6, start_col:start_col+line_length] = 0.1\n",
    "    \n",
    "    # Add title\n",
    "    doc[15:28, 80:320] = 0.05\n",
    "    \n",
    "    # Apply degradations\n",
    "    # 1. Uneven illumination\n",
    "    x, y = np.meshgrid(np.linspace(0, 1, 400), np.linspace(0, 1, 300))\n",
    "    illumination = 0.6 + 0.4 * np.sin(x * np.pi) * (0.7 + 0.3 * y)\n",
    "    degraded = doc * illumination\n",
    "    \n",
    "    # 2. Add noise\n",
    "    degraded = degraded + np.random.randn(*degraded.shape) * 0.05\n",
    "    \n",
    "    # 3. Blur\n",
    "    degraded = filters.gaussian(degraded, sigma=1.2)\n",
    "    \n",
    "    # 4. Reduce contrast\n",
    "    degraded = exposure.rescale_intensity(degraded, out_range=(0.2, 0.8))\n",
    "    \n",
    "    return np.clip(degraded, 0, 1), np.clip(doc, 0, 1)\n",
    "\n",
    "degraded_doc, clean_doc = create_degraded_document()\n",
    "show_images([degraded_doc, clean_doc], ['Degraded Document', 'Original (Target)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design and implement your restoration pipeline\n",
    "\n",
    "def restore_document(degraded):\n",
    "    \"\"\"\n",
    "    Restore a degraded document image.\n",
    "    \n",
    "    Your pipeline should address:\n",
    "    1. Uneven illumination\n",
    "    2. Noise\n",
    "    3. Low contrast\n",
    "    4. Blur\n",
    "    \n",
    "    Consider the ORDER of operations carefully!\n",
    "    \"\"\"\n",
    "    result = degraded.copy()\n",
    "    \n",
    "    # TODO: Step 1 - Address illumination\n",
    "    # Hint: Consider using morphological operations or background estimation\n",
    "    \n",
    "    # TODO: Step 2 - Denoise\n",
    "    # Hint: What type of noise is present?\n",
    "    \n",
    "    # TODO: Step 3 - Enhance contrast\n",
    "    # Hint: Consider histogram equalization or contrast stretching\n",
    "    \n",
    "    # TODO: Step 4 - Sharpen (if needed)\n",
    "    # Hint: Be careful not to amplify noise\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply your pipeline\n",
    "# restored = restore_document(degraded_doc)\n",
    "\n",
    "# Show results\n",
    "# show_images([degraded_doc, restored, clean_doc],\n",
    "#             ['Degraded', 'Your Restoration', 'Target'])\n",
    "\n",
    "# Calculate PSNR\n",
    "# psnr = calculate_psnr(clean_doc, restored)\n",
    "# print(f\"PSNR: {psnr:.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Bonus Analysis (4 points)\n\n**Q1:** Explain why you chose the ORDER of operations in your pipeline. Why not a different order?\n\n*Your answer:*\n\n\n**Q2:** What is the most challenging degradation to correct? Why?\n\n*Your answer:*\n\n\n**Q3:** How would you adapt your pipeline if this were a color document instead of grayscale?\n\n*Your answer:*\n\n\n### Comparison Requirement (2 points)\n\n**Try TWO different orderings** of your pipeline operations. Document the results:\n\n**Ordering A:** (your main approach)\n- Step 1: _____\n- Step 2: _____\n- Step 3: _____\n- Step 4: _____\n- PSNR achieved: _____\n\n**Ordering B:** (alternative approach)\n- Step 1: _____\n- Step 2: _____\n- Step 3: _____\n- Step 4: _____\n- PSNR achieved: _____\n\n**Q4:** Why did one ordering work better than the other? Be specific about what went wrong with the worse ordering.\n\n*Your answer:*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# LLM Usage Log\n\nDocument all LLM interactions below. **Be honest** - this log helps you reflect on your learning process.\n\nFor each LLM interaction, answer:\n1. What you asked\n2. Which LLM you used\n3. **What you LEARNED from the response** (not just \"used the code\")\n\n| Question/Task | LLM Used | What I LEARNED |\n|---------------|----------|----------------|\n| Example: \"How to implement 2D convolution\" | ChatGPT | I learned that convolution requires flipping the kernel first, which I didn't realize before |\n| | | |\n| | | |\n| | | |\n| | | |\n\n### LLM Reflection (Required)\n\n**Did using an LLM help you LEARN, or did it just give you answers?** Be honest.\n\n*Your answer:*\n\n\n**What is ONE thing you would have struggled to understand WITHOUT LLM help?**\n\n*Your answer:*\n\n\n**What is ONE thing you learned BETTER by trying yourself first before asking an LLM?**\n\n*Your answer:*\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "# Submission Checklist\n\nBefore submitting, verify:\n\n- [ ] Student name and ID filled in at the top\n- [ ] **MY_SEED replaced with YOUR student number** (personalized parameters)\n- [ ] All code cells executed (outputs visible)\n- [ ] All TODO items completed\n- [ ] All analysis questions answered **in your own words**\n- [ ] All **reflection questions** answered honestly\n- [ ] All **comparison requirements** completed\n- [ ] LLM usage fully documented with **what you learned**\n- [ ] **Process log completed** (failed attempts, debugging, time spent)\n- [ ] All plots/figures visible\n- [ ] Notebook exported as PDF\n- [ ] Both `.ipynb` and `.pdf` files ready for submission\n\n**File naming:** `LastName_FirstName_FinalsExam.ipynb` and `.pdf`\n\n---\n\n## UP Honor Code Statement\n\n*\"Honor and Excellence\" (Karangalan at Kahusayan)*\n\nAs a student of the University of the Philippines, I am committed to upholding the highest standards of academic integrity. The pursuit of knowledge is not merely about obtaining correct answers, but about genuine learning and intellectual growth.\n\n### I pledge the following:\n\n1. **Honesty in Work:** All answers, code, and analysis in this exam represent my own understanding. Where I received help (from LLMs, resources, or others), I have documented it truthfully.\n\n2. **Integrity in Learning:** I used AI tools as learning aids, not as substitutes for understanding. I can explain any code I submitted and defend any answer I wrote.\n\n3. **Respect for the Process:** I did not share exam questions or answers with classmates. I understand that copying defeats the purpose of education.\n\n4. **Commitment to Excellence:** I approached this exam as an opportunity to demonstrate genuine learning, not just to obtain a grade.\n\n---\n\n**By submitting this exam, I affirm that I have upheld the UP tradition of Honor and Excellence.**\n\n**Student Signature:** ______________________ \n\n**Date:** __________\n\n**Student Number:** ______________________",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}