<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 12: Neural Networks</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html, body {
            height: 100%;
            width: 100%;
        }

        body {
            font-family: 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
            background: linear-gradient(135deg, #1B4332 0%, #2D6A4F 50%, #8B0000 100%);
            color: #333;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .presenter-header {
            background: linear-gradient(135deg, #1B4332 0%, #8B0000 100%);
            color: white;
            padding: 6px 25px;
            border-bottom: 2px solid #FFD700;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-shrink: 0;
            gap: 20px;
            min-height: auto;
            height: 50px;
        }

        .header-left {
            display: flex;
            align-items: center;
            gap: 12px;
            flex: 1;
        }

        .home-button {
            background: #FFD700;
            color: #1B4332;
            border: none;
            padding: 5px 12px;
            font-size: 0.8em;
            border-radius: 3px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            gap: 4px;
            white-space: nowrap;
        }

        .home-button:hover {
            background: white;
            transform: scale(1.05);
        }

        .presenter-header h2 {
            font-size: 1.1em;
            font-weight: 600;
            margin: 0;
        }

        .slide-counter {
            font-size: 0.85em;
            color: #FFD700;
            font-weight: 600;
            white-space: nowrap;
            min-width: auto;
            text-align: right;
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .slide-counter-label {
            font-size: 0.75em;
            opacity: 0.9;
            display: inline;
        }

        #slide-input {
            width: 50px;
            padding: 5px 8px;
            font-size: 0.9em;
            background: rgba(255, 255, 255, 0.1);
            color: #FFD700;
            border: 1px solid #FFD700;
            border-radius: 4px;
            text-align: center;
            font-weight: 600;
            font-family: monospace;
            margin-right: 10px;
            cursor: pointer;
            transition: all 0.2s ease;
        }

        #slide-input:hover {
            background: rgba(255, 255, 255, 0.15);
        }

        #slide-input:focus {
            outline: none;
            background: rgba(255, 255, 255, 0.25);
            box-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
        }

        #slide-input::selection {
            background: #FFD700;
            color: #1B4332;
        }

        /* Remove number input spinner */
        #slide-input::-webkit-outer-spin-button,
        #slide-input::-webkit-inner-spin-button {
            -webkit-appearance: none;
            margin: 0;
        }

        #slide-input[type=number] {
            -moz-appearance: textfield;
        }

        .presentation-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            overflow: hidden;
            padding: 10px 15px;
            justify-content: center;
            align-items: center;
        }

        .slide-viewer {
            width: 100%;
            max-width: 1600px;
            aspect-ratio: 16 / 9;
            background: white;
            border-radius: 8px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .slide-content {
            flex: 1;
            padding: 30px 40px;
            overflow-y: auto;
            background: white;
            display: flex;
            flex-direction: column;
            justify-content: flex-start;
        }

        .slide-content h2 {
            color: #1B4332;
            font-size: 2.2em;
            margin-bottom: 14px;
            border-bottom: 3px solid #FFD700;
            padding-bottom: 10px;
            font-weight: 700;
        }

        .slide-content h3 {
            color: #8B0000;
            font-size: 1.5em;
            margin-top: 12px;
            margin-bottom: 10px;
            font-weight: 600;
        }

        .slide-content h4 {
            color: #1B4332;
            font-size: 1.25em;
            margin-top: 10px;
            margin-bottom: 6px;
            font-weight: 600;
        }

        .slide-content h5 {
            color: #1B4332;
            font-size: 1.1em;
            margin-top: 8px;
            margin-bottom: 5px;
            font-weight: 600;
        }

        .slide-content p {
            margin-bottom: 8px;
            color: #555;
            font-size: 1.05em;
            line-height: 1.5;
        }

        .slide-content ul, .slide-content ol {
            margin-bottom: 8px;
            margin-left: 15px;
            list-style: none;
        }

        .slide-content li {
            margin-bottom: 5px;
            color: #555;
            line-height: 1.5;
            font-size: 1.05em;
            margin-left: 20px;
        }

        .slide-content ul li:before {
            content: "‚ñ∏";
            color: #FFD700;
            font-weight: bold;
            margin-right: 8px;
            margin-left: -20px;
        }

        /* Link styling */
        .slide-content a {
            color: #8B0000;
            text-decoration: none;
            font-weight: 600;
            border-bottom: 2px solid #FFD700;
            padding: 0 2px;
            transition: all 0.2s ease;
            cursor: pointer;
        }

        .slide-content a:hover {
            color: #FFD700;
            background-color: rgba(139, 0, 0, 0.1);
            border-bottom: 2px solid #8B0000;
        }

        .slide-content a:active {
            opacity: 0.8;
        }

        /* Two-column layout */
        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 18px;
            margin: 12px 0;
        }

        .two-column-left, .two-column-right {
            display: flex;
            flex-direction: column;
            justify-content: flex-start;
        }

        .two-column-left {
            max-width: 100%;
            overflow: hidden;
        }

        .two-column-left img {
            max-width: 100%;
            width: 100%;
            height: auto;
        }

        /* Three-column layout */
        .three-column {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 12px;
            margin: 12px 0;
        }

        /* Image styling */
        .slide-image {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            margin: 15px 0;
        }

        .image-caption {
            text-align: center;
            color: #666;
            font-size: 0.85em;
            margin-top: 10px;
            font-style: italic;
            line-height: 1.4;
        }

        /* Definition and highlight boxes */
        .definition {
            background: linear-gradient(120deg, #E8F5E9 0%, #F1F8F6 100%);
            padding: 10px 12px;
            border-radius: 6px;
            margin: 8px 0;
            border-left: 4px solid #1B4332;
            font-size: 0.85em;
        }

        .highlight {
            background: linear-gradient(120deg, #FFF9E6 0%, #FFFDF2 100%);
            padding: 10px 12px;
            border-radius: 6px;
            margin: 8px 0;
            border-left: 4px solid #FFD700;
            font-size: 0.85em;
        }

        .warning {
            background: linear-gradient(120deg, #FFEBEE 0%, #FFF5F7 100%);
            padding: 10px 12px;
            border-radius: 6px;
            margin: 8px 0;
            border-left: 4px solid #8B0000;
            font-size: 0.85em;
        }

        /* Mermaid diagram styling */
        .mermaid {
            display: flex;
            justify-content: center;
            margin: 12px 0;
            background: #f9f9f9;
            padding: 12px;
            border-radius: 8px;
            border: 1px solid #e0e0e0;
            max-height: 350px;
            overflow-y: auto;
        }

        /* Math/Code blocks */
        .math-block, .code-block {
            background: #f5f5f5;
            padding: 10px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            margin: 8px 0;
            border-left: 4px solid #1B4332;
            overflow-x: auto;
        }

        .math-block {
            border-left-color: #8B0000;
        }

        .code-block {
            border-left-color: #1B4332;
            background: #f9f9f9;
            border: 1px solid #e0e0e0;
            line-height: 1.4;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Code and pre tags */
        .slide-content pre {
            background: #f9f9f9;
            padding: 12px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 1.0em;
            margin: 10px 0;
            border-left: 4px solid #1B4332;
            overflow-x: auto;
            line-height: 1.4;
            white-space: pre-wrap;
            word-wrap: break-word;
            border: 1px solid #e0e0e0;
        }

        .slide-content code {
            font-family: 'Courier New', monospace;
            background: #f0f0f0;
            padding: 2px 6px;
            border-radius: 3px;
            color: #1B4332;
            font-size: 1.05em;
            font-weight: 500;
        }

        .slide-content pre code {
            background: none;
            padding: 0;
            color: inherit;
            font-size: 1em;
        }

        /* Pseudocode styling */
        .pseudocode {
            background: #f9f9f9;
            padding: 12px;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            font-size: 1.0em;
            margin: 10px 0;
            border-left: 4px solid #2D6A4F;
            overflow-x: auto;
            line-height: 1.5;
            white-space: pre-wrap;
            word-wrap: break-word;
            border: 1px solid #e0e0e0;
            color: #333;
        }

        .slide-footer {
            background: #f5f5f5;
            padding: 8px 30px;
            border-top: 2px solid #E0E0E0;
            font-size: 0.8em;
            color: #888;
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 20px;
            flex-shrink: 0;
        }

        .footer-left {
            color: #1B4332;
            font-weight: 600;
        }

        .footer-right {
            color: #FFD700;
            font-weight: 600;
        }

        .footer-buttons {
            display: flex;
            gap: 10px;
            flex-shrink: 0;
        }

        button {
            background: linear-gradient(135deg, #1B4332 0%, #8B0000 100%);
            color: white;
            border: 1px solid #FFD700;
            padding: 6px 14px;
            font-size: 0.8em;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 2px 6px rgba(27, 67, 50, 0.15);
            white-space: nowrap;
        }

        button:hover {
            transform: translateY(-1px);
            box-shadow: 0 3px 10px rgba(27, 67, 50, 0.25);
            color: #FFD700;
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .hidden {
            display: none !important;
        }

        /* Scrollbar styling */
        .slide-content::-webkit-scrollbar {
            width: 8px;
        }

        .slide-content::-webkit-scrollbar-track {
            background: #f1f1f1;
        }

        .slide-content::-webkit-scrollbar-thumb {
            background: #1B4332;
            border-radius: 4px;
        }

        .slide-content::-webkit-scrollbar-thumb:hover {
            background: #8B0000;
        }

        /* MathJax equation styling */
        mjx-container {
            font-size: 1.3em !important;
            margin: 10px 0;
        }

        .math-display {
            background: #f9f9f9;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #1B4332;
            margin: 15px 0;
            overflow-x: auto;
            font-size: 1.4em;
        }

        .math-inline {
            padding: 2px 4px;
            font-size: 1.15em;
        }

        /* Visualization image styling - INCREASED FOR CLASSROOM VISIBILITY */
        .viz-image {
            max-width: 100%;
            width: 100%;
            height: auto;
            max-height: 500px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
            margin: 8px 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
            display: block;
        }

        .viz-caption {
            font-size: 0.85em;
            color: #666;
            text-align: center;
            margin-top: 8px;
            font-style: italic;
            border-left: 3px solid #FF6B6B;
            padding-left: 10px;
        }

        /* Dense information card styling */
        .dense-info-card {
            background: linear-gradient(135deg, #F0F8FF 0%, #F5F5F5 100%);
            border: 2px solid #8B0000;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            font-size: 0.9em;
            line-height: 1.4;
        }

        .dense-info-card h4 {
            color: #8B0000;
            margin-top: 0;
            border-bottom: 2px solid #DAA520;
            padding-bottom: 8px;
        }

        .dense-info-card h5 {
            color: #1B4332;
            margin-top: 10px;
            margin-bottom: 5px;
        }

        .dense-info-card table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 10px;
        }

        .dense-info-card th,
        .dense-info-card td {
            padding: 8px;
            text-align: left;
            border-bottom: 1px solid #DAA520;
        }

        .dense-info-card th {
            background-color: #FFD700;
            font-weight: bold;
            color: #333;
        }

        .dense-info-card td {
            background-color: rgba(255, 215, 0, 0.05);
        }

        /* Comparison grid */
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 15px 0;
        }

        .comparison-item {
            border: 1px solid #DAA520;
            padding: 12px;
            background-color: rgba(255, 215, 0, 0.05);
            border-radius: 6px;
        }

        .comparison-item h5 {
            color: #8B0000;
            margin-top: 0;
        }

        .comparison-item p {
            font-size: 0.9em;
            margin: 5px 0 0 0;
        }

        /* Multi-panel visualization grid */
        .multi-panel {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 15px;
            margin: 15px 0;
        }

        .panel {
            border: 1px solid #CCC;
            border-radius: 4px;
            padding: 10px;
            background-color: #FAFAFA;
        }

        .panel-title {
            font-weight: bold;
            color: #8B0000;
            margin-bottom: 8px;
            font-size: 0.95em;
        }

        .panel p {
            font-size: 0.85em;
            color: #666;
            margin: 5px 0;
        }

        /* Metric boxes */
        .metric-box {
            display: inline-block;
            background-color: #FFD700;
            color: #333;
            padding: 10px 14px;
            margin: 5px 5px 5px 0;
            border-radius: 4px;
            font-weight: bold;
            font-size: 0.95em;
        }

        /* Side-by-side image layout */
        .image-grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 12px 0;
            align-items: start;
        }

        .image-grid-3 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 12px;
            margin: 12px 0;
        }

        .image-grid-4 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr 1fr;
            gap: 10px;
            margin: 12px 0;
        }

        .image-container {
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .image-container img {
            max-width: 100%;
            width: 100%;
            height: auto;
            object-fit: contain;
        }

        .image-container .viz-caption {
            border-left: 2px solid #999;
            padding-left: 10px;
            text-align: left;
            width: 100%;
            margin-top: 8px;
            font-size: 0.9em;
            color: #555;
        }

        /* Timeline/evolution visualization */
        .evolution-section {
            background: #F9F9F9;
            border-left: 4px solid #8B0000;
            padding: 12px 15px;
            margin: 12px 0;
            border-radius: 4px;
        }

        .evolution-section h5 {
            color: #8B0000;
            margin-top: 0;
        }

        /* Key insight boxes */
        .insight-box {
            background: linear-gradient(120deg, #FFF9E6 0%, #FFF5E6 100%);
            border: 2px solid #FFD700;
            border-radius: 6px;
            padding: 12px;
            margin: 12px 0;
        }

        .insight-box strong {
            color: #8B0000;
        }

        /* ============================================
           CLASSROOM-OPTIMIZED LAYOUTS
           For back-of-class visibility with larger images
           ============================================ */

        /* Large image with floating content boxes */
        .classroom-layout {
            display: grid;
            grid-template-columns: 1.5fr 1fr;
            gap: 20px;
            margin: 12px 0;
            align-items: start;
        }

        .classroom-layout-large-image img {
            width: 100%;
            max-height: 600px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
        }

        .classroom-layout-large-image .viz-caption {
            font-size: 0.9em;
            color: #555;
            margin-top: 8px;
            line-height: 1.4;
        }

        /* Floating content boxes alongside image */
        .classroom-layout-content {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .floating-box {
            background: linear-gradient(120deg, #F5F9FF 0%, #FFFAF5 100%);
            border-left: 4px solid #1B4332;
            border-radius: 6px;
            padding: 14px;
            font-size: 0.95em;
            line-height: 1.5;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.08);
        }

        .floating-box h4 {
            color: #1B4332;
            font-size: 1.15em;
            margin: 0 0 6px 0;
            border-bottom: 2px solid #FFD700;
            padding-bottom: 5px;
        }

        .floating-box h5 {
            color: #8B0000;
            font-size: 1.02em;
            margin: 6px 0 4px 0;
        }

        .floating-box p {
            margin: 0 0 4px 0;
            font-size: 0.95em;
        }

        .floating-box ul {
            margin: 5px 0 0 15px;
            padding: 0;
            list-style: none;
        }

        .floating-box li {
            margin: 3px 0 3px 20px;
            font-size: 0.95em;
        }

        .floating-box ul li:before {
            content: "‚ñ∏";
            color: #FFD700;
            font-weight: bold;
            margin-right: 8px;
            margin-left: -20px;
        }

        /* Two-column with equal-sized images */
        .classroom-dual-image {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 16px;
            margin: 12px 0;
        }

        .classroom-dual-image .image-container {
            display: flex;
            flex-direction: column;
        }

        .classroom-dual-image img {
            width: 100%;
            max-height: 500px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
        }

        .classroom-dual-image .viz-caption {
            font-size: 0.9em;
            color: #555;
            margin-top: 8px;
        }

        /* Three-column image layout for classroom */
        .classroom-triple-image {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 12px;
            margin: 12px 0;
        }

        .classroom-triple-image .image-container {
            display: flex;
            flex-direction: column;
        }

        .classroom-triple-image img {
            width: 100%;
            max-height: 350px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
        }

        .classroom-triple-image .viz-caption {
            font-size: 0.85em;
            color: #555;
            margin-top: 6px;
        }

        /* Image-centric layout with text overlay boxes */
        .classroom-primary-image {
            position: relative;
            margin: 12px 0;
            display: flex;
            flex-direction: column;
        }

        .classroom-primary-image img {
            width: 100%;
            max-height: 550px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
        }

        .classroom-primary-image-caption {
            font-size: 0.9em;
            color: #555;
            margin-top: 8px;
            line-height: 1.4;
        }

        .classroom-primary-image-notes {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 12px;
            margin-top: 12px;
        }

        .image-note-box {
            background: linear-gradient(120deg, #FFFAF5 0%, #F5F9FF 100%);
            border-left: 3px solid #FFD700;
            padding: 10px;
            border-radius: 4px;
            font-size: 0.8em;
            line-height: 1.4;
        }

        .image-note-box strong {
            color: #1B4332;
        }

        /* Stacked layout for tall content */
        .classroom-stacked {
            display: flex;
            flex-direction: column;
            gap: 16px;
            margin: 12px 0;
        }

        .classroom-stacked-section {
            display: grid;
            grid-template-columns: 1.3fr 1fr;
            gap: 16px;
            align-items: start;
        }

        .classroom-stacked-section img {
            width: 100%;
            max-height: 450px;
            object-fit: contain;
            border: 1px solid #CCC;
            border-radius: 4px;
        }

        /* Key metrics display boxes */
        .metric-display {
            background: linear-gradient(135deg, #FFF8E6 0%, #F5F5FF 100%);
            border: 2px solid #FFD700;
            border-radius: 6px;
            padding: 12px;
            margin: 8px 0;
            font-size: 0.85em;
            text-align: center;
        }

        .metric-display strong {
            color: #1B4332;
            font-size: 1.05em;
            display: block;
        }

        .metric-label {
            color: #666;
            font-size: 0.8em;
            margin-top: 4px;
        }

        @media (max-width: 1200px) {
            .two-column {
                grid-template-columns: 1fr;
            }
            .three-column {
                grid-template-columns: 1fr 1fr;
            }
            .image-grid-4 {
                grid-template-columns: 1fr 1fr;
            }
            .comparison-grid {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 768px) {
            .presenter-header {
                flex-direction: column;
                gap: 8px;
                padding: 6px 20px;
            }

            .slide-content {
                padding: 20px;
            }

            .slide-content h2 {
                font-size: 1.6em;
            }

            .slide-footer {
                flex-wrap: wrap;
                padding: 6px 15px;
                font-size: 0.75em;
            }

            .footer-buttons {
                gap: 8px;
            }

            button {
                padding: 5px 10px;
                font-size: 0.75em;
            }

            .three-column {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="presenter-header">
        <div class="header-left">
            <a href="/" class="home-button">üè† Home</a>
            <h2>Module 12: Neural Networks</h2>
        </div>
        <div style="display: flex; align-items: center; gap: 6px; color: #FFD700; font-size: 0.9em; font-weight: 600;">
            <input type="number" id="slide-input" value="1" min="1" max="1" onchange="goToSlide()" onkeypress="handleSlideInputKey(event)" title="Jump to slide (type number and press Enter)">
            <span>/</span>
            <span id="total-slides">1</span>
        </div>
    </div>

    <div class="presentation-container">
        <div class="slide-viewer">
            <div class="slide-content" id="slide-content">
                <!-- Slides will be inserted here -->
            </div>
            <div class="slide-footer">
                <div class="footer-left">CMSC 173: Machine Learning</div>
                <div class="footer-buttons">
                    <button id="prev-btn" onclick="previousSlide()">‚Üê Previous</button>
                    <button id="next-btn" onclick="nextSlide()">Next ‚Üí</button>
                </div>
                <div class="footer-right">University of the Philippines - Cebu</div>
            </div>
        </div>
    </div>

    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true, theme: 'default' });

        // Slide content data - Complete 47 slides
        const slides = [
            // Slide 1: Title
            {
                title: "Artificial Neural Networks",
                content: `
                    <div style="text-align: center; margin-top: 80px;">
                        <h2 style="font-size: 3em; border: none;">Artificial Neural Networks</h2>
                        <p style="font-size: 1.5em; color: #8B0000; margin-top: 30px;">CMSC 173 - Machine Learning</p>
                        <p style="font-size: 1.2em; color: #555; margin-top: 20px;">University of the Philippines - Cebu</p>
                    </div>
                `
            },
            // Slide 2: Introduction to Neural Networks (Consolidated 2+3)
            {
                title: "Introduction to Neural Networks",
                content: `
                    <h4 style="margin-bottom: 10px;">From Biology to Artificial Systems</h4>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Biological List -->
                        <div style="padding: 8px; background: #F5F9FF; border-left: 3px solid #1B4332; border-radius: 4px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>Biological</strong></h5>
                            <ul style="margin: 0; padding: 0 0 0 12px; font-size: 0.8em; line-height: 1.3;">
                                <li style="margin: 2px 0;">Neurons: Basic units</li>
                                <li style="margin: 2px 0;">Synapses: Connections</li>
                                <li style="margin: 2px 0;">Learning: Adaptation</li>
                                <li style="margin: 2px 0;">Parallel processing</li>
                            </ul>
                        </div>

                        <!-- Artificial List -->
                        <div style="padding: 8px; background: #FFFAF5; border-left: 3px solid #8B0000; border-radius: 4px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #8B0000;"><strong>Artificial</strong></h5>
                            <ul style="margin: 0; padding: 0 0 0 12px; font-size: 0.8em; line-height: 1.3;">
                                <li style="margin: 2px 0;">Perceptrons: Units</li>
                                <li style="margin: 2px 0;">Weights: Parameters</li>
                                <li style="margin: 2px 0;">Training: Optimization</li>
                                <li style="margin: 2px 0;">Layers: Organization</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Large Perceptron Diagram with Mathematics -->
                    <h4 style="margin: 10px 0 8px 0;">The Perceptron: Building Block of Neural Networks</h4>
                    <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 15px; margin: 0; display: grid; grid-template-columns: 1.3fr 1fr; gap: 15px; align-items: start;">
                        <!-- Left: Diagram -->
                        <div>
                            <div class="mermaid" style="display: flex; justify-content: center; margin: 0; padding: 0; max-height: 380px;">
graph LR
    X1["x‚ÇÅ"]
    X2["x‚ÇÇ"]
    Xn["x‚Çô"]
    W["√ów‚ÇÅ, √ów‚ÇÇ, ..."]
    SUM["Œ£ + b"]
    ACT["œÉ(z)<br/>Activation"]
    OUT["y<br/>Output"]

    X1 --> W
    X2 --> W
    Xn --> W
    W --> SUM
    SUM --> ACT
    ACT --> OUT

    style X1 fill:#E8F4F8
    style X2 fill:#E8F4F8
    style Xn fill:#E8F4F8
    style W fill:#FFE8E8
    style SUM fill:#FFE8E8
    style ACT fill:#E8E8FF
    style OUT fill:#FFF8E8
                            </div>
                            <div style="font-size: 0.75em; text-align: center; margin-top: 6px; color: #555;">Computational Flow</div>
                        </div>

                        <!-- Right: Mathematical Equations -->
                        <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Mathematical Model</strong></h5>

                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <div style="margin-bottom: 8px;">
                                    <div style="font-weight: bold; color: #8B0000; font-size: 0.9em;">Forward Pass:</div>
                                    <div style="text-align: center; margin: 4px 0; padding: 6px; background: white; border-radius: 3px;">
                                        $$z = w^T x + b$$
                                    </div>
                                    <div style="text-align: center; margin: 4px 0; padding: 6px; background: white; border-radius: 3px;">
                                        $$y = \\sigma(z)$$
                                    </div>
                                </div>

                                <div style="border-top: 1px solid #FFD700; padding-top: 8px;">
                                    <div style="font-weight: bold; color: #1B4332; font-size: 0.9em;">Key Components:</div>
                                    <ul style="margin: 4px 0 0 0; padding-left: 12px; font-size: 0.8em;">
                                        <li style="margin: 2px 0;"><strong>x</strong>: Input vector</li>
                                        <li style="margin: 2px 0;"><strong>w</strong>: Weight vector</li>
                                        <li style="margin: 2px 0;"><strong>b</strong>: Bias term</li>
                                        <li style="margin: 2px 0;"><strong>œÉ</strong>: Activation function</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="insight-box" style="margin-top: 8px; padding: 10px;">
                        <strong style="font-size: 0.9em;">Key Insight:</strong> <span style="font-size: 0.85em;">Neural networks learn complex non-linear mappings by adjusting weights through training. The perceptron combines inputs, weights, and activation functions to create learnable processing units.</span>
                    </div>
                `
            },
            // Slide 3: Why Neural Networks? (Consolidated 4 with visual)
            {
                title: "Why Neural Networks? The Limitations of Linear Models",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <h4>Linear Models Cannot Solve:</h4>
                            <ul>
                                <li>Non-linear decision boundaries</li>
                                <li>The XOR problem</li>
                                <li>Complex pattern recognition</li>
                                <li>Hierarchical feature learning</li>
                            </ul>
                            <h5><strong>XOR Problem Example:</strong></h5>
                            <table style="border-collapse: collapse; width: 100%; margin: 10px 0;">
                                <tr style="border-bottom: 2px solid #8B0000;">
                                    <th style="padding: 8px; border-right: 1px solid #ccc;">x‚ÇÅ</th>
                                    <th style="padding: 8px; border-right: 1px solid #ccc;">x‚ÇÇ</th>
                                    <th style="padding: 8px;">XOR</th>
                                </tr>
                                <tr><td style="padding: 8px; border-right: 1px solid #ccc;">0</td><td style="padding: 8px; border-right: 1px solid #ccc;">0</td><td style="padding: 8px;">0</td></tr>
                                <tr><td style="padding: 8px; border-right: 1px solid #ccc;">0</td><td style="padding: 8px; border-right: 1px solid #ccc;">1</td><td style="padding: 8px;">1</td></tr>
                                <tr><td style="padding: 8px; border-right: 1px solid #ccc;">1</td><td style="padding: 8px; border-right: 1px solid #ccc;">0</td><td style="padding: 8px;">1</td></tr>
                                <tr><td style="padding: 8px; border-right: 1px solid #ccc;">1</td><td style="padding: 8px; border-right: 1px solid #ccc;">1</td><td style="padding: 8px;">0</td></tr>
                            </table>
                            <p style="color: #8B0000; font-weight: bold;">‚ö† No straight line can separate these classes!</p>
                        </div>
                        <div class="two-column-right">
                            <h4>Neural Networks Advantages:</h4>
                            <ul>
                                <li><strong>Non-linear boundaries:</strong> Curved decision surfaces</li>
                                <li><strong>Universal approximation:</strong> Can learn any function</li>
                                <li><strong>Hierarchical learning:</strong> Multi-layer feature extraction</li>
                                <li><strong>Scalability:</strong> Works for high-dimensional data</li>
                                <li><strong>End-to-end learning:</strong> Automatic feature engineering</li>
                            </ul>
                            <div class="definition" style="margin-top: 15px;">
                                <strong>Universal Approximation Theorem:</strong> A neural network with a single hidden layer can approximate any continuous function to arbitrary accuracy (given sufficient neurons).
                            </div>
                        </div>
                    </div>

                    <div class="metric-box" style="margin-top: 15px;">
                        ‚úì Solution: Add hidden layers to create non-linear transformations
                    </div>
                `
            },
            // Slide 4: Multi-Layer Network Architecture (Consolidated 5+6)
            {
                title: "Multi-Layer Network Architecture: From Single Unit to Networks",
                content: `
                    <h4 style="margin-bottom: 10px;">Comparing Single Unit vs Multi-Layer Architecture</h4>

                    <!-- Side-by-side diagrams with descriptions -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 12px;">
                        <!-- Left: Single Perceptron -->
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; text-align: center; color: #1B4332;"><strong>Single Perceptron Unit</strong></h5>
                            <div class="mermaid" style="display: flex; justify-content: center; margin: 0; padding: 0; max-height: 280px;">
graph TB
    X1["x‚ÇÅ"]
    X2["x‚ÇÇ"]
    X3["x‚ÇÉ"]
    SUM["Œ£ z = w¬∑x + b"]
    ACT["Activation œÉ(z)"]
    OUT["Output y"]

    X1 -->|w‚ÇÅ| SUM
    X2 -->|w‚ÇÇ| SUM
    X3 -->|w‚ÇÉ| SUM
    SUM --> ACT
    ACT --> OUT

    style SUM fill:#FFE8E8
    style ACT fill:#E8E8FF
    style OUT fill:#FFF8E8
                            </div>
                            <div style="font-size: 0.8em; text-align: center; margin-top: 6px; color: #555;">Basics: Weighted sum ‚Üí Activation</div>
                        </div>

                        <!-- Right: Multi-Layer Architecture -->
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; text-align: center; color: #1B4332;"><strong>Multi-Layer Network</strong></h5>
                            <div class="mermaid" style="display: flex; justify-content: center; margin: 0; padding: 0; max-height: 280px;">
graph LR
    subgraph Input["Input"]
        I1["x‚ÇÅ"]
        I2["x‚ÇÇ"]
        I3["x‚ÇÉ"]
    end

    subgraph Hidden["Hidden"]
        H1["h‚ÇÅ"]
        H2["h‚ÇÇ"]
        H3["h‚ÇÉ"]
    end

    subgraph Output["Output"]
        O1["y‚ÇÅ"]
        O2["y‚ÇÇ"]
    end

    I1 --> H1
    I1 --> H2
    I2 --> H1
    I2 --> H2
    I3 --> H1
    I3 --> H2

    H1 --> O1
    H2 --> O1
    H1 --> O2
    H2 --> O2

    style Input fill:#E8F4F8
    style Hidden fill:#E8E8FF
    style Output fill:#FFF8E8
                            </div>
                            <div style="font-size: 0.8em; text-align: center; margin-top: 6px; color: #555;">Layered: Multiple transformations</div>
                        </div>
                    </div>

                    <!-- Key Characteristics Box -->
                    <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 12px; margin-bottom: 12px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <div>
                                <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>Single Unit</strong></h5>
                                <ul style="margin: 0; padding-left: 14px; font-size: 0.8em; line-height: 1.4;">
                                    <li style="margin: 2px 0;">Limited expressiveness</li>
                                    <li style="margin: 2px 0;">Linear decision boundaries</li>
                                    <li style="margin: 2px 0;">Cannot learn XOR</li>
                                    <li style="margin: 2px 0;">Fast computation</li>
                                </ul>
                            </div>
                            <div>
                                <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>Multi-Layer Network</strong></h5>
                                <ul style="margin: 0; padding-left: 14px; font-size: 0.8em; line-height: 1.4;">
                                    <li style="margin: 2px 0;">Non-linear capabilities</li>
                                    <li style="margin: 2px 0;">Arbitrary decision boundaries</li>
                                    <li style="margin: 2px 0;">Solves XOR problem</li>
                                    <li style="margin: 2px 0;">Hierarchical learning</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Detailed computation example -->
                    <h4 style="margin: 10px 0 8px 0;">How Computation Flows Through Layers</h4>
                    <div style="display: grid; grid-template-columns: 1.2fr 1fr; gap: 12px;">
                        <!-- Left: Network diagram with inputs/hidden/output -->
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 10px; text-align: center;">
                            <p style="font-size: 0.85em; font-weight: 600; color: #1B4332; margin: 0 0 6px 0;">Input Layer ‚Üí Hidden Layer ‚Üí Output Layer</p>
                            <div class="mermaid" style="display: flex; justify-content: center; margin: 0; padding: 0; max-height: 300px;">
graph LR
    subgraph I["Inputs"]
        X1["x‚ÇÅ"]
        X2["x‚ÇÇ"]
    end

    subgraph H["Hidden<br/>(3 neurons)"]
        H1["h‚ÇÅ"]
        H2["h‚ÇÇ"]
        H3["h‚ÇÉ"]
    end

    subgraph O["Output"]
        Y["≈∑"]
    end

    X1 --> H1
    X1 --> H2
    X2 --> H1
    X2 --> H2
    H1 --> Y
    H2 --> Y

    style I fill:#E8F4F8
    style H fill:#E8E8FF
    style O fill:#FFF8E6
                            </div>
                        </div>

                        <!-- Right: Computation breakdown -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.9em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Forward Pass Example</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.6; color: #333;">
                                <strong style="color: #8B0000;">Layer 1:</strong><br/>
                                $z^{(1)} = W^{(1)}x + b^{(1)}$<br/>
                                $a^{(1)} = \\sigma(z^{(1)})$<br/>
                                <br/>
                                <strong style="color: #8B0000;">Layer 2:</strong><br/>
                                $z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$<br/>
                                $\\hat{y} = \\sigma(z^{(2)})$
                            </div>
                        </div>
                    </div>

                    <div class="insight-box" style="margin-top: 8px; padding: 10px;">
                        <strong style="font-size: 0.9em;">Key Principle:</strong> <span style="font-size: 0.85em;">Hidden layers create non-linear feature transformations. Each layer's output becomes the next layer's input, enabling learning of hierarchical representations.</span>
                    </div>
                `
            },
            // Slide 5: Perceptron Mathematical Formulation
            {
                title: "Perceptron: Mathematical Formulation",
                content: `
                    <!-- SVG Perceptron Diagram -->
                    <div style="display: flex; justify-content: center; margin: 0 0 12px 0;">
                        <object data="/static/diagrams/perceptron_diagram.svg" type="image/svg+xml" style="width: 100%; max-width: 600px; height: auto;"></object>
                    </div>

                    <h4>Complete Mathematical Description:</h4>
                    <div class="math-block">
$$z = \\sum_i w_i x_i + b = w^T x + b$$

$$y = \\sigma(z) = \\sigma(w^T x + b)$$
                    </div>

                    <!-- Three-column floating boxes for input, computation, output -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                        <!-- Column 1: Input -->
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 1em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Input</strong></h5>
                            <p style="font-size: 0.9em; margin: 4px 0;">Vector $x$:</p>
                            <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.85em; margin: 4px 0;">
                                $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$
                            </div>
                            <ul style="margin: 6px 0 0 15px; padding: 0; font-size: 0.85em; list-style: none;">
                                <li style="margin: 3px 0 3px 20px;">Features/inputs</li>
                                <li style="margin: 3px 0 3px 20px;">$n$ dimensions</li>
                                <li style="margin: 3px 0 3px 20px;">Real-valued data</li>
                            </ul>
                        </div>

                        <!-- Column 2: Computation -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 1em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Computation</strong></h5>
                            <p style="font-size: 0.9em; margin: 4px 0;">Weighted sum:</p>
                            <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.85em; margin: 4px 0;">
                                $z = w^T x + b$
                            </div>
                            <p style="font-size: 0.9em; margin: 6px 0 4px 0;">Then apply activation:</p>
                            <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.85em; margin: 4px 0;">
                                $y = \\sigma(z)$
                            </div>
                        </div>

                        <!-- Column 3: Output -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 1em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Output</strong></h5>
                            <p style="font-size: 0.9em; margin: 4px 0;">Prediction $y$:</p>
                            <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.85em; margin: 4px 0;">
                                $y \\in [0, 1]$ or $\\{0, 1\\}$
                            </div>
                            <ul style="margin: 6px 0 0 15px; padding: 0; font-size: 0.85em; list-style: none;">
                                <li style="margin: 3px 0 3px 20px;">Decision/probability</li>
                                <li style="margin: 3px 0 3px 20px;">Single scalar</li>
                                <li style="margin: 3px 0 3px 20px;">Activation-dependent</li>
                            </ul>
                        </div>
                    </div>

                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="warning">
                                <h5>Step Function (Original)</h5>
                                <div class="math-block">
$$\\sigma(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$
                                </div>
                                <strong>Problem:</strong> Not differentiable
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Sigmoid Function (Modern)</h5>
                                <div class="math-block">
$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$
                                </div>
                                <strong>Advantage:</strong> Smooth and differentiable
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 6: Perceptron Learning Algorithm
            {
                title: "Perceptron Learning Algorithm",
                content: `
                    <h4>Goal: Learn weights w and bias b to minimize prediction error</h4>

                    <!-- Three-column comparison of learning approaches -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                        <!-- Column 1: Key Concepts -->
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Core Concepts</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Error:</strong> Difference between prediction and target</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">$\\text{error} = y - \\hat{y}$</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Learning Rate:</strong> Step size</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">$\\alpha$ (e.g., 0.01)</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Update Rule:</strong> Adjust parameters</p>
                            <p style="font-size: 0.8em; margin: 2px 0; color: #555;">$\\theta := \\theta + \\Delta\\theta$</p>
                        </div>

                        <!-- Column 2: Original Perceptron Rule -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Original Rule</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;">When misclassified:</p>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.8em; margin: 4px 0; line-height: 1.4;">
                                $w_j := w_j + \\alpha e x_{ij}$<br/>
                                $b := b + \\alpha e$<br/>
                                where $e = y - \\hat{y}$
                            </div>
                            <p style="font-size: 0.8em; margin: 6px 0 0 0; color: #555;"><em>‚úì Simple, Guaranteed to converge (linear data)</em></p>
                        </div>

                        <!-- Column 3: Modern Gradient Descent -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Modern Approach</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;">Minimize loss:</p>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.8em; margin: 4px 0; line-height: 1.4;">
                                $L = \\frac{1}{2}(y-\\hat{y})^2$<br/>
                                $w_j := w_j - \\alpha\\frac{\\partial L}{\\partial w_j}$
                            </div>
                            <p style="font-size: 0.8em; margin: 6px 0 0 0; color: #555;"><em>‚úì Works for non-linear data, general purpose</em></p>
                        </div>
                    </div>

                    <div class="warning" style="margin-top: 10px;">
                        <strong>Key Insight:</strong> Single perceptron can only learn linearly separable functions. Multi-layer networks overcome this limitation by creating non-linear feature transformations.
                    </div>
                `
            },
            // Slide 7: Loss Functions (Critical Foundation)
            {
                title: "Loss Functions: Measuring Prediction Error",
                content: `
                    <h4>Loss functions quantify how well the network's predictions match the target values</h4>
                    <div class="three-column">
                        <div>
                            <div class="definition">
                                <h5>Mean Squared Error (MSE)</h5>
                                <div class="math-block">
$$L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$
                                </div>
                                <p><strong>Use case:</strong> Regression problems</p>
                                <p><strong>Properties:</strong></p>
                                <ul>
                                    <li>Sensitive to outliers</li>
                                    <li>Continuous and differentiable</li>
                                    <li>Penalizes large errors heavily</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Binary Cross-Entropy (BCE)</h5>
                                <div class="math-block">
$$L = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$
                                </div>
                                <p><strong>Use case:</strong> Binary classification (two classes)</p>
                                <p><strong>Properties:</strong></p>
                                <ul>
                                    <li>Probability-based</li>
                                    <li>Interpretable as log-likelihood</li>
                                    <li>Works with sigmoid output</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Categorical Cross-Entropy (CCE)</h5>
                                <div class="math-block">
$$L = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{C}y_{ij}\\log(\\hat{y}_{ij})$$
                                </div>
                                <p><strong>Use case:</strong> Multi-class classification (&gt;2 classes)</p>
                                <p><strong>Properties:</strong></p>
                                <ul>
                                    <li>One-hot encoded labels</li>
                                    <li>Standard for classification</li>
                                    <li>Works with softmax output</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="highlight">
                        <strong>Choice of loss function should match your task:</strong> regression ‚Üí MSE, binary classification ‚Üí BCE, multi-class ‚Üí CCE
                    </div>
                `
            },
            // Slide 8: Batch Normalization (Modern Essential)
            {
                title: "Batch Normalization: Stabilizing Training",
                content: `
                    <h4 style="margin-bottom: 10px;">From Problem to Solution</h4>

                    <!-- Top: Problem and Solution Boxes -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Problem Box -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.95em; color: #8B0000;"><strong>The Problem</strong></h5>
                            <p style="margin: 0 0 8px 0; font-size: 0.85em; line-height: 1.4;">Internal Covariate Shift: Distribution of inputs to each layer changes during training, making learning unstable and slow.</p>
                            <div style="font-size: 0.8em; color: #555;">
                                <strong>Consequences:</strong>
                                <ul style="margin: 4px 0 0 0; padding-left: 14px;">
                                    <li style="margin: 2px 0;">Slower convergence</li>
                                    <li style="margin: 2px 0;">Lower learning rate needed</li>
                                    <li style="margin: 2px 0;">Difficult weight initialization</li>
                                </ul>
                            </div>
                        </div>

                        <!-- Solution Box -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.95em; color: #1B4332;"><strong>The Solution</strong></h5>
                            <p style="margin: 0 0 8px 0; font-size: 0.85em; line-height: 1.4;">Batch Normalization: Normalize layer inputs to zero mean (Œº=0) and unit variance (œÉ¬≤=1) during training.</p>
                            <div style="font-size: 0.8em; color: #555;">
                                <strong>Benefits:</strong>
                                <ul style="margin: 4px 0 0 0; padding-left: 14px;">
                                    <li style="margin: 2px 0;">Faster convergence</li>
                                    <li style="margin: 2px 0;">Higher learning rates</li>
                                    <li style="margin: 2px 0;">Acts as regularization</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Bottom: Mathematical Formulation -->
                    <h4 style="margin: 10px 0 8px 0;">Mathematical Formulation</h4>
                    <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 12px;">
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px;">
                            <!-- Left: Normalization Steps -->
                            <div>
                                <div style="margin-bottom: 10px;">
                                    <div style="font-weight: bold; color: #1B4332; font-size: 0.9em; margin-bottom: 4px;">Step 1: Batch Mean</div>
                                    <div style="text-align: center; background: white; padding: 8px; border-radius: 3px; font-size: 0.9em;">
                                        $$\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m}x_i$$
                                    </div>
                                </div>

                                <div style="margin-bottom: 10px;">
                                    <div style="font-weight: bold; color: #1B4332; font-size: 0.9em; margin-bottom: 4px;">Step 2: Batch Variance</div>
                                    <div style="text-align: center; background: white; padding: 8px; border-radius: 3px; font-size: 0.9em;">
                                        $$\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_B)^2$$
                                    </div>
                                </div>
                            </div>

                            <!-- Right: Transformation and Scaling -->
                            <div>
                                <div style="margin-bottom: 10px;">
                                    <div style="font-weight: bold; color: #1B4332; font-size: 0.9em; margin-bottom: 4px;">Step 3: Normalize</div>
                                    <div style="text-align: center; background: white; padding: 8px; border-radius: 3px; font-size: 0.85em;">
                                        $$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$
                                    </div>
                                    <div style="font-size: 0.75em; color: #555; margin-top: 4px; text-align: center;">(Œµ prevents division by zero)</div>
                                </div>

                                <div style="margin-bottom: 10px;">
                                    <div style="font-weight: bold; color: #1B4332; font-size: 0.9em; margin-bottom: 4px;">Step 4: Scale & Shift</div>
                                    <div style="text-align: center; background: white; padding: 8px; border-radius: 3px; font-size: 0.9em;">
                                        $$y_i = \\gamma\\hat{x}_i + \\beta$$
                                    </div>
                                    <div style="font-size: 0.75em; color: #555; margin-top: 4px; text-align: center;">(Œ≥, Œ≤ are learnable)</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="insight-box" style="margin-top: 8px; padding: 10px;">
                        <strong style="font-size: 0.9em;">Key Insight:</strong> <span style="font-size: 0.85em;">BatchNorm is applied between linear/convolutional layers and activation functions. The learnable parameters Œ≥ (scale) and Œ≤ (shift) allow the network to undo normalization if beneficial.</span>
                    </div>
                `
            },
            // Slide 9: PyTorch Implementation Example (Essential for Application)
            {
                title: "Neural Network Implementation: PyTorch",
                content: `
                    <h4>Complete example: Digit classifier with batch normalization</h4>
                    <div class="code-block">
import torch
import torch.nn as nn

class SimpleDigitClassifier(nn.Module):
    def __init__(self, input_size=784, hidden_size=128):
        super().__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.layer3 = nn.Linear(64, 10)  # 10 digit classes

    def forward(self, x):
        x = self.layer1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.layer3(x)
        return x

# Create model, loss, optimizer
model = SimpleDigitClassifier()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(10):
    outputs = model(x_train)
    loss = criterion(outputs, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
                    </div>
                    <div class="definition">
                        <strong>nn.Module is the base class</strong> for all neural network components. Subclass it and define __init__ (architecture) and forward (computation flow).
                    </div>
                    <div class="highlight">
                        <strong>nn.CrossEntropyLoss combines log softmax and NLL loss</strong> - use this for multi-class classification. It expects raw logits (not probabilities).
                    </div>
                    <div class="highlight">
                        <strong>The training loop:</strong> forward pass ‚Üí compute loss ‚Üí zero gradients ‚Üí backward pass (backpropagation) ‚Üí optimizer step (weight update)
                    </div>
                `
            },
            // Slide 10: Activation Functions - Visual Overview
            {
                title: "Activation Functions: Visual Overview",
                content: `
                    <h4>Non-linearity is the foundation of neural network expressiveness</h4>

                    <!-- Two-row layout: Activation diagram on top, comparison image below -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin: 12px 0;">
                        <!-- Left: Activation function conceptual diagram -->
                        <div style="text-align: center;">
                            <div style="background: white; padding: 12px; border: 2px solid #1B4332; border-radius: 6px; margin-bottom: 8px;">
                                <svg viewBox="0 0 400 120" style="width: 100%; max-width: 350px; height: auto;">
                                    <!-- Input -->
                                    <circle cx="40" cy="60" r="20" fill="#FFE8E8" stroke="#8B0000" stroke-width="2"/>
                                    <text x="40" y="65" text-anchor="middle" font-size="16" font-weight="bold" fill="#8B0000">x</text>

                                    <!-- Arrow 1 -->
                                    <line x1="60" y1="60" x2="110" y2="60" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

                                    <!-- Activation circle -->
                                    <circle cx="160" cy="60" r="30" fill="#E8F4F8" stroke="#1B4332" stroke-width="2"/>
                                    <text x="160" y="55" text-anchor="middle" font-size="14" font-weight="bold" fill="#1B4332">œÉ</text>
                                    <text x="160" y="72" text-anchor="middle" font-size="11" fill="#1B4332">Activation</text>

                                    <!-- Arrow 2 -->
                                    <line x1="190" y1="60" x2="240" y2="60" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

                                    <!-- Output -->
                                    <circle cx="290" cy="60" r="20" fill="#FFF8E6" stroke="#FFD700" stroke-width="2"/>
                                    <text x="290" y="65" text-anchor="middle" font-size="16" font-weight="bold" fill="#1B4332">a(x)</text>

                                    <!-- Arrow marker definition -->
                                    <defs>
                                        <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                            <polygon points="0 0, 10 3, 0 6" fill="#333" />
                                        </marker>
                                    </defs>
                                </svg>
                            </div>
                            <p style="font-size: 0.85em; color: #555; margin: 0;">Input transforms through activation function</p>
                        </div>

                        <!-- Right: Mathematical representation -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Activation Function</strong></h5>
                            <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.85em; margin: 6px 0; line-height: 1.5;">
                                $a = \\sigma(z) = \\sigma(w^T x + b)$
                            </div>
                            <p style="font-size: 0.85em; margin: 6px 0;">Where:</p>
                            <ul style="margin: 4px 0 0 15px; padding: 0; font-size: 0.8em; list-style: none;">
                                <li style="margin: 2px 0 2px 20px;">$z$ = weighted sum + bias</li>
                                <li style="margin: 2px 0 2px 20px;">$\\sigma$ = activation function</li>
                                <li style="margin: 2px 0 2px 20px;">$a$ = activated output</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Large comparison image -->
                    <div style="display: flex; justify-content: center; align-items: center; margin: 12px 0;">
                        <img src="/images/module12/activation_functions.png" alt="Activation Functions Comparison" style="max-width: 100%; max-height: 420px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                    </div>
                    <p class="viz-caption" style="text-align: center;">Six activation functions with derivatives: Sigmoid, Tanh, ReLU, Leaky ReLU, ELU, Softplus</p>

                    <!-- Two-column explanation boxes -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 8px;">
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 3px;"><strong>Without Activation</strong></h5>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0;">Stacked linear layers ‚Üí Linear function (limited)</p>
                        </div>
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 3px;"><strong>With Activation</strong></h5>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0;">Non-linear transformation ‚Üí Universal approximation</p>
                        </div>
                    </div>
                `
            },
            // Slide 10a: Activation Functions - Comparison Table
            {
                title: "Activation Functions: Characteristics & Comparison",
                content: `
                    <h4>Understanding when and where to use each activation function</h4>

                    <!-- Three-column comparison boxes -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                        <!-- Column 1: Sigmoid -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Sigmoid</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Range:</strong> (0, 1)</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">Smooth, probability-like output</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Max Deriv:</strong> 0.25</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">‚ö† Vanishing gradient problem</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Best For:</strong></p>
                            <p style="font-size: 0.8em; color: #555; margin: 0;">Binary classification output layer</p>
                        </div>

                        <!-- Column 2: Tanh -->
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Tanh</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Range:</strong> (-1, 1)</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">Centered output around zero</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Max Deriv:</strong> 1</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">‚úì Better than Sigmoid</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Best For:</strong></p>
                            <p style="font-size: 0.8em; color: #555; margin: 0;">Hidden layers (historical)</p>
                        </div>

                        <!-- Column 3: ReLU -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>ReLU</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Range:</strong> [0, ‚àû)</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">Simple: max(0, x)</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Max Deriv:</strong> 1</p>
                            <p style="font-size: 0.8em; color: #555; margin: 2px 0 6px 0;">‚úì No vanishing gradients</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>Best For:</strong></p>
                            <p style="font-size: 0.8em; color: #555; margin: 0;">Hidden layers (modern default)</p>
                        </div>
                    </div>

                    <!-- Key insights section -->
                    <div class="definition" style="margin-top: 10px;">
                        <h5 style="margin: 0 0 8px 0;">Key Insights</h5>
                        <ul style="margin: 0; padding: 0 0 0 15px; font-size: 0.95em; list-style: none;">
                            <li style="margin: 4px 0 4px 20px;">ReLU enables deep networks: $1^{10} = 1$ (gradient preserved)</li>
                            <li style="margin: 4px 0 4px 20px;">Sigmoid causes vanishing: $0.25^{10} \\approx 10^{-7}$ (gradient near zero)</li>
                            <li style="margin: 4px 0 4px 20px;">Watch for dead ReLU neurons - use Leaky ReLU or Batch Norm if needed</li>
                            <li style="margin: 4px 0 4px 20px;">Modern trend: GELU, SiLU, Mish gaining popularity in transformer models</li>
                        </ul>
                    </div>
                `
            },
            // Slide 11: Activation Functions Mathematical Properties
            {
                title: "Activation Functions: Mathematical Properties",
                content: `
                    <h4>Understanding activation function behavior through derivatives and output ranges</h4>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <img src="/images/module12/activation_derivatives.png" alt="Activation function derivatives" class="viz-image">
                            <p class="viz-caption">How gradients flow: Sigmoid (max=0.25) vs Tanh/ReLU (max=1)</p>
                        </div>
                        <div class="image-container">
                            <img src="/images/module12/activation_ranges.png" alt="Output ranges" class="viz-image">
                            <p class="viz-caption">Output bounds for each activation function</p>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>Critical Gradient Flow Insight</h4>
                        <p>The <strong>maximum derivative value</strong> directly impacts gradient flow during backpropagation:</p>
                        <ul>
                            <li><strong>Sigmoid:</strong> Max derivative = 0.25 ‚Üí Gradients shrink significantly (vanishing gradient problem)</li>
                            <li><strong>Tanh:</strong> Max derivative = 1 ‚Üí Better gradient flow than sigmoid</li>
                            <li><strong>ReLU:</strong> Max derivative = 1 ‚Üí Constant gradients in active regions, no saturation</li>
                        </ul>

                        <h5>Why This Matters for Deep Networks</h5>
                        <p>In a 10-layer network, sigmoid gradients get multiplied repeatedly: $0.25^{10}$ ‚âà 10‚Åª‚Å∑ (essentially zero!)</p>
                        <p>ReLU avoids this: $1^{10}$ = 1 (gradient preserved), enabling very deep networks</p>

                        <h5>Output Range Implications</h5>
                        <table>
                            <tr>
                                <th>Function</th>
                                <th>Range</th>
                                <th>Typical Use</th>
                            </tr>
                            <tr>
                                <td>Sigmoid: (0,1)</td>
                                <td>Bounded, interpretable as probability</td>
                                <td>Output layer for binary classification</td>
                            </tr>
                            <tr>
                                <td>Tanh: (-1,1)</td>
                                <td>Centered around 0, symmetric</td>
                                <td>Hidden layers (historical preference)</td>
                            </tr>
                            <tr>
                                <td>ReLU: [0,‚àû)</td>
                                <td>Unbounded positive, sparse activation</td>
                                <td>Hidden layers (modern default)</td>
                            </tr>
                        </table>
                    </div>
                `
            },
            // Slide 12: ReLU Family
            {
                title: "Activation Functions: ReLU Family",
                content: `
                    <h4>Modern activation functions and solutions to ReLU problems</h4>

                    <div class="comparison-grid">
                        <div class="comparison-item">
                            <h5>ReLU (Rectified Linear Unit)</h5>
                            <div class="math-block">
$$\\text{ReLU}(x) = \\max(0, x)$$
                            </div>
                            <p><strong>Advantages:</strong> Fast computation, no vanishing gradients for x > 0, sparse activation (biological plausibility)</p>
                            <p><strong>Challenge:</strong> "Dying ReLU" problem - neurons can get stuck at zero</p>
                        </div>

                        <div class="comparison-item">
                            <h5>Leaky ReLU</h5>
                            <div class="math-block">
$$\\text{LeakyReLU}(x) = \\max(\\alpha x, x)$$
                            </div>
                            <p><strong>Solution:</strong> Allows small gradient for negative inputs (typical Œ± = 0.01)</p>
                            <p><strong>Benefit:</strong> Prevents dead neurons while maintaining most ReLU advantages</p>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>The "Dying ReLU" Problem Explained</h4>
                        <p><strong>What happens:</strong> During training, if a neuron's weights become negative and most inputs are negative, the ReLU output stays at zero. The gradient is also zero, so gradients cannot flow to update the weights.</p>

                        <h5>Why This Matters</h5>
                        <p>Dead neurons contribute nothing to the network output and waste parameter capacity. In large networks, many neurons can die simultaneously.</p>

                        <h5>Solutions</h5>
                        <table>
                            <tr>
                                <th>Solution</th>
                                <th>How It Works</th>
                                <th>Effectiveness</th>
                            </tr>
                            <tr>
                                <td>Leaky ReLU</td>
                                <td>Small negative gradient (Œ± ‚âà 0.01) allows weight updates</td>
                                <td>‚úì‚úì‚úì Most common fix</td>
                            </tr>
                            <tr>
                                <td>ELU (Exponential Linear Unit)</td>
                                <td>Smooth curve for negative inputs with small slope</td>
                                <td>‚úì‚úì Good alternative</td>
                            </tr>
                            <tr>
                                <td>Proper Initialization</td>
                                <td>He initialization prevents negative pre-activations</td>
                                <td>‚úì‚úì Preventive measure</td>
                            </tr>
                            <tr>
                                <td>Batch Normalization</td>
                                <td>Normalizes inputs to ReLU, reducing dead neurons</td>
                                <td>‚úì‚úì‚úì Most effective</td>
                            </tr>
                        </table>

                        <h5>Practical Recommendation</h5>
                        <p><strong>Default strategy:</strong> Start with ReLU. If you suspect dead neurons (check activations during training), switch to Leaky ReLU or use Batch Normalization.</p>
                    </div>
                `
            },
            // Slide 13: Activation Function Derivatives
            {
                title: "Activation Function Derivatives",
                content: `
                    <h4>Understanding how gradients flow through different activation functions</h4>

                    <img src="/images/module12/activation_derivatives.png" alt="Activation function derivatives comparison" class="viz-image">
                    <p class="viz-caption">Derivative curves for 6 activation functions showing how gradient magnitude varies with input</p>

                    <div class="dense-info-card">
                        <h4>The Mathematics of Backpropagation</h4>
                        <p>During backpropagation, gradients are computed using the chain rule, which multiplies derivatives at each layer:</p>
                        <div class="math-block">
$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial a^{(l)}} \\cdot \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\cdot \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}$$
                        </div>
                        <p>The derivative $\\frac{\\partial a^{(l)}}{\\partial z^{(l)}}$ depends on which activation function is used!</p>

                        <h5>Activation Function Derivatives (Mathematical Form)</h5>
                        <table>
                            <tr>
                                <th>Function</th>
                                <th>Mathematical Form</th>
                                <th>Max Value</th>
                                <th>Critical Issue</th>
                            </tr>
                            <tr>
                                <td>Sigmoid</td>
                                <td>$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$</td>
                                <td>0.25</td>
                                <td>Very small max derivative</td>
                            </tr>
                            <tr>
                                <td>Tanh</td>
                                <td>$\\tanh'(x) = 1 - \\tanh^2(x)$</td>
                                <td>1.0</td>
                                <td>Better than sigmoid, still saturation regions</td>
                            </tr>
                            <tr>
                                <td>ReLU</td>
                                <td>$\\text{ReLU}'(x) = 1$ (if $x>0$), else $0$</td>
                                <td>1.0</td>
                                <td>Constant gradient for active neurons</td>
                            </tr>
                            <tr>
                                <td>Leaky ReLU</td>
                                <td>$\\text{LeakyReLU}'(x) = \\alpha$ (if $x<0$), else $1$</td>
                                <td>1.0 (typically)</td>
                                <td>Always non-zero gradient</td>
                            </tr>
                        </table>

                        <h5>Why This Matters for Deep Networks</h5>
                        <p>Consider a 10-layer network with sigmoid activations. The gradient at the first layer would be multiplied by the derivative at each layer:</p>
                        <p><strong>Sigmoid case:</strong> $0.25^{10}$ ‚âà $10^{-7}$ (vanishing gradient!)</p>
                        <p><strong>ReLU case:</strong> $1^{10}$ = $1$ (gradient preserved!)</p>
                        <p>This is why ReLU is preferred for deep networks‚Äîit maintains gradient flow through many layers.</p>
                    </div>
                `
            },
            // Slide 14: Choosing Activation Functions
            {
                title: "Choosing Activation Functions: A Practical Guide",
                content: `
                    <h4>Decision tree for selecting activation functions based on layer type and task</h4>

                    <div class="comparison-grid">
                        <div class="comparison-item">
                            <h5>Hidden Layers Strategy</h5>
                            <p><strong>Default Choice: ReLU</strong></p>
                            <ul>
                                <li>‚úì Computationally efficient (just max(0, x))</li>
                                <li>‚úì Prevents vanishing gradients</li>
                                <li>‚úì Works well in deep networks</li>
                                <li>‚ö† May have dead neurons</li>
                            </ul>
                            <p><strong>If ReLU causes issues:</strong></p>
                            <ul>
                                <li>‚Üí Leaky ReLU (small negative gradient)</li>
                                <li>‚Üí ELU (smooth negative region)</li>
                                <li>‚Üí GELU (smoother, modern choice)</li>
                            </ul>
                        </div>

                        <div class="comparison-item">
                            <h5>Output Layer Strategy</h5>
                            <p><strong>Binary Classification:</strong> Sigmoid</p>
                            <ul>
                                <li>Output range: (0, 1) = probability</li>
                                <li>Use with cross-entropy loss</li>
                            </ul>
                            <p><strong>Multi-class Classification:</strong> Softmax</p>
                            <ul>
                                <li>Outputs sum to 1 (valid probability distribution)</li>
                                <li>Use with cross-entropy loss</li>
                            </ul>
                            <p><strong>Regression:</strong> Linear or Tanh</p>
                            <ul>
                                <li>Linear: unbounded output</li>
                                <li>Tanh: bounded output (-1, 1)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>Decision Framework for Activation Functions</h4>

                        <h5>Quick Selection Table</h5>
                        <table>
                            <tr>
                                <th>Scenario</th>
                                <th>Activation(s)</th>
                                <th>Reason</th>
                                <th>Fallback</th>
                            </tr>
                            <tr>
                                <td>Deep hidden layers (>10)</td>
                                <td>ReLU, Leaky ReLU</td>
                                <td>Vanishing gradient avoidance</td>
                                <td>GELU, Mish</td>
                            </tr>
                            <tr>
                                <td>Shallow network (1-2 hidden)</td>
                                <td>ReLU, Tanh</td>
                                <td>Either works fine</td>
                                <td>Sigmoid</td>
                            </tr>
                            <tr>
                                <td>Binary classification output</td>
                                <td>Sigmoid</td>
                                <td>Maps to probability (0,1)</td>
                                <td>Tanh + rescale</td>
                            </tr>
                            <tr>
                                <td>Multi-class output</td>
                                <td>Softmax</td>
                                <td>Probability distribution</td>
                                <td>Sigmoid (multiple)</td>
                            </tr>
                            <tr>
                                <td>Regression task</td>
                                <td>Linear (no activation)</td>
                                <td>Unbounded output</td>
                                <td>Tanh if bounded</td>
                            </tr>
                            <tr>
                                <td>Dead neuron problem detected</td>
                                <td>Leaky ReLU or Batch Norm</td>
                                <td>Maintain gradient flow</td>
                                <td>ELU or lower learning rate</td>
                            </tr>
                        </table>

                        <h5>Implementation Tips</h5>
                        <ul>
                            <li><strong>Default starting point:</strong> ReLU for all hidden layers, appropriate output activation for your task</li>
                            <li><strong>If training is slow:</strong> Check activation statistics‚Äîare neurons "dying"? Analyze mean activations per layer</li>
                            <li><strong>Batch Normalization interaction:</strong> Can be used with any activation, often reduces sensitivity to activation choice</li>
                            <li><strong>Modern trend:</strong> GELU, SiLU (Swish), Mish are gaining popularity in transformers and large models</li>
                            <li><strong>Avoid for hidden layers:</strong> Sigmoid (unless you know why), ELU for large networks (slower computation)</li>
                        </ul>
                    </div>
                `
            },
            // Slide 15: Multi-Layer Architecture
            {
                title: "Multi-Layer Neural Network Architecture",
                content: `
                    <h4>How networks learn to separate non-linear decision boundaries through layered transformations</h4>

                    <img src="/images/module12/decision_boundaries_basic.png" alt="Decision boundary learning through layers" class="viz-image">
                    <p class="viz-caption">Evolution of decision boundaries: Linear separator ‚Üí 2-layer network ‚Üí 3-layer network showing increased complexity</p>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Network Structure:</h4>
                                <ul>
                                    <li><strong>Input layer:</strong> 2D data (x, y)</li>
                                    <li><strong>Hidden layer 1:</strong> 5 neurons with ReLU</li>
                                    <li><strong>Hidden layer 2:</strong> 3 neurons with ReLU</li>
                                    <li><strong>Output layer:</strong> 1 neuron (binary classification)</li>
                                </ul>
                                <h5>Weight Matrices:</h5>
                                <ul>
                                    <li>$W^{(1)}, b^{(1)}$: Input ‚Üí Hidden1</li>
                                    <li>$W^{(2)}, b^{(2)}$: Hidden1 ‚Üí Hidden2</li>
                                    <li>$W^{(3)}, b^{(3)}$: Hidden2 ‚Üí Output</li>
                                </ul>
                            </div>
                        </div>
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Increasing Expressiveness</h4>
                                <p><strong>1 Linear Layer:</strong> Can only learn straight line separators</p>
                                <p><strong>2 Hidden Layers:</strong> Can learn curved boundaries (XOR problem solved)</p>
                                <p><strong>3+ Layers:</strong> Can learn arbitrarily complex non-convex regions</p>
                                <h5>Mathematical Insight</h5>
                                <p>Each ReLU layer performs a piecewise linear transformation. Stacking them creates increasingly complex piecewise linear decision surfaces that approximate smooth non-linear boundaries.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 16: Mathematical Representation
            {
                title: "Network Architecture: Mathematical Representation",
                content: `
                    <h4>For a network with $L$ layers:</h4>

                    <!-- Main computation equations -->
                    <div class="math-block" style="margin-bottom: 10px;">
$$a^{(0)} = x \\quad \\text{(input layer)}$$

For $l = 1, 2, \\ldots, L$:
$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\quad \\text{(pre-activation)}$$
$$a^{(l)} = \\sigma^{(l)}(z^{(l)}) \\quad \\text{(post-activation)}$$

$$\\hat{y} = a^{(L)} \\quad \\text{(output layer)}$$
                    </div>

                    <!-- Three-column breakdown of variables -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                        <!-- Column 1: Inputs/Outputs -->
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Inputs & Outputs</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>$a^{(l)}$:</strong> Layer activation</p>
                            <p style="font-size: 0.8em; margin: 2px 0; color: #555;">Output of layer $l$</p>
                            <p style="font-size: 0.85em; margin: 6px 0 4px 0;"><strong>$z^{(l)}$:</strong> Pre-activation</p>
                            <p style="font-size: 0.8em; margin: 2px 0; color: #555;">Before activation fn</p>
                            <p style="font-size: 0.85em; margin: 6px 0 4px 0;"><strong>$\\hat{y}$:</strong> Prediction</p>
                            <p style="font-size: 0.8em; margin: 2px 0; color: #555;">Final network output</p>
                        </div>

                        <!-- Column 2: Parameters -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Parameters</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>$W^{(l)}$:</strong> Weights</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">Shape: $n_l \\times n_{l-1}$</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>$b^{(l)}$:</strong> Biases</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">Shape: $n_l \\times 1$</p>
                            <p style="font-size: 0.8em; margin: 4px 0; color: #555;"><em>Learnable parameters updated during training</em></p>
                        </div>

                        <!-- Column 3: Dimensions & Structure -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Dimensions</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>$n_l$:</strong> Layer size</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">Neurons in layer $l$</p>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>$\\sigma^{(l)}$:</strong> Activation</p>
                            <p style="font-size: 0.8em; margin: 2px 0 6px 0; color: #555;">Activation function</p>
                            <p style="font-size: 0.8em; margin: 4px 0; color: #555;"><em>Each layer transforms dimensionality</em></p>
                        </div>
                    </div>

                    <div class="definition" style="margin-top: 10px;">
                        <strong>Recursive Structure:</strong> Each layer depends on the previous layer's output, creating a deep computational graph. This enables hierarchical feature learning.
                    </div>
                `
            },
            // Slide 17: Network Dimensions
            {
                title: "Network Dimensions and Parameters",
                content: `
                    <!-- Three-column layout for dimensions, computation, and example -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin: 12px 0;">
                        <!-- Column 1: Single Sample Dimensions -->
                        <div style="background: linear-gradient(120deg, #E8F4F8 0%, #F5F9FF 100%); border-left: 4px solid #1B4332; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Single Sample</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;">For layer $l$:</p>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.8em; margin: 4px 0; line-height: 1.4;">
                                Input: $a^{(l-1)}$ ‚Üí $(n_{l-1}, 1)$<br/>
                                Weights: $W^{(l)}$ ‚Üí $(n_l, n_{l-1})$<br/>
                                Output: $a^{(l)}$ ‚Üí $(n_l, 1)$
                            </div>
                        </div>

                        <!-- Column 2: Batch Processing -->
                        <div style="background: linear-gradient(120deg, #FFE8E8 0%, #FFFAF5 100%); border-left: 4px solid #8B0000; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Batch Processing</strong></h5>
                            <p style="font-size: 0.85em; margin: 4px 0;"><strong>With $m$ samples:</strong></p>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.8em; margin: 4px 0; line-height: 1.4;">
                                Input: $A^{(l-1)}$ ‚Üí $(n_{l-1}, m)$<br/>
                                Weights: $W^{(l)}$ ‚Üí $(n_l, n_{l-1})$<br/>
                                Output: $A^{(l)}$ ‚Üí $(n_l, m)$
                            </div>
                            <p style="font-size: 0.8em; margin: 6px 0 0 0; color: #555;"><em>$m$ = batch size</em></p>
                        </div>

                        <!-- Column 3: Parameter Counts -->
                        <div style="background: linear-gradient(120deg, #FFF8E6 0%, #FFF9F5 100%); border-left: 4px solid #FFD700; border-radius: 6px; padding: 10px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332; border-bottom: 2px solid #FFD700; padding-bottom: 4px;"><strong>Total Parameters</strong></h5>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.8em; margin: 4px 0; line-height: 1.3;">
                                $$\\sum_l (n_l \\times n_{l-1} + n_l)$$
                            </div>
                            <p style="font-size: 0.85em; margin: 6px 0 4px 0;"><strong>Example:</strong><br/>784 ‚Üí 128 ‚Üí 64 ‚Üí 10</p>
                            <div style="background: white; padding: 6px; border-radius: 4px; font-size: 0.75em; margin: 4px 0; line-height: 1.3; text-align: center;">
                                $100{,}352 + 8{,}256 + 650$<br/>$= 109{,}258$ params
                            </div>
                        </div>
                    </div>

                    <div class="definition" style="margin-top: 10px;">
                        <strong>Memory Scales With:</strong> Network depth (more layers), layer width (more neurons), and batch size (more samples processed together)
                    </div>
                `
            },
            // Slide 18: Design Considerations
            {
                title: "Network Design Considerations: Depth vs Width",
                content: `
                    <h4>Understanding how layer-by-layer transformations reshape data and learning dynamics</h4>

                    <img src="/images/module12/layer_transformation.png" alt="Layer-by-layer data transformation" class="viz-image">
                    <p class="viz-caption">How input data is progressively transformed through hidden layers into linearly separable representations</p>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Deeper Networks</h4>
                                <p><strong>Strategy:</strong> More layers, fewer neurons per layer</p>
                                <ul>
                                    <li>‚úì Better feature hierarchies (low-level ‚Üí high-level)</li>
                                    <li>‚úì Can represent more complex functions</li>
                                    <li>‚úì More parameter efficiency</li>
                                    <li>‚ö† Risk: vanishing gradients (solved with ReLU + Batch Norm)</li>
                                    <li>‚ö† Harder to train (requires careful initialization)</li>
                                </ul>
                                <h5>Best For:</h5>
                                <p>Complex hierarchical problems (image recognition, NLP), where features build on each other</p>
                            </div>
                        </div>
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Wider Networks</h4>
                                <p><strong>Strategy:</strong> Fewer layers, more neurons per layer</p>
                                <ul>
                                    <li>‚úì Easier to train (fewer gradient bottlenecks)</li>
                                    <li>‚úì More capacity per layer</li>
                                    <li>‚úì Faster training</li>
                                    <li>‚ö† Risk: overfitting (needs regularization)</li>
                                    <li>‚ö† More parameters overall</li>
                                </ul>
                                <h5>Best For:</h5>
                                <p>Simpler problems with abundant regularization, or when training efficiency matters</p>
                            </div>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>Architecture Design Guidelines</h4>
                        <table>
                            <tr>
                                <th>Design Choice</th>
                                <th>Recommendation</th>
                                <th>Reasoning</th>
                            </tr>
                            <tr>
                                <td>Hidden Layer Size</td>
                                <td>Start: $\\sqrt{n_{in} \\times n_{out}}$<br/>Range: $n_{in}$ to $10 \cdot n_{in}$</td>
                                <td>Between input and output dimensions usually works; adjust based on performance</td>
                            </tr>
                            <tr>
                                <td>Number of Layers</td>
                                <td>1-2 simple ‚Üí 3-4 moderate ‚Üí 5+ complex</td>
                                <td>Match problem complexity; deeper for hierarchical features</td>
                            </tr>
                            <tr>
                                <td>Very Deep (20+)</td>
                                <td>Use Batch Norm, residual connections, good initialization</td>
                                <td>Requires special techniques to prevent training issues</td>
                            </tr>
                            <tr>
                                <td>Network Shape</td>
                                <td>Funnel (wider‚Üínarrower) or rectangle (constant)</td>
                                <td>Funnel reduces dimensions progressively; rectangle maintains capacity</td>
                            </tr>
                        </table>

                        <h5>Practical Strategy</h5>
                        <p><strong>Start small:</strong> 1-2 hidden layers ‚Üí Increase depth gradually if needed ‚Üí Use validation performance as guide ‚Üí Apply batch normalization once depth exceeds ~5 layers</p>
                    </div>
                `
            },
            // Slide 19: Forward Propagation Flow
            {
                title: "Forward Propagation: Information Flow Through Layers",
                content: `
                    <h4>How data transforms through each layer: linear ‚Üí non-linear ‚Üí output space</h4>

                    <img src="/images/module12/activation_impact.png" alt="Activation function impact on data transformation" class="viz-image">
                    <p class="viz-caption">Visualization showing how sigmoid vs ReLU activation functions shape feature representations at each layer</p>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Layer-by-Layer Computation</h4>
                                <div class="math-block">
$$z^{(1)} = W^{(1)}x + b^{(1)} \\quad \\text{(pre-activation)}$$
$$a^{(1)} = \\sigma(z^{(1)}) \\quad \\text{(post-activation)}$$

$$z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$$
$$a^{(2)} = \\sigma(z^{(2)})$$

$$\\hat{y} = a^{(L)} \\quad \\text{(output)}$$
                                </div>
                            </div>
                        </div>
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Key Forward Pass Concepts</h4>
                                <ul>
                                    <li><strong>Left-to-right flow:</strong> Information passes sequentially from input to output</li>
                                    <li><strong>Linear transformation:</strong> Each layer performs $z = Wa + b$</li>
                                    <li><strong>Non-linearity:</strong> Activation function $\\sigma(z)$ creates expressiveness</li>
                                    <li><strong>State storage:</strong> All intermediate values $\\{z^{(l)}, a^{(l)}\\}$ stored for backpropagation</li>
                                    <li><strong>Output prediction:</strong> Final layer output $\\hat{y}$ is the network's prediction</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 20: Forward Propagation Algorithm
            {
                title: "Forward Propagation Algorithm",
                content: `
                    <h4>Step-by-Step Process:</h4>
                    <div class="code-block">
Algorithm: Forward Propagation
Input: $x$, weights $\\{W^{(l)}\\}$, biases $\\{b^{(l)}\\}$

1. Set $a^{(0)} = x$
2. For each layer $l = 1$ to $L$:
   - Compute pre-activation: $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$
   - Apply activation: $a^{(l)} = \\sigma^{(l)}(z^{(l)})$
3. Output: $\\hat{y} = a^{(L)}$
                    </div>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Vectorized Implementation</h5>
                                <p>For batch processing:</p>
                                <div class="math-block">
$$Z^{(l)} = A^{(l-1)} (W^{(l)})^T + b^{(l)}$$
$$A^{(l)} = \\sigma^{(l)}(Z^{(l)})$$
                                </div>
                                <p>where $A^{(l)}$ has shape $(m, n_l)$ for $m$ examples.</p>
                                <p><strong>Complexity:</strong> $O(L \\cdot N \\cdot M)$</p>
                                <p>$L$ = layers, $N$ = max neurons/layer, $M$ = batch size</p>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Example Calculation</h5>
                                <p>Network: 2 ‚Üí 3 ‚Üí 1</p>
                                <p>Input: $x = [0.5, 0.8]^T$</p>
                                <p><strong>Layer 1:</strong></p>
                                <div class="math-block">
$$z^{(1)} = W^{(1)}x + b^{(1)}$$
$$a^{(1)} = \\sigma(z^{(1)})$$
                                </div>
                                <p><strong>Layer 2:</strong></p>
                                <div class="math-block">
$$z^{(2)} = w^{(2)T}a^{(1)} + b^{(2)}$$
$$\\hat{y} = \\sigma(z^{(2)})$$
                                </div>
                                <p>All intermediate values $z^{(l)}, a^{(l)}$ are stored for backpropagation.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 21: Implementation Details
            {
                title: "Network Depth Impact on Learning Capacity",
                content: `
                    <h4>How network depth affects expressiveness: shallow vs deep architectures</h4>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="/images/module12/network_depth_comparison.png" alt="Network depth comparison" class="viz-image" style="max-height: 280px;">
                        </div>
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="/images/module12/scaling_laws.png" alt="Network scaling laws" class="viz-image" style="max-height: 280px;">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <p class="viz-caption" style="text-align: center; margin: 0;">Shallow vs Deep: Decision boundary expressiveness comparison</p>
                        <p class="viz-caption" style="text-align: center; margin: 0;">Scaling laws: How performance improves with network size and depth</p>
                    </div>

                    <div class="dense-info-card">
                        <h4>Implementation Considerations</h4>

                        <h5>Memory & Computational Complexity</h5>
                        <p><strong>Forward Pass Storage:</strong> We must store all intermediate values for backpropagation</p>
                        <div class="math-block">
$$\\text{Memory} \\propto \\sum_l (n_l \\times m) + \\text{parameters}$$
                        </div>
                        <p>where $n_l$ = neurons in layer $l$, $m$ = batch size</p>
                        <table>
                            <tr>
                                <th>Memory Factor</th>
                                <th>Impact</th>
                                <th>Trade-off</th>
                            </tr>
                            <tr>
                                <td>Larger batch sizes</td>
                                <td>More memory, better GPU utilization</td>
                                <td>Limited by GPU VRAM</td>
                            </tr>
                            <tr>
                                <td>Deeper networks</td>
                                <td>Store more intermediate values</td>
                                <td>More information, more memory</td>
                            </tr>
                            <tr>
                                <td>Wider layers</td>
                                <td>More activations to store</td>
                                <td>Faster forward pass, higher memory</td>
                            </tr>
                        </table>

                        <h5>Numerical Stability in Deep Networks</h5>
                        <ul>
                            <li><strong>Overflow:</strong> Intermediate values grow too large ‚Üí use activation clipping</li>
                            <li><strong>Underflow:</strong> Very small values ‚Üí zero, losing information</li>
                            <li><strong>Gradient issues:</strong> Very large/small gradients in early layers</li>
                        </ul>

                        <h5>Solutions for Deep Networks</h5>
                        <ul>
                            <li>‚úì Use <strong>ReLU</strong> activations (avoid sigmoid/tanh)</li>
                            <li>‚úì Apply <strong>Batch Normalization</strong> (stabilizes learning, normalizes layer inputs)</li>
                            <li>‚úì Use <strong>Residual connections</strong> (skip connections for very deep: 50+ layers)</li>
                            <li>‚úì Careful <strong>weight initialization</strong> (Xavier for tanh, He for ReLU)</li>
                            <li>‚úì <strong>Gradient clipping</strong> if gradients explode during training</li>
                        </ul>
                    </div>
                `
            },
            // Slide 22: Handworked Example Part 1
            {
                title: "Forward Pass: Handworked Example",
                content: `
                    <h4>Network: 2 inputs ‚Üí 2 hidden ‚Üí 1 output (sigmoid activation)</h4>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Given</h5>
                                <p><strong>Input:</strong></p>
                                <div class="math-block">
$$x = [0.5, 0.8]^T$$
                                </div>
                                <p><strong>Weights & Biases:</strong></p>
                                <div class="math-block">
$$W^{(1)} = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.1 \\end{bmatrix}, \\quad b^{(1)} = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$$

$$W^{(2)} = \\begin{bmatrix} 0.6 & 0.5 \\end{bmatrix}, \\quad b^{(2)} = 0.3$$
                                </div>
                                <p><strong>Activation:</strong></p>
                                <div class="math-block">
$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$
                                </div>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="highlight">
                                <h5>Step 1: Hidden Layer</h5>
                                <div class="math-block">
$$z^{(1)} = W^{(1)}x + b^{(1)}$$

$$= \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.3 & 0.1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$$

$$= \\begin{bmatrix} 0.2(0.5) + 0.4(0.8) \\\\ 0.3(0.5) + 0.1(0.8) \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$$

$$= \\begin{bmatrix} 0.1 + 0.32 \\\\ 0.15 + 0.08 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$$

$$= \\begin{bmatrix} 0.52 \\\\ 0.43 \\end{bmatrix}$$
                                </div>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 23: Handworked Example Part 2
            {
                title: "Forward Pass: Handworked Example (continued)",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="highlight">
                                <h5>Step 2: Hidden Activations</h5>
                                <div class="math-block">
$$a^{(1)} = \\sigma(z^{(1)}) = \\sigma([0.52, 0.43]^T)$$

$$a_1^{(1)} = \\sigma(0.52) = \\frac{1}{1 + e^{-0.52}} = \\frac{1}{1 + 0.595} = 0.627$$

$$a_2^{(1)} = \\sigma(0.43) = \\frac{1}{1 + e^{-0.43}} = \\frac{1}{1 + 0.651} = 0.606$$

$$a^{(1)} = \\begin{bmatrix} 0.627 \\\\ 0.606 \\end{bmatrix}$$
                                </div>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="highlight">
                                <h5>Step 3: Output Layer</h5>
                                <div class="math-block">
$$z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}$$

$$= \\begin{bmatrix} 0.6 & 0.5 \\end{bmatrix} \\begin{bmatrix} 0.627 \\\\ 0.606 \\end{bmatrix} + 0.3$$

$$= 0.6(0.627) + 0.5(0.606) + 0.3$$
$$= 0.376 + 0.303 + 0.3 = 0.979$$

<strong>Final Output:</strong>
$$\\hat{y} = \\sigma(0.979) = \\frac{1}{1 + e^{-0.979}} = 0.727$$
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="definition">
                        <strong>Summary:</strong> Input [0.5, 0.8] ‚Üí Hidden [0.627, 0.606] ‚Üí Output 0.727
                    </div>
                `
            },
            // Slide 24: Backpropagation Error Flow
            {
                title: "Backpropagation: Error Flow",
                content: `
                    <!-- SVG Backpropagation Error Flow Diagram -->
                    <div style="display: flex; justify-content: center; margin: 0 0 12px 0;">
                        <object data="/static/diagrams/backpropagation_diagram.svg" type="image/svg+xml" style="width: 100%; max-width: 750px; height: auto;"></object>
                    </div>

                    <div class="two-column">
                        <div class="two-column-left">
                            <h4>Core Algorithm:</h4>
                            <ol style="font-size: 0.9em;">
                                <li><strong>Forward Pass:</strong> Compute all activations top-to-bottom</li>
                                <li><strong>Loss Evaluation:</strong> Calculate $L$ at output</li>
                                <li><strong>Output Error:</strong> Compute $\\delta^{(L)}$</li>
                                <li><strong>Backpropagate Errors:</strong> For each layer, compute $\\delta^{(l)}$ from $\\delta^{(l+1)}$</li>
                                <li><strong>Compute Gradients:</strong> Use errors to get $\\frac{\\partial L}{\\partial W}$</li>
                            </ol>
                        </div>
                        <div class="two-column-right">
                            <h4>Key Equations:</h4>
                            <p style="font-size: 0.85em;"><strong>Output error:</strong></p>
                            <p style="font-size: 0.85em; text-align: center; background: #F0F0F0; padding: 4px; border-radius: 3px;">$\\delta^{(L)} = \\frac{\\partial L}{\\partial a^{(L)}} \\odot \\sigma'(z^{(L)})$</p>
                            <p style="font-size: 0.85em; margin-top: 6px;"><strong>Backprop error:</strong></p>
                            <p style="font-size: 0.85em; text-align: center; background: #F0F0F0; padding: 4px; border-radius: 3px;">$\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$</p>
                            <div class="highlight" style="margin-top: 8px;">
                                <strong>Efficiency:</strong> $O(n)$ time complexity, not exponential!
                            </div>
                        </div>
                    </div>
                    <div class="definition">
                        <strong>Backpropagation:</strong> Efficient algorithm to compute gradients by propagating errors backward through the network using the chain rule.
                    </div>

                    <h4 style="margin-top: 12px;">Layer-wise Analysis During Backpropagation</h4>
                    <div style="display: flex; justify-content: center; align-items: center; margin: 8px 0;">
                        <img src="/images/module12/layer_analysis.png" alt="Layer Analysis During Training" style="max-width: 100%; max-height: 260px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                    </div>
                    <p class="viz-caption" style="text-align: center;">Gradient magnitude and activation statistics across layers showing healthy gradient flow</p>
                `
            },
            // Slide 25: Chain Rule
            {
                title: "Mathematical Foundation: Chain Rule",
                content: `
                    <h4>Goal: Compute $\\frac{\\partial L}{\\partial W^{(l)}}$ and $\\frac{\\partial L}{\\partial b^{(l)}}$ for all layers</h4>
                    <p><strong>Chain Rule Application:</strong></p>
                    <div class="math-block">
$$\\frac{\\partial L}{\\partial W^{(l)}} = \\left(\\frac{\\partial L}{\\partial z^{(l)}}\\right) \\left(\\frac{\\partial z^{(l)}}{\\partial W^{(l)}}\\right)$$
$$\\frac{\\partial L}{\\partial b^{(l)}} = \\left(\\frac{\\partial L}{\\partial z^{(l)}}\\right) \\left(\\frac{\\partial z^{(l)}}{\\partial b^{(l)}}\\right)$$
$$\\frac{\\partial L}{\\partial a^{(l-1)}} = \\left(\\frac{\\partial L}{\\partial z^{(l)}}\\right) \\left(\\frac{\\partial z^{(l)}}{\\partial a^{(l-1)}}\\right)$$
                    </div>
                    <p><strong>Key Insight:</strong> Define error terms $\\delta^{(l)} = \\frac{\\partial L}{\\partial z^{(l)}}$</p>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Gradient Computations</h5>
                                <div class="math-block">
$$\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$$

$$\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}$$

$$\\delta^{(l-1)} = (W^{(l)})^T \\delta^{(l)} \\odot \\sigma'(z^{(l-1)})$$
                                </div>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Output Layer</h5>
                                <p>For output layer $L$:</p>
                                <div class="math-block">
$$\\delta^{(L)} = \\frac{\\partial L}{\\partial a^{(L)}} \\odot \\sigma'(z^{(L)})$$
                                </div>
                                <p>Common case (MSE + sigmoid):</p>
                                <div class="math-block">
$$\\delta^{(L)} = (a^{(L)} - y) \\odot a^{(L)} \\odot (1 - a^{(L)})$$
                                </div>
                            </div>
                        </div>
                    </div>
                    <p>where $\\odot$ denotes element-wise multiplication.</p>
                `
            },
            // Slide 26: Backpropagation Algorithm
            {
                title: "Backpropagation Algorithm",
                content: `
                    <div class="code-block">
Algorithm: Backpropagation
Input: Training example $(x, y)$, network weights

1. Forward Pass: Compute all $a^{(l)}$ and $z^{(l)}$ (STORE THEM!)
2. Compute Output Error: $\\delta^{(L)} = \\frac{\\partial L}{\\partial a^{(L)}} \\odot \\sigma'(z^{(L)})$
3. For $l = L-1$ down to 1:
   - Propagate Error: $\\delta^{(l)} = (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$
4. For $l = 1$ to $L$:
   - Compute Gradients:
     $\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$
     $\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}$
                    </div>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Computational Complexity</h5>
                                <p><strong>Time:</strong> O(number of weights)</p>
                                <ul>
                                    <li>Same order as forward pass</li>
                                    <li>Very efficient vs numerical gradients</li>
                                </ul>
                                <p><strong>Space:</strong> O(network size)</p>
                                <ul>
                                    <li>Must store all activations</li>
                                    <li>Memory scales with depth</li>
                                </ul>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="highlight">
                                <h5>Why Backpropagation Works</h5>
                                <ul>
                                    <li><strong>Efficiency:</strong> Reuses computations via chain rule</li>
                                    <li><strong>Automatic:</strong> No manual gradient derivation</li>
                                    <li><strong>Exact:</strong> Computes exact gradients (not approximations)</li>
                                    <li><strong>General:</strong> Works for any differentiable network</li>
                                </ul>
                                <p><strong>Historical Impact:</strong></p>
                                <ul>
                                    <li>Rumelhart, Hinton, Williams (1986)</li>
                                    <li>Made deep learning practical</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 27: Computational Graph
            {
                title: "Computational Graph Perspective",
                content: `
                    <!-- SVG Computational Graph Diagram -->
                    <div style="display: flex; justify-content: center; margin: 0 0 12px 0;">
                        <object data="/static/diagrams/computational_graph_diagram.svg" type="image/svg+xml" style="width: 100%; max-width: 800px; height: auto;"></object>
                    </div>

                    <div class="two-column">
                        <div class="two-column-left">
                            <h4>Modern Perspective:</h4>
                            <p><strong>Automatic Differentiation:</strong> Backpropagation viewed as systematic application of chain rule on computational graphs.</p>
                            <h4>Graph Components:</h4>
                            <ul style="font-size: 0.9em;">
                                <li><strong>Nodes:</strong> Operations (√ó, +, œÉ)</li>
                                <li><strong>Edges:</strong> Data flow direction</li>
                                <li><strong>Inputs:</strong> Variables (x, w, b)</li>
                                <li><strong>Loss:</strong> Final output L</li>
                            </ul>
                        </div>
                        <div class="two-column-right">
                            <h4>Two Pass System:</h4>
                            <ul style="font-size: 0.9em;">
                                <li><strong>Forward Pass:</strong> Compute intermediate values z‚ÇÅ, z‚ÇÇ, y</li>
                                <li><strong>Backward Pass:</strong> Compute gradients via chain rule</li>
                            </ul>
                            <h4>Example: y = œÉ(wx + b)</h4>
                            <ul style="font-size: 0.9em;">
                                <li>z‚ÇÅ = w √ó x</li>
                                <li>z‚ÇÇ = z‚ÇÅ + b</li>
                                <li>y = œÉ(z‚ÇÇ)</li>
                            </ul>
                            <p style="font-size: 0.9em; margin-top: 8px;"><strong>Modern frameworks:</strong> TensorFlow, PyTorch, JAX build and differentiate these automatically!</p>
                        </div>
                    </div>
                    <div class="highlight">
                        <strong>Modern View:</strong> Backpropagation is automatic differentiation on computational graphs. Modern frameworks handle this automatically.
                    </div>
                `
            },
            // Slide 28: 4-Layer Derivation
            {
                title: "4-Layer Neural Network: Differential Equation Derivation",
                content: `
                    <h4>Network Structure: Input ‚Üí Hidden1 ‚Üí Hidden2 ‚Üí Hidden3 ‚Üí Output</h4>
                    <p><strong>Forward Pass Equations:</strong></p>
                    <div class="math-block">
$$a^{(0)} = x \\quad \\text{(input)}$$

$$z^{(1)} = W^{(1)}a^{(0)} + b^{(1)}, \\quad a^{(1)} = \\sigma(z^{(1)})$$
$$z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}, \\quad a^{(2)} = \\sigma(z^{(2)})$$
$$z^{(3)} = W^{(3)}a^{(2)} + b^{(3)}, \\quad a^{(3)} = \\sigma(z^{(3)})$$
$$z^{(4)} = W^{(4)}a^{(3)} + b^{(4)}, \\quad a^{(4)} = \\sigma(z^{(4)}) \\quad \\text{(output)}$$
                    </div>
                    <p><strong>Loss Function:</strong> $L = \\frac{1}{2}\\|a^{(4)} - y\\|^2$</p>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Output Layer Error</h5>
                                <p>Starting from the output layer:</p>
                                <div class="math-block">
$$\\delta^{(4)} = \\frac{\\partial L}{\\partial z^{(4)}}$$
$$= \\frac{\\partial L}{\\partial a^{(4)}} \\odot \\frac{\\partial a^{(4)}}{\\partial z^{(4)}}$$
$$= (a^{(4)} - y) \\odot \\sigma'(z^{(4)})$$
                                </div>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Chain Rule for Hidden Layers</h5>
                                <p>For hidden layers ($l = 3, 2, 1$):</p>
                                <div class="math-block">
$$\\delta^{(l)} = \\frac{\\partial L}{\\partial z^{(l)}}$$
$$= \\frac{\\partial L}{\\partial z^{(l+1)}} \\frac{\\partial z^{(l+1)}}{\\partial a^{(l)}} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}$$
$$= (W^{(l+1)})^T \\delta^{(l+1)} \\odot \\sigma'(z^{(l)})$$
                                </div>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 29: Complete 4-Layer Backprop
            {
                title: "4-Layer Network: Complete Backpropagation Derivation",
                content: `
                    <h4 style="margin-bottom: 10px;">Complete Gradient Computation Through All Layers</h4>

                    <!-- Two-column mathematical formulation -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 12px;">
                        <!-- Left: Error Propagation -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Error Propagation (Backward)</strong></h5>

                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <div style="margin-bottom: 8px; padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #8B0000; margin-bottom: 3px;">Layer 4 (Output):</div>
                                    <div style="text-align: center;">$$\\delta^{(4)} = (a^{(4)} - y) \\odot \\sigma'(z^{(4)})$$</div>
                                </div>

                                <div style="margin-bottom: 8px; padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #8B0000; margin-bottom: 3px;">Layer 3:</div>
                                    <div style="text-align: center;">$$\\delta^{(3)} = (W^{(4)})^T \\delta^{(4)} \\odot \\sigma'(z^{(3)})$$</div>
                                </div>

                                <div style="margin-bottom: 8px; padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #8B0000; margin-bottom: 3px;">Layer 2:</div>
                                    <div style="text-align: center;">$$\\delta^{(2)} = (W^{(3)})^T \\delta^{(3)} \\odot \\sigma'(z^{(2)})$$</div>
                                </div>

                                <div style="padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #8B0000; margin-bottom: 3px;">Layer 1:</div>
                                    <div style="text-align: center;">$$\\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\odot \\sigma'(z^{(1)})$$</div>
                                </div>
                            </div>
                        </div>

                        <!-- Right: Parameter Gradients & Updates -->
                        <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Parameter Gradients & Updates</strong></h5>

                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <div style="margin-bottom: 8px;">
                                    <div style="font-weight: bold; color: #1B4332; margin-bottom: 3px; font-size: 0.9em;">For each layer $l \\in \\{1,2,3,4\\}$:</div>
                                </div>

                                <div style="margin-bottom: 8px; padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #1B4332; margin-bottom: 3px; font-size: 0.9em;">Weight Gradients:</div>
                                    <div style="text-align: center;">$$\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$$</div>
                                </div>

                                <div style="margin-bottom: 8px; padding: 8px; background: white; border-radius: 3px;">
                                    <div style="font-weight: bold; color: #1B4332; margin-bottom: 3px; font-size: 0.9em;">Bias Gradients:</div>
                                    <div style="text-align: center;">$$\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}$$</div>
                                </div>

                                <div style="padding: 8px; background: white; border-radius: 3px; border: 2px solid #1B4332;">
                                    <div style="font-weight: bold; color: #1B4332; margin-bottom: 4px; font-size: 0.9em;">Parameter Updates (Œ± = learning rate):</div>
                                    <div style="text-align: center; font-size: 0.85em;">
                                        $$W^{(l)} := W^{(l)} - \\alpha \\frac{\\partial L}{\\partial W^{(l)}}$$
                                    </div>
                                    <div style="text-align: center; font-size: 0.85em;">
                                        $$b^{(l)} := b^{(l)} - \\alpha \\frac{\\partial L}{\\partial b^{(l)}}$$
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="insight-box" style="padding: 10px;">
                        <strong style="font-size: 0.9em;">Key Insight:</strong> <span style="font-size: 0.85em;">The error flows backward through the network: Each layer's error (Œ¥) depends on the next layer's error multiplied by the transpose of connecting weights. Gradients are computed from these errors, then weights are updated in the direction opposite to gradients.</span>
                    </div>
                `
            },
            // Slide 30: Gradient Descent
            {
                title: "Gradient Descent Optimization: Understanding Loss Landscapes",
                content: `
                    <h4>How networks find good weights by following gradients down loss surfaces</h4>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <img src="/images/module12/loss_landscape_3d.png" alt="3D loss landscape" class="viz-image">
                            <p class="viz-caption">3D loss surfaces for different optimization problems showing valleys and plateaus</p>
                        </div>
                        <div class="image-container">
                            <img src="/images/module12/gradient_descent_paths.png" alt="Gradient descent trajectories" class="viz-image">
                            <p class="viz-caption">Learning rate impact on convergence: Too small (slow), optimal (convergent), too large (divergent)</p>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>Weight Update Rule</h4>
                        <div class="math-block">
$$W^{(l)} := W^{(l)} - \\alpha \\cdot \\frac{\\partial L}{\\partial W^{(l)}}$$
$$b^{(l)} := b^{(l)} - \\alpha \\cdot \\frac{\\partial L}{\\partial b^{(l)}}$$
                        </div>
                        <p>where $\\alpha$ is the <strong>learning rate</strong> ‚Äî the step size controlling how much we move in the gradient direction.</p>

                        <h5>Learning Rate Effects</h5>
                        <table>
                            <tr>
                                <th>Learning Rate</th>
                                <th>Behavior</th>
                                <th>Convergence</th>
                                <th>Issue</th>
                            </tr>
                            <tr>
                                <td>$\\alpha \\ll 10^{-4}$</td>
                                <td>Tiny steps, crawls downhill</td>
                                <td>Very slow</td>
                                <td>Training takes forever</td>
                            </tr>
                            <tr>
                                <td>$\\alpha = 10^{-4}$ to $10^{-2}$ ‚úì</td>
                                <td>Steady descent, smooth convergence</td>
                                <td>Good</td>
                                <td>None (optimal range)</td>
                            </tr>
                            <tr>
                                <td>$\\alpha = 10^{-1}$ to $10^{0}$</td>
                                <td>Large steps, may overshoot</td>
                                <td>Unstable</td>
                                <td>Loss may bounce or diverge</td>
                            </tr>
                            <tr>
                                <td>$\\alpha \\gg 1$</td>
                                <td>Huge jumps, wildly diverges</td>
                                <td>Fails (NaN)</td>
                                <td>Loss explodes immediately</td>
                            </tr>
                        </table>

                        <h5>Advanced Optimizers</h5>
                        <ul>
                            <li><strong>SGD:</strong> Standard gradient descent (simple, reliable)</li>
                            <li><strong>Momentum:</strong> Accumulates gradient direction (accelerates convergence)</li>
                            <li><strong>Adam (Adaptive Moment Estimation):</strong> Default choice‚Äîadapts learning rate per parameter based on gradient history</li>
                            <li><strong>RMSprop:</strong> Similar to Adam, slightly different formulation</li>
                        </ul>

                        <h5>Practical Recommendation</h5>
                        <p><strong>Default strategy:</strong> Use Adam optimizer with default hyperparameters (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999). Only adjust learning rate if training is too slow or unstable. Start with Œ±=0.001 and adjust from there.</p>
                    </div>
                `
            },
            // Slide 31: Overfitting Problem
            {
                title: "The Overfitting Problem",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <img src="/images/module12/loss_curves.png" alt="Overfitting Demo" class="viz-image">
                            <div class="image-caption">Training vs Validation Loss - Detecting Overfitting</div>
                        </div>
                        <div class="two-column-right">
                            <h4>Symptoms:</h4>
                            <ul>
                                <li>High training accuracy, low validation</li>
                                <li>Model memorizes noise</li>
                                <li>Complex decision boundaries</li>
                                <li>Large train-validation gap</li>
                            </ul>
                            <h4>Root Causes:</h4>
                            <ul>
                                <li>Too many parameters</li>
                                <li>Training too long</li>
                                <li>Insufficient regularization</li>
                                <li>Model complexity > problem complexity</li>
                            </ul>
                        </div>
                    </div>
                    <div class="warning">
                        <strong>Overfitting:</strong> Model learns training data too well, memorizing noise instead of generalizable patterns.
                    </div>
                `
            },
            // Slide 32: L2 Regularization (Ridge)
            {
                title: "L2 Regularization (Ridge): Weight Decay",
                content: `
                    <h4>Most popular regularization technique: Add L2 norm penalty to loss function</h4>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Left: Mathematical Foundation -->
                        <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Mathematical Form</strong></h5>
                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <p style="margin: 6px 0;"><strong>Loss Function:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; margin-bottom: 8px;">
                                    $$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_l \\|W^{(l)}\\|_2^2$$
                                </div>

                                <p style="margin: 6px 0;"><strong>Where:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; margin-bottom: 8px; font-size: 0.8em;">
                                    $$\\|W^{(l)}\\|_2^2 = \\sum_i \\sum_j (W_{ij}^{(l)})^2$$
                                </div>

                                <p style="margin: 6px 0;"><strong>Gradient Update:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.8em;">
                                    $$\\frac{\\partial L_{\\text{total}}}{\\partial W^{(l)}} = \\frac{\\partial L_{\\text{data}}}{\\partial W^{(l)}} + 2\\lambda W^{(l)}$$
                                </div>
                            </div>
                        </div>

                        <!-- Right: Effects and Interpretation -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Effect on Weights</strong></h5>
                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <p style="margin: 6px 0;"><strong>How it works:</strong></p>
                                <ul style="margin: 4px 0 8px 15px; padding: 0; list-style: none;">
                                    <li style="margin: 4px 0 4px 20px;">‚úì Adds penalty term proportional to weight magnitude</li>
                                    <li style="margin: 4px 0 4px 20px;">‚úì Shrinks all weights towards zero uniformly</li>
                                    <li style="margin: 4px 0 4px 20px;">‚úì Prefers smaller, smoother weights</li>
                                </ul>

                                <p style="margin: 6px 0;"><strong>Intuition:</strong></p>
                                <ul style="margin: 4px 0 0 15px; padding: 0; list-style: none;">
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Large weights are penalized heavily</li>
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Encourages distributed, smooth solutions</li>
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Prevents over-reliance on specific features</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Visualization and Characteristics -->
                    <div class="dense-info-card">
                        <h5>Key Characteristics</h5>
                        <table>
                            <tr>
                                <th>Property</th>
                                <th>Description</th>
                                <th>Implication</th>
                            </tr>
                            <tr>
                                <td><strong>Weight Behavior</strong></td>
                                <td>All weights shrink smoothly</td>
                                <td>No exact zeros in final model</td>
                            </tr>
                            <tr>
                                <td><strong>Penalty Function</strong></td>
                                <td>Quadratic ($W^2$)</td>
                                <td>Larger weights penalized more heavily</td>
                            </tr>
                            <tr>
                                <td><strong>Sparsity</strong></td>
                                <td>Low sparsity</td>
                                <td>Keeps all features (no automatic selection)</td>
                            </tr>
                            <tr>
                                <td><strong>Use Case</strong></td>
                                <td>General-purpose regularization</td>
                                <td>Default choice for most problems</td>
                            </tr>
                            <tr>
                                <td><strong>$\\lambda$ Selection</strong></td>
                                <td>Typically 0.0001 to 0.1</td>
                                <td>Start small, tune via validation</td>
                            </tr>
                        </table>
                    </div>

                    <!-- Visualization Image -->
                    <h4 style="margin-top: 12px;">L2 Regularization Effect Visualization</h4>
                    <div style="display: flex; justify-content: center; align-items: center; margin: 8px 0;">
                        <img src="/images/module12/l1_vs_l2_regularization.png" alt="L1 vs L2 Regularization" style="max-width: 100%; max-height: 260px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                    </div>
                    <p class="viz-caption" style="text-align: center;">L2 regularization creates smooth solutions in weight space (circular contours)</p>

                    <div class="highlight">
                        <strong>Practical Tip:</strong> L2 regularization is the default in most frameworks. PyTorch's \`weight_decay\` parameter implements L2 regularization directly in the optimizer.
                    </div>
                `
            },
            // Slide 33: L1 Regularization (Lasso)
            {
                title: "L1 Regularization (Lasso): Feature Selection",
                content: `
                    <h4>Promotes sparse solutions: automatic feature selection and model simplification</h4>

                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Left: Mathematical Foundation -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000;"><strong>Mathematical Form</strong></h5>
                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <p style="margin: 6px 0;"><strong>Loss Function:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; margin-bottom: 8px;">
                                    $$L_{\\text{total}} = L_{\\text{data}} + \\lambda \\sum_l \\|W^{(l)}\\|_1$$
                                </div>

                                <p style="margin: 6px 0;"><strong>Where:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; margin-bottom: 8px; font-size: 0.8em;">
                                    $$\\|W^{(l)}\\|_1 = \\sum_i \\sum_j |W_{ij}^{(l)}|$$
                                </div>

                                <p style="margin: 6px 0;"><strong>Gradient Update:</strong></p>
                                <div style="background: white; padding: 8px; border-radius: 4px; font-size: 0.8em;">
                                    $$\\frac{\\partial L_{\\text{total}}}{\\partial W^{(l)}} = \\frac{\\partial L_{\\text{data}}}{\\partial W^{(l)}} + \\lambda \\text{sign}(W^{(l)})$$
                                </div>
                            </div>
                        </div>

                        <!-- Right: Effects and Interpretation -->
                        <div style="background: #FFFAF5; border-left: 4px solid #8B0000; border-radius: 6px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000;"><strong>Effect on Weights</strong></h5>
                            <div style="font-size: 0.85em; line-height: 1.6;">
                                <p style="margin: 6px 0;"><strong>How it works:</strong></p>
                                <ul style="margin: 4px 0 8px 15px; padding: 0; list-style: none;">
                                    <li style="margin: 4px 0 4px 20px;">‚úì Adds penalty proportional to absolute weight value</li>
                                    <li style="margin: 4px 0 4px 20px;">‚úì Drives many weights exactly to zero</li>
                                    <li style="margin: 4px 0 4px 20px;">‚úì Creates sparse, interpretable models</li>
                                </ul>

                                <p style="margin: 6px 0;"><strong>Intuition:</strong></p>
                                <ul style="margin: 4px 0 0 15px; padding: 0; list-style: none;">
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Constant penalty regardless of magnitude</li>
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Promotes exact zeros (automatic feature removal)</li>
                                    <li style="margin: 4px 0 4px 20px;">‚Ä¢ Performs implicit feature selection</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Visualization and Characteristics -->
                    <div class="dense-info-card">
                        <h5>Key Characteristics</h5>
                        <table>
                            <tr>
                                <th>Property</th>
                                <th>Description</th>
                                <th>Implication</th>
                            </tr>
                            <tr>
                                <td><strong>Weight Behavior</strong></td>
                                <td>Many weights become exactly zero</td>
                                <td>Natural feature selection</td>
                            </tr>
                            <tr>
                                <td><strong>Penalty Function</strong></td>
                                <td>Linear ($|W|$)</td>
                                <td>Constant penalty per unit magnitude</td>
                            </tr>
                            <tr>
                                <td><strong>Sparsity</strong></td>
                                <td>High sparsity possible</td>
                                <td>Many weights pruned to zero</td>
                            </tr>
                            <tr>
                                <td><strong>Use Case</strong></td>
                                <td>Feature selection needed</td>
                                <td>Interpretable, efficient models</td>
                            </tr>
                            <tr>
                                <td><strong>$\\lambda$ Selection</strong></td>
                                <td>Typically 0.0001 to 0.01</td>
                                <td>Higher $\\lambda$ ‚Üí more sparsity</td>
                            </tr>
                        </table>
                    </div>

                    <div class="highlight">
                        <strong>When to Use L1:</strong> When you have many features, want interpretability, or need model compression. Rarely used alone in deep learning; more common in traditional ML.
                    </div>
                `
            },
            // Slide 34: L1 vs L2 Comparison
            {
                title: "L1 vs L2 Regularization Comparison",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <img src="/images/module12/gradient_solutions.png" alt="L1 vs L2" class="viz-image">
                            <div class="image-caption">L1 vs L2 Regularization Effects (Solution Comparison)</div>
                            <p style="margin-top: 15px;">L1 creates sparse solutions (feature selection), L2 provides smooth weight shrinkage</p>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>When to Use L2</h5>
                                <ul>
                                    <li>General-purpose regularization</li>
                                    <li>All features potentially relevant</li>
                                    <li>Want smooth weight shrinkage</li>
                                    <li>Most common choice</li>
                                </ul>
                            </div>
                            <div class="definition">
                                <h5>When to Use L1</h5>
                                <ul>
                                    <li>Feature selection needed</li>
                                    <li>Many irrelevant features</li>
                                    <li>Want sparse models</li>
                                    <li>Interpretability important</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 35: Dropout
            {
                title: "Dropout: A Different Approach",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <img src="/images/module12/gradient_magnitude_layers.png" alt="Dropout" class="viz-image">
                            <div class="image-caption">Dropout & Regularization Effects on Network Training</div>
                        </div>
                        <div class="two-column-right">
                            <h4>Dropout Technique:</h4>
                            <p>Randomly <strong>set neurons to zero</strong> during training to prevent co-adaptation and improve generalization.</p>
                            <h4>Training vs Testing:</h4>
                            <ul>
                                <li><strong>Training:</strong> Randomly drop neurons (e.g., p=0.5)</li>
                                <li><strong>Testing:</strong> All neurons active, but scaled</li>
                            </ul>
                            <h4>Benefits:</h4>
                            <ul>
                                <li>Prevents complex co-adaptations</li>
                                <li>Model averaging effect (ensemble)</li>
                                <li>Robust feature learning</li>
                                <li>Easy to implement</li>
                            </ul>
                        </div>
                    </div>
                    <div class="highlight">
                        <strong>Typical Rates:</strong> 0.2-0.5 for hidden layers, 0.1-0.2 for input layer
                    </div>
                `
            },
            // Slide 35: Dropout Mathematics
            {
                title: "Dropout: Mathematical Formulation",
                content: `
                    <p><strong>Training Phase:</strong></p>
                    <div class="math-block">
$$r^{(l)} \\sim \\text{Bernoulli}(p) \\quad \\text{(dropout mask)}$$
$$\\tilde{a}^{(l)} = r^{(l)} \\odot a^{(l)} \\quad \\text{(apply mask)}$$
$$z^{(l+1)} = W^{(l+1)} \\tilde{a}^{(l)} + b^{(l+1)}$$
                    </div>
                    <p><strong>Testing Phase:</strong></p>
                    <div class="math-block">
$$z^{(l+1)} = p \\cdot W^{(l+1)} a^{(l)} + b^{(l+1)} \\quad \\text{(scale weights)}$$
                    </div>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="definition">
                                <h5>Dropout Benefits</h5>
                                <ul>
                                    <li><strong>Prevents overfitting:</strong> Reduces complex co-adaptations</li>
                                    <li><strong>Model averaging:</strong> Approximates ensemble of networks</li>
                                    <li><strong>Robust features:</strong> Forces redundant representations</li>
                                    <li><strong>Easy to implement:</strong> Simple modification to forward pass</li>
                                </ul>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Implementation Notes</h5>
                                <p><strong>Training vs Testing:</strong></p>
                                <ul>
                                    <li>Training: Randomly drop neurons</li>
                                    <li>Testing: Use all neurons but scale outputs</li>
                                    <li>Modern frameworks handle this automatically</li>
                                </ul>
                                <p><strong>Why Scaling Works:</strong></p>
                                <ul>
                                    <li>Training: Neuron "on" with probability p</li>
                                    <li>Testing: All "on", so scale by p to match</li>
                                    <li>Maintains expected activation levels</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 36: Regularization Comparison
            {
                title: "Regularization Comparison",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <img src="/images/module12/accuracy_curves.png" alt="Regularization Comparison" class="viz-image">
                            <div class="image-caption">Regularization Impact on Training & Validation Performance</div>
                        </div>
                        <div class="two-column-right">
                            <div class="highlight">
                                <h5>Choosing Regularization</h5>
                                <p><strong>Start with:</strong></p>
                                <ul>
                                    <li>L2 regularization (Œª = 0.01)</li>
                                    <li>Dropout (rate = 0.5)</li>
                                    <li>Early stopping</li>
                                </ul>
                                <p><strong>If still overfitting:</strong></p>
                                <ul>
                                    <li>Increase regularization strength</li>
                                    <li>Add more dropout</li>
                                    <li>Reduce model complexity</li>
                                </ul>
                            </div>
                            <div class="definition">
                                <h5>Other Techniques</h5>
                                <p><strong>Early Stopping:</strong></p>
                                <ul>
                                    <li>Monitor validation loss</li>
                                    <li>Stop when it starts increasing</li>
                                    <li>Simple and effective</li>
                                </ul>
                                <p><strong>Data Augmentation:</strong></p>
                                <ul>
                                    <li>Artificially increase training data</li>
                                    <li>Add noise, rotations, etc.</li>
                                    <li>Domain-specific techniques</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            // Slide 37: Training Curves
            {
                title: "Training Curves: Detecting Overfitting and Convergence Patterns",
                content: `
                    <h4>Monitoring loss and accuracy during training reveals model behavior and optimization issues</h4>

                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin-bottom: 12px;">
                        <div class="image-container">
                            <img src="/images/module12/loss_curves.png" alt="Loss curves during training" class="viz-image">
                            <p class="viz-caption">Training vs validation loss: Gap indicates overfitting magnitude</p>
                        </div>
                        <div class="image-container">
                            <img src="/images/module12/accuracy_curves.png" alt="Accuracy curves during training" class="viz-image">
                            <p class="viz-caption">Training vs validation accuracy: Shows generalization performance over epochs</p>
                        </div>
                        <div class="image-container">
                            <img src="/images/module12/weight_evolution.png" alt="Weight evolution during training" class="viz-image">
                            <p class="viz-caption">Weight parameter evolution: Magnitude and distribution changes over epochs</p>
                        </div>
                    </div>

                    <div class="dense-info-card">
                        <h4>Interpreting Training Curves</h4>

                        <h5>Healthy Training (What You Want)</h5>
                        <ul>
                            <li>Both training and validation curves decrease smoothly</li>
                            <li>Small gap between training and validation (< 5% difference)</li>
                            <li>Curves plateau as training continues</li>
                            <li>No sudden spikes or oscillations</li>
                        </ul>

                        <h5>Overfitting Signals (Training-Validation Divergence)</h5>
                        <table>
                            <tr>
                                <th>Pattern</th>
                                <th>What It Means</th>
                                <th>Solutions</th>
                            </tr>
                            <tr>
                                <td>Training ‚Üì, Validation ‚Üë</td>
                                <td>Model memorizing training data</td>
                                <td>Add L1/L2 regularization, more dropout, less complexity</td>
                            </tr>
                            <tr>
                                <td>Very large gap at end</td>
                                <td>Severe overfitting</td>
                                <td>Reduce capacity, use more regularization, more data</td>
                            </tr>
                            <tr>
                                <td>Both decrease slowly</td>
                                <td>Learning rate too low or insufficient capacity</td>
                                <td>Increase learning rate, add hidden layers/neurons</td>
                            </tr>
                            <tr>
                                <td>Curves oscillate wildly</td>
                                <td>Learning rate too high or batch too small</td>
                                <td>Decrease learning rate, increase batch size</td>
                            </tr>
                        </table>

                        <h5>Practical Diagnosis Strategy</h5>
                        <ol>
                            <li><strong>Early stopping point:</strong> Stop training when validation loss starts increasing (prevent overfitting)</li>
                            <li><strong>Gap analysis:</strong> If train-val gap < 2% ‚Üí good generalization; 2-5% ‚Üí acceptable; > 10% ‚Üí overfitting</li>
                            <li><strong>Learning rate tuning:</strong> Loss should decrease smoothly‚Äîif bouncing, reduce learning rate</li>
                            <li><strong>Final plateau:</strong> Check if curves have plateaued‚Äîif not, may benefit from more epochs</li>
                        </ol>
                    </div>
                `
            },
            // Slide 38: Weight Initialization
            {
                title: "Weight Initialization: Critical for Deep Learning Success",
                content: `
                    <h4 style="margin-bottom: 10px;">Why Initial Weights Matter</h4>
                    <p style="margin-bottom: 12px; font-size: 0.9em;">Poor initialization prevents learning or causes training failure. Good initialization enables stable gradient flow from the start.</p>

                    <!-- Bad vs Good Comparison -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Left: Poor Methods -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #8B0000;"><strong>‚ùå Poor Methods (DON'T USE)</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>All Zeros:</strong> $W = 0$ <br/> <span style="color: #555;">No learning (symmetry)</span>
                                </div>
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>U(0,1):</strong> Too large <br/> <span style="color: #555;">Gradients vanish</span>
                                </div>
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>N(0,1):</strong> Very large <br/> <span style="color: #555;">Exploding gradients</span>
                                </div>
                                <div style="padding: 6px; background: white; border-radius: 3px;">
                                    <strong>N(0,0.001):</strong> Too small <br/> <span style="color: #555;">Extremely slow</span>
                                </div>
                            </div>
                        </div>

                        <!-- Right: Good Methods -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>‚úì Recommended Methods</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>Xavier/Glorot:</strong> $\\mathcal{N}(0, \\sqrt{1/n_{in}})$ <br/> <span style="color: #555;">Sigmoid, Tanh</span>
                                </div>
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>He Init:</strong> $\\mathcal{N}(0, \\sqrt{2/n_{in}})$ <br/> <span style="color: #555;">ReLU (modern) ‚≠ê</span>
                                </div>
                                <div style="margin-bottom: 8px; padding: 6px; background: white; border-radius: 3px;">
                                    <strong>LeCun Normal:</strong> $\\mathcal{N}(0, \\sqrt{1/n_{in}})$ <br/> <span style="color: #555;">Convolutional nets</span>
                                </div>
                                <div style="padding: 6px; background: white; border-radius: 3px;">
                                    <strong>Bias:</strong> $b = 0$ <br/> <span style="color: #555;">All architectures</span>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Mathematical Reasoning & Image -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Left: Mathematical Principle -->
                        <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 12px;">
                            <h5 style="margin: 0 0 8px 0; font-size: 0.95em; color: #1B4332;"><strong>Mathematical Principle</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.6;">
                                <div style="margin-bottom: 6px;">
                                    <strong>Goal:</strong> Keep activation variance constant across layers
                                </div>
                                <div style="margin-bottom: 6px; padding: 6px; background: white; border-radius: 3px;">
                                    If Var(a‚ÅΩÀ°‚Åæ) grows/shrinks, early layers get very large/small signals
                                </div>
                                <div style="margin-bottom: 6px;">
                                    <strong>Xavier:</strong> Var = 1/n·µ¢‚Çô (maintain balanced signal)
                                </div>
                                <div style="padding: 6px; background: white; border-radius: 3px;">
                                    <strong>He:</strong> Var = 2/n·µ¢‚Çô (ReLU zeros ~50% of activations)
                                </div>
                            </div>
                        </div>

                        <!-- Right: Visualization Image -->
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 8px; display: flex; align-items: center; justify-content: center;">
                            <img src="/images/module12/initialization_methods.png" alt="Weight initialization methods comparison" class="viz-image" style="max-height: 240px; margin: 0;">
                        </div>
                    </div>

                    <!-- Practical Implementation -->
                    <div style="background: #F5F5F5; border-left: 4px solid #666; border-radius: 4px; padding: 10px;">
                        <h5 style="margin: 0 0 6px 0; font-size: 0.9em;"><strong>Practical Tips</strong></h5>
                        <ul style="margin: 0; padding-left: 14px; font-size: 0.8em; line-height: 1.4;">
                            <li style="margin: 2px 0;">‚úì Modern frameworks (PyTorch/TensorFlow) use sensible defaults</li>
                            <li style="margin: 2px 0;">‚úì ReLU networks: He initialization is automatic</li>
                            <li style="margin: 2px 0;">‚úì Batch Norm reduces sensitivity to initialization method</li>
                        </ul>
                    </div>
                `
            },
            // Slide 39: Learning Rate and Optimization
            {
                title: "Learning Rate and Optimization",
                content: `
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="warning">
                                <h5>Learning Rate Selection</h5>
                                <p><strong>Too high:</strong> Overshooting, instability</p>
                                <ul>
                                    <li>Loss explodes or oscillates</li>
                                    <li>Network doesn't converge</li>
                                    <li>Weights become very large</li>
                                </ul>
                                <p><strong>Too low:</strong> Slow convergence</p>
                                <ul>
                                    <li>Training takes forever</li>
                                    <li>Gets stuck in local minima</li>
                                    <li>Poor final performance</li>
                                </ul>
                                <p><strong>Good range:</strong> Typically 10<sup>-4</sup> to 10<sup>-1</sup></p>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="definition">
                                <h5>Advanced Optimizers</h5>
                                <p><strong>SGD with Momentum:</strong></p>
                                <div class="math-block">
$$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla L$$
$$W := W - \\alpha v_t$$
                                </div>
                                <p><strong>Adam (Adaptive Moments):</strong></p>
                                <div class="math-block">
$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla L$$
$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla L)^2$$
$$W := W - \\alpha \\cdot \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$
                                </div>
                                <p><strong>Default choice:</strong> Adam with $\\alpha = 0.001$</p>
                            </div>
                        </div>
                    </div>
                    <div class="highlight">
                        <strong>Learning Rate Scheduling:</strong> Decay strategies (step, exponential, cosine). Start high, reduce during training.
                    </div>

                    <h4 style="margin-top: 12px;">Learning Rate Impact & Optimizer Comparison</h4>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin: 8px 0;">
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="/images/module12/learning_rate_impact.png" alt="Learning Rate Impact on Training" style="max-width: 100%; max-height: 280px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                        </div>
                        <div style="display: flex; justify-content: center; align-items: center;">
                            <img src="/images/module12/optimizer_comparison.png" alt="Optimizer Comparison" style="max-width: 100%; max-height: 280px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                        </div>
                    </div>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px;">
                        <p class="viz-caption" style="text-align: center; margin: 0;">Effect of different learning rates on convergence speed and stability</p>
                        <p class="viz-caption" style="text-align: center; margin: 0;">SGD vs Momentum vs Adam: Convergence paths and efficiency</p>
                    </div>
                `
            },
            // Slide 40: Training Diagnostics
            {
                title: "Training Diagnostics",
                content: `
                    <h4>Monitor these metrics during training:</h4>
                    <div class="three-column">
                        <div>
                            <div class="definition">
                                <h5>Loss Monitoring</h5>
                                <ul>
                                    <li><strong>Training loss:</strong> Should decrease monotonically</li>
                                    <li><strong>Validation loss:</strong> Should decrease then stabilize</li>
                                    <li><strong>Gap:</strong> Indicates overfitting if too large</li>
                                </ul>
                                <p><strong>Warning Signs:</strong></p>
                                <ul>
                                    <li>Loss increases ‚Üí LR too high</li>
                                    <li>Loss plateaus early ‚Üí LR too low</li>
                                    <li>Val loss increases ‚Üí overfitting</li>
                                    <li>Loss becomes NaN ‚Üí gradient explosion</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Gradient Monitoring</h5>
                                <ul>
                                    <li><strong>Gradient norms:</strong> 10<sup>-6</sup> to 10<sup>-1</sup> (good)</li>
                                    <li><strong>Vanishing:</strong> Gradients ‚Üí 0</li>
                                    <li><strong>Exploding:</strong> Gradients very large</li>
                                </ul>
                                <p><strong>Health Check:</strong></p>
                                <ul>
                                    <li>‚úì Training loss decreasing?</li>
                                    <li>‚úì Gradients reasonable?</li>
                                    <li>‚úì Weights updating properly?</li>
                                    <li>‚úì Validation improving?</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Activation & Weight Monitoring</h5>
                                <ul>
                                    <li><strong>Activation stats:</strong> Mean, std, sparsity</li>
                                    <li><strong>Dead neurons:</strong> Always output zero</li>
                                    <li><strong>Saturated neurons:</strong> Always in saturation</li>
                                </ul>
                                <p><strong>Healthy:</strong></p>
                                <ul>
                                    <li>Reasonable variance</li>
                                    <li>Some sparsity</li>
                                    <li>No dead layers</li>
                                    <li>$|\\Delta W|/|W| \\approx 10^{-3}$</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <h4 style="margin-top: 12px;">Visualization of Training Diagnostics</h4>
                    <div style="display: flex; justify-content: center; align-items: center; margin: 8px 0;">
                        <img src="/images/module12/convergence_diagnostics.png" alt="Training Convergence Diagnostics" style="max-width: 100%; max-height: 280px; object-fit: contain; border: 1px solid #CCC; border-radius: 4px;">
                    </div>
                    <p class="viz-caption" style="text-align: center;">Real training metrics: Loss curves, gradient norms, weight statistics, and activation distributions across layers</p>

                    <div class="highlight">
                        <strong>Tools:</strong> TensorBoard, Weights & Biases for comprehensive monitoring and visualization
                    </div>
                `
            },
            // Slide 41: Common Problems
            {
                title: "Common Problems and Solutions: Gradient Flow Issues",
                content: `
                    <h4 style="margin-bottom: 10px;">Four Major Training Problems & Solutions</h4>

                    <!-- 2x2 Grid of Problems -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Problem 1: Vanishing Gradients -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #8B0000;"><strong>1. Vanishing Gradients</strong></h5>
                            <p style="margin: 0 0 6px 0; font-size: 0.8em; line-height: 1.4;">Early layers don't update. Loss plateaus. Training collapses.</p>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px; margin-bottom: 4px;">
                                <strong>Root:</strong> Sigmoid/Tanh derivs ‚âà 0; Chain rule multiplies tiny values
                            </div>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px;">
                                <strong>Solutions:</strong> ‚úì‚úì‚úì ReLU, BatchNorm | ‚úì‚úì Init | ‚úì Residual
                            </div>
                        </div>

                        <!-- Problem 2: Exploding Gradients -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #8B0000;"><strong>2. Exploding Gradients</strong></h5>
                            <p style="margin: 0 0 6px 0; font-size: 0.8em; line-height: 1.4;">Loss becomes NaN. Weights ‚Üí ‚àû. Training crashes.</p>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px; margin-bottom: 4px;">
                                <strong>Root:</strong> Large weights; Chain rule multiplies large values
                            </div>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px;">
                                <strong>Solutions:</strong> ‚úì‚úì‚úì Gradient clipping | ‚úì‚úì BatchNorm | ‚úì Low LR
                            </div>
                        </div>

                        <!-- Problem 3: Overfitting -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>3. Overfitting</strong></h5>
                            <p style="margin: 0 0 6px 0; font-size: 0.8em; line-height: 1.4;">Training loss ‚Üì but Validation loss ‚Üë. Model memorizes noise.</p>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px; margin-bottom: 4px;">
                                <strong>Signs:</strong> Train/test gap growing; Test accuracy plateaus
                            </div>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px;">
                                <strong>Solutions:</strong> L1/L2, Dropout, Early stop, More data
                            </div>
                        </div>

                        <!-- Problem 4: Slow Convergence -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>4. Slow Convergence</strong></h5>
                            <p style="margin: 0 0 6px 0; font-size: 0.8em; line-height: 1.4;">Loss decreases slowly. Plateaus at suboptimal solution.</p>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px; margin-bottom: 4px;">
                                <strong>Signs:</strong> Training takes forever; Stuck at local minimum
                            </div>
                            <div style="font-size: 0.75em; background: white; padding: 6px; border-radius: 3px;">
                                <strong>Solutions:</strong> Higher LR (careful), Adam optimizer, Better init
                            </div>
                        </div>
                    </div>

                    <!-- Visualization Images -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 8px;">
                            <img src="/images/module12/gradient_magnitude_layers.png" alt="Gradient magnitude across layers" class="viz-image" style="max-height: 200px; margin: 0;">
                            <div style="font-size: 0.75em; text-align: center; color: #555; margin-top: 4px;">ReLU maintains gradient flow vs Sigmoid vanishing</div>
                        </div>
                        <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 8px;">
                            <img src="/images/module12/gradient_solutions.png" alt="Solutions effectiveness" class="viz-image" style="max-height: 200px; margin: 0;">
                            <div style="font-size: 0.75em; text-align: center; color: #555; margin-top: 4px;">Impact of ReLU, BatchNorm, Residual connections</div>
                        </div>
                    </div>

                    <!-- Debugging Checklist -->
                    <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 10px;">
                        <h5 style="margin: 0 0 6px 0; font-size: 0.9em;"><strong>Systematic Debugging Checklist</strong></h5>
                        <ol style="margin: 0; padding-left: 18px; font-size: 0.8em; line-height: 1.4;">
                            <li style="margin: 2px 0;">Loss curve: Decreasing? Increasing? Exploding? Constant?</li>
                            <li style="margin: 2px 0;">Gradients: In [10‚Åª‚Å∂, 10‚Åª¬π] range? Vanishing? Exploding?</li>
                            <li style="margin: 2px 0;">Activations: Reasonable? Not zeros? Not saturated?</li>
                            <li style="margin: 2px 0;">Implementation: Forward/backward correct? No sign errors?</li>
                            <li style="margin: 2px 0;">Hyperparameters: Learning rate, batch size, depth/width</li>
                        </ol>
                    </div>
                `
            },
            // Slide 42: Key Takeaways
            {
                title: "Neural Networks: Key Takeaways",
                content: `
                    <div class="three-column">
                        <div>
                            <div class="definition">
                                <h5>Core Concepts</h5>
                                <ul>
                                    <li>Perceptron: Basic unit</li>
                                    <li>Multi-layer: Complex mappings</li>
                                    <li>Activation: Non-linearity</li>
                                    <li>Forward prop: Predictions</li>
                                    <li>Backprop: Gradients</li>
                                    <li>Regularization: Prevent overfitting</li>
                                </ul>
                            </div>
                            <div class="definition" style="margin-top: 15px;">
                                <h5>Mathematical Foundation</h5>
                                <ul>
                                    <li>Matrix operations for efficiency</li>
                                    <li>Chain rule for gradients</li>
                                    <li>Optimization theory</li>
                                    <li>Probability theory</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="highlight">
                                <h5>Best Practices</h5>
                                <ul>
                                    <li><strong>Architecture:</strong> Start simple</li>
                                    <li><strong>Init:</strong> Xavier/He</li>
                                    <li><strong>Optimizer:</strong> Adam</li>
                                    <li><strong>Regularization:</strong> L2 + Dropout</li>
                                    <li><strong>Monitoring:</strong> Track metrics</li>
                                    <li><strong>Debugging:</strong> Systematic</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="highlight">
                                <h5>When to Use Neural Networks</h5>
                                <ul>
                                    <li>Large datasets available</li>
                                    <li>Complex non-linear patterns</li>
                                    <li>End-to-end learning desired</li>
                                    <li>Feature engineering difficult</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="highlight" style="margin-top: 20px;">
                        <strong>Foundation:</strong> These fundamentals scale to all modern deep learning architectures: CNNs, RNNs, Transformers, ResNets, etc.
                    </div>
                `
            },
            // Slide 43: Applications
            {
                title: "Applications & Architecture Comparison",
                content: `
                    <h4 style="margin-bottom: 10px;">Neural Networks Across Domains</h4>

                    <!-- Image -->
                    <div style="background: white; border: 1px solid #CCC; border-radius: 4px; padding: 8px; margin-bottom: 12px;">
                        <img src="/images/module12/architecture_comparison.png" alt="Architecture comparison" class="viz-image" style="max-height: 200px; margin: 0; width: 100%;">
                        <div style="font-size: 0.75em; text-align: center; color: #555; margin-top: 4px;">Parameters, FLOPs, memory, and inference time trade-offs</div>
                    </div>

                    <!-- Application Domains (2x2 Grid) -->
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-bottom: 12px;">
                        <!-- Computer Vision -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>üëÅÔ∏è Computer Vision</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <p style="margin: 0 0 4px 0;"><strong>Classification:</strong> ResNet, EfficientNet, ViT</p>
                                <p style="margin: 0 0 4px 0;"><strong>Detection:</strong> YOLO, Faster R-CNN</p>
                                <p style="margin: 0 0 4px 0;"><strong>Segmentation:</strong> U-Net, DeepLab</p>
                                <p style="margin: 0 0 4px 0;"><strong>Face & Medical:</strong> DeepFace, diagnosis</p>
                            </div>
                        </div>

                        <!-- NLP -->
                        <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>üìù NLP (Transformers)</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <p style="margin: 0 0 4px 0;"><strong>Language Models:</strong> GPT-4, BERT, Claude</p>
                                <p style="margin: 0 0 4px 0;"><strong>Translation:</strong> Google Translate, DeepL</p>
                                <p style="margin: 0 0 4px 0;"><strong>Conversational:</strong> ChatGPT, Alexa</p>
                                <p style="margin: 0 0 4px 0;"><strong>Analysis:</strong> Sentiment, summarization, Q&A</p>
                            </div>
                        </div>

                        <!-- Sequential Data -->
                        <div style="background: #E8F4F8; border-left: 4px solid #1B4332; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #1B4332;"><strong>üîä Sequential (RNN/LSTM)</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <p style="margin: 0 0 4px 0;"><strong>Speech:</strong> Recognition (Whisper), synthesis</p>
                                <p style="margin: 0 0 4px 0;"><strong>Recommendations:</strong> Netflix, Spotify, YouTube</p>
                                <p style="margin: 0 0 4px 0;"><strong>Time-Series:</strong> Stock, weather, energy</p>
                            </div>
                        </div>

                        <!-- Emerging -->
                        <div style="background: #FFE8E8; border-left: 4px solid #8B0000; border-radius: 4px; padding: 10px;">
                            <h5 style="margin: 0 0 6px 0; font-size: 0.9em; color: #8B0000;"><strong>üöÄ Emerging Frontiers</strong></h5>
                            <div style="font-size: 0.8em; line-height: 1.5;">
                                <p style="margin: 0 0 4px 0;"><strong>Generative:</strong> DALL-E, Stable Diffusion</p>
                                <p style="margin: 0 0 4px 0;"><strong>Multimodal:</strong> CLIP, GPT-4V</p>
                                <p style="margin: 0 0 4px 0;"><strong>RL & Science:</strong> AlphaFold, climate, robotics</p>
                            </div>
                        </div>
                    </div>

                    <!-- Architecture Selection Comparison -->
                    <div style="background: #FFF8E6; border-left: 4px solid #FFD700; border-radius: 4px; padding: 10px;">
                        <h5 style="margin: 0 0 8px 0; font-size: 0.9em;"><strong>Architecture Selection Guide</strong></h5>
                        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 12px; font-size: 0.8em;">
                            <!-- Row 1 -->
                            <div style="padding: 6px; background: white; border-radius: 3px;">
                                <strong>Image Classification</strong><br/>
                                ResNet, EfficientNet, ViT<br/>
                                <span style="color: #555;">25M-2B params | Hours-days</span>
                            </div>
                            <div style="padding: 6px; background: white; border-radius: 3px;">
                                <strong>Text Generation</strong><br/>
                                Transformer (GPT-style)<br/>
                                <span style="color: #555;">125M-175B params | Days-months</span>
                            </div>
                            <!-- Row 2 -->
                            <div style="padding: 6px; background: white; border-radius: 3px;">
                                <strong>Object Detection</strong><br/>
                                YOLO, Faster R-CNN<br/>
                                <span style="color: #555;">50M-300M params | Hours-days</span>
                            </div>
                            <div style="padding: 6px; background: white; border-radius: 3px;">
                                <strong>Speech Recognition</strong><br/>
                                RNN/LSTM, Conformer<br/>
                                <span style="color: #555;">10M-500M params | Hours-days</span>
                            </div>
                        </div>
                    </div>

                    <div class="insight-box" style="margin-top: 8px; padding: 10px;">
                        <strong style="font-size: 0.9em;">Impact:</strong> <span style="font-size: 0.85em;">Neural networks have revolutionized every domain‚Äîfrom creative generation (DALL-E) to scientific discovery (AlphaFold). These fundamental concepts scale to all modern architectures.</span>
                    </div>
                `
            },
            // Slide 44: Looking Forward
            {
                title: "Data Pipelines & Advanced Architectures",
                content: `
                    <h4>Professional neural network training requires efficient data handling and specialized architectures</h4>

                    <img src="/images/module12/mnist_pipeline.png" alt="Complete data pipeline" class="viz-image">
                    <p class="viz-caption">End-to-end pipeline: raw data ‚Üí preprocessing ‚Üí augmentation ‚Üí batching ‚Üí model ‚Üí evaluation</p>

                    <div class="image-grid-2">
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Specialized Architectures</h4>
                                <p><strong>Convolutional Neural Networks (CNNs)</strong></p>
                                <ul>
                                    <li>‚úì Spatial structure exploitation via local receptive fields</li>
                                    <li>‚úì Translation invariance through shared weights</li>
                                    <li>‚úì Computer vision: classification, detection, segmentation</li>
                                </ul>
                                <p><strong>Recurrent Neural Networks (RNNs)</strong></p>
                                <ul>
                                    <li>‚úì Sequential data processing with hidden state</li>
                                    <li>‚úì Memory and temporal dynamics</li>
                                    <li>‚úì LSTM, GRU variants prevent vanishing gradient</li>
                                </ul>
                                <p><strong>Transformer Networks</strong></p>
                                <ul>
                                    <li>‚úì Self-attention mechanisms for long-range dependencies</li>
                                    <li>‚úì Parallel processing (unlike RNNs)</li>
                                    <li>‚úì Modern NLP backbone (GPT, BERT, Claude)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="image-container">
                            <div class="dense-info-card">
                                <h4>Advanced Techniques</h4>
                                <p><strong>Batch Normalization</strong></p>
                                <ul>
                                    <li>Reduces internal covariate shift</li>
                                    <li>Accelerates training 5-10x</li>
                                    <li>Reduces sensitivity to initialization</li>
                                </ul>
                                <p><strong>Residual Connections (ResNets)</strong></p>
                                <ul>
                                    <li>Skip connections enable 100+ layer networks</li>
                                    <li>Dramatically improves gradient flow</li>
                                    <li>Foundation of modern deep architectures</li>
                                </ul>
                                <p><strong>Attention Mechanisms</strong></p>
                                <ul>
                                    <li>Selective focus on relevant information</li>
                                    <li>Solves long-range dependency problem</li>
                                    <li>Powers Transformers and modern LLMs</li>
                                </ul>
                                <p><strong>Generative Models</strong></p>
                                <ul>
                                    <li>VAEs: Variational Autoencoders</li>
                                    <li>GANs: Generative Adversarial Networks</li>
                                    <li>Diffusion: Current state-of-the-art generation</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="highlight" style="margin-top: 20px;">
                        <strong>Next Steps:</strong> Implement these architectures using PyTorch or TensorFlow, experiment with real datasets, and apply them to domain-specific problems.
                    </div>
                `
            },
            // Slide 45: Deep Learning Frameworks & Tools
            {
                title: "Neural Network Training Tools & Frameworks",
                content: `
                    <h4>Popular Deep Learning Frameworks</h4>
                    <div class="three-column">
                        <div>
                            <div class="definition">
                                <h5>PyTorch</h5>
                                <p><strong>Strengths:</strong></p>
                                <ul>
                                    <li>Dynamic computational graphs</li>
                                    <li>Pythonic and intuitive API</li>
                                    <li>Excellent for research</li>
                                    <li>Strong GPU support (CUDA)</li>
                                    <li>Active research community</li>
                                </ul>
                                <p style="font-size: 0.9em;"><strong>Use Case:</strong> Research, experimentation, models</p>
                                <p style="font-size: 0.9em;"><strong>Language:</strong> Python</p>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>TensorFlow/Keras</h5>
                                <p><strong>Strengths:</strong></p>
                                <ul>
                                    <li>Production-ready and scalable</li>
                                    <li>Keras: high-level user-friendly API</li>
                                    <li>Comprehensive tools ecosystem</li>
                                    <li>Google backing (TPU support)</li>
                                    <li>Mobile/edge deployment</li>
                                </ul>
                                <p style="font-size: 0.9em;"><strong>Use Case:</strong> Production, enterprises, scale</p>
                                <p style="font-size: 0.9em;"><strong>Language:</strong> Python</p>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Other Frameworks</h5>
                                <p><strong>JAX:</strong> NumPy-like with autodiff, high-perf computing</p>
                                <p><strong>MXNet:</strong> Multi-language, efficient, distributed</p>
                                <p><strong>Caffe2:</strong> Mobile deployment, lightweight inference</p>
                                <p><strong>ONNX:</strong> Framework interoperability standard</p>
                            </div>
                        </div>
                    </div>
                    <div class="highlight" style="margin-top: 15px;">
                        <h5>Quick Comparison</h5>
                        <div style="font-size: 0.9em;">
                            <p><strong>PyTorch:</strong> Moderate learning curve, excellent for research, very production-ready</p>
                            <p><strong>TensorFlow:</strong> Steeper learning curve, best for production at scale, Google ecosystem</p>
                            <p><strong>Keras:</strong> Easiest learning curve, great for prototyping, now integrated with TensorFlow</p>
                            <p><strong>JAX:</strong> For scientific computing and functional programming approaches</p>
                        </div>
                    </div>
                    <h4 style="margin-top: 15px;">Essential Supporting Tools</h4>
                    <div class="two-column">
                        <div class="two-column-left">
                            <div class="highlight">
                                <h5>Development & Training</h5>
                                <ul>
                                    <li><strong>Jupyter:</strong> Interactive notebooks</li>
                                    <li><strong>Google Colab:</strong> Free GPU/TPU access</li>
                                    <li><strong>Weights & Biases:</strong> Experiment tracking</li>
                                    <li><strong>TensorBoard:</strong> Visualization</li>
                                    <li><strong>Hydra:</strong> Config management</li>
                                </ul>
                            </div>
                        </div>
                        <div class="two-column-right">
                            <div class="highlight">
                                <h5>Deployment & Optimization</h5>
                                <ul>
                                    <li><strong>ONNX:</strong> Model conversion</li>
                                    <li><strong>TVM:</strong> Inference optimization</li>
                                    <li><strong>Docker:</strong> Containerization</li>
                                    <li><strong>Kubernetes:</strong> Orchestration</li>
                                    <li><strong>FastAPI:</strong> REST API serving</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="definition" style="margin-top: 15px;">
                        <h5>Getting Started Path</h5>
                        <p><strong>For Learning:</strong> Start with Keras or PyTorch. Use Google Colab for free GPU.</p>
                        <p><strong>For Research:</strong> PyTorch is dominant in academic papers and cutting-edge models (GPT, BERT, Vision Transformers).</p>
                        <p><strong>For Production:</strong> TensorFlow with TensorFlow Serving, or PyTorch with FastAPI + Docker. Track experiments with W&B.</p>
                    </div>
                `
            },
            // Slide 46: Interactive TensorFlow Playground
            {
                title: "Interactive Learning: TensorFlow Playground",
                content: `
                    <div class="highlight">
                        <h4>üéÆ Visualize Neural Networks in Real-Time</h4>
                        <p style="font-size: 0.95em; margin: 10px 0;">TensorFlow Playground is an interactive web tool that lets you design, train, and visualize neural networks without writing code!</p>
                    </div>

                    <div class="two-column" style="margin-top: 15px;">
                        <div>
                            <div class="definition">
                                <h5>What You Can Explore</h5>
                                <ul style="font-size: 0.9em;">
                                    <li><strong>Network Architecture:</strong> Add/remove hidden layers and neurons</li>
                                    <li><strong>Activation Functions:</strong> Switch between ReLU, Tanh, Sigmoid</li>
                                    <li><strong>Learning Rate:</strong> Control training speed</li>
                                    <li><strong>Datasets:</strong> Circle, XOR, Gaussian, Spiral</li>
                                    <li><strong>Regularization:</strong> L1, L2, and Dropout</li>
                                    <li><strong>Real-time Training:</strong> Watch loss decrease</li>
                                </ul>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Key Insights from Playground</h5>
                                <ul style="font-size: 0.9em;">
                                    <li><strong>Overfitting:</strong> See how deep networks memorize vs. generalize</li>
                                    <li><strong>Hidden Layers:</strong> Understand how layers combine features</li>
                                    <li><strong>Activation Impact:</strong> Observe non-linearity in action</li>
                                    <li><strong>Complexity:</strong> Experiment with network size vs. dataset difficulty</li>
                                    <li><strong>Regularization:</strong> Watch how L1/L2/Dropout prevent overfitting</li>
                                    <li><strong>Decision Boundaries:</strong> See how networks separate data classes</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="highlight" style="margin-top: 15px;">
                        <h5>üîó Access the Playground</h5>
                        <p style="font-size: 1em; color: #1B4332;"><strong>URL:</strong> <code style="background: #f0f0f0; padding: 3px 6px; border-radius: 3px;">https://playground.tensorflow.org/</code></p>
                        <p style="font-size: 0.9em; margin-top: 8px; color: #555;">üëâ <strong>Recommended Activity:</strong> Try the Spiral dataset with different activation functions to see how ReLU, Tanh, and Sigmoid affect the decision boundary!</p>
                    </div>

                    <div class="definition" style="margin-top: 12px;">
                        <h5>Learning Path</h5>
                        <p style="font-size: 0.9em;"><strong>1. Start Simple:</strong> Circle dataset with 1 hidden layer ‚Üí understand basic classification</p>
                        <p style="font-size: 0.9em;"><strong>2. Add Complexity:</strong> XOR dataset shows why we need hidden layers</p>
                        <p style="font-size: 0.9em;"><strong>3. Experiment:</strong> Spiral dataset shows limits of shallow networks</p>
                        <p style="font-size: 0.9em;"><strong>4. Advanced:</strong> Adjust regularization, batch size, and activation functions</p>
                    </div>
                `
            },
            // Slide 47: Multi-Layer Perceptron (MLP) Architecture
            {
                title: "Multi-Layer Perceptron (MLP) Architecture",
                content: `
                    <div style="display: grid; grid-template-columns: 1fr 1.2fr; gap: 20px; align-items: start;">
                        <div>
                            <div class="definition">
                                <h5>What is MLP?</h5>
                                <p style="font-size: 0.85em;"><strong>Other names:</strong></p>
                                <ul style="font-size: 0.85em; margin-left: 15px;">
                                    <li>Feedforward Neural Network (FFNN)</li>
                                    <li>Backpropagation Neural Network (BPNN)</li>
                                    <li>Fully-connected NN (FCNN)</li>
                                </ul>
                            </div>
                            <div class="definition" style="margin-top: 12px;">
                                <h5>Key Characteristics</h5>
                                <p style="font-size: 0.85em;">‚úì Each neuron connected to all neurons in next layer</p>
                                <p style="font-size: 0.85em;">‚úì Data flows in one direction (input ‚Üí output)</p>
                                <p style="font-size: 0.85em;">‚úì Trained with backpropagation algorithm</p>
                                <p style="font-size: 0.85em;">‚úì Most fundamental deep learning architecture</p>
                            </div>
                            <div class="highlight" style="margin-top: 12px;">
                                <h5>üíª PyTorch Implementation</h5>
                                <pre style="font-size: 0.75em; background: #f5f5f5; padding: 8px; border-radius: 4px; overflow-x: auto;"><code>import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_size=784, hidden_size=128, num_classes=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Usage
model = MLP(784, 128, 10)
x = torch.randn(32, 784)  # Batch of 32
y = model(x)  # Output: [32, 10]</code></pre>
                            </div>
                        </div>
                        <div>
                            <svg viewBox="0 0 500 350" style="background: white; border-radius: 6px; border: 1px solid #ddd;">
                                <!-- Layer labels -->
                                <text x="50" y="320" font-size="12" fill="#8B0000" font-weight="bold">Input Layer</text>
                                <text x="200" y="320" font-size="12" fill="#1B4332" font-weight="bold">Hidden Layers</text>
                                <text x="430" y="320" font-size="12" fill="#2E7D32" font-weight="bold">Output</text>

                                <!-- INPUT LAYER (x1, x2) -->
                                <circle cx="50" cy="80" r="18" fill="#FFB3BA" stroke="#8B0000" stroke-width="2"/>
                                <text x="50" y="85" text-anchor="middle" font-size="12" fill="#000" font-weight="bold">x‚ÇÅ</text>

                                <circle cx="50" cy="180" r="18" fill="#FFB3BA" stroke="#8B0000" stroke-width="2"/>
                                <text x="50" y="185" text-anchor="middle" font-size="12" fill="#000" font-weight="bold">x‚ÇÇ</text>

                                <!-- HIDDEN LAYER 1 (3 neurons) -->
                                <circle cx="200" cy="60" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="200" y="65" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÅ</text>

                                <circle cx="200" cy="130" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="200" y="135" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÇ</text>

                                <circle cx="200" cy="200" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="200" y="205" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÉ</text>

                                <!-- HIDDEN LAYER 2 (3 neurons) -->
                                <circle cx="340" cy="60" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="340" y="65" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÑ</text>

                                <circle cx="340" cy="130" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="340" y="135" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÖ</text>

                                <circle cx="340" cy="200" r="18" fill="#C8E6C9" stroke="#1B4332" stroke-width="2"/>
                                <text x="340" y="205" text-anchor="middle" font-size="11" fill="#000" font-weight="bold">h‚ÇÜ</text>

                                <!-- OUTPUT LAYER -->
                                <circle cx="450" cy="130" r="18" fill="#B3E5FC" stroke="#2E7D32" stroke-width="2"/>
                                <text x="450" y="140" text-anchor="middle" font-size="12" fill="#000" font-weight="bold">f(x)</text>

                                <!-- CONNECTIONS Input to Hidden1 -->
                                <line x1="68" y1="75" x2="182" y2="60" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="68" y1="85" x2="182" y2="75" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="68" y1="175" x2="182" y2="125" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="68" y1="185" x2="182" y2="140" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="68" y1="80" x2="182" y2="200" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="68" y1="180" x2="182" y2="200" stroke="#999" stroke-width="1" opacity="0.6"/>

                                <!-- CONNECTIONS Hidden1 to Hidden2 -->
                                <line x1="218" y1="60" x2="322" y2="60" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="218" y1="130" x2="322" y2="130" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="218" y1="200" x2="322" y2="200" stroke="#999" stroke-width="1" opacity="0.6"/>
                                <line x1="218" y1="60" x2="322" y2="130" stroke="#999" stroke-width="1" opacity="0.4"/>
                                <line x1="218" y1="130" x2="322" y2="60" stroke="#999" stroke-width="1" opacity="0.4"/>
                                <line x1="218" y1="130" x2="322" y2="200" stroke="#999" stroke-width="1" opacity="0.4"/>

                                <!-- CONNECTIONS Hidden2 to Output -->
                                <line x1="358" y1="60" x2="432" y2="130" stroke="#2E7D32" stroke-width="2" opacity="0.7"/>
                                <line x1="358" y1="130" x2="432" y2="130" stroke="#2E7D32" stroke-width="2" opacity="0.7"/>
                                <line x1="358" y1="200" x2="432" y2="130" stroke="#2E7D32" stroke-width="2" opacity="0.7"/>

                                <!-- Neuron detail box -->
                                <rect x="240" y="240" width="220" height="60" fill="#FFF9E6" stroke="#FFD700" stroke-width="2" rx="4"/>
                                <text x="350" y="260" text-anchor="middle" font-size="11" fill="#8B0000" font-weight="bold">Activation Function: œÉ</text>
                                <text x="350" y="278" text-anchor="middle" font-size="10" fill="#333">a(x) = œÉ(Wx + b)</text>
                                <text x="350" y="293" text-anchor="middle" font-size="9" fill="#555" font-style="italic">Linear combination + Non-linearity</text>
                            </svg>
                        </div>
                    </div>
                `
            },
            // Slide 48: Activation Functions Interactive
            {
                title: "Activation Functions & Training Code",
                content: `
                    <p style="font-size: 0.9em; color: #555; margin-bottom: 12px;"><strong>Comparing activation functions and training a complete model:</strong></p>
                    <div class="two-column">
                        <div>
                            <div class="definition">
                                <h5>ReLU (Rectified Linear Unit)</h5>
                                <p style="font-size: 0.85em;">œÉ(z) = max(0, z)</p>
                                <p style="font-size: 0.85em; color: #555;"><strong>‚úì Fast computation</strong></p>
                                <p style="font-size: 0.85em; color: #555;"><strong>‚úì Prevents vanishing gradients</strong></p>
                                <p style="font-size: 0.85em; color: #8B0000;"><strong>‚úó Dead ReLU problem</strong></p>
                            </div>
                            <div class="definition" style="margin-top: 10px;">
                                <h5>Sigmoid</h5>
                                <p style="font-size: 0.85em;">œÉ(z) = 1/(1 + e^(-z))</p>
                                <p style="font-size: 0.85em; color: #555;"><strong>‚úì Output in [0,1]</strong></p>
                                <p style="font-size: 0.85em; color: #555;"><strong>‚úì Probabilistic interpretation</strong></p>
                                <p style="font-size: 0.85em; color: #8B0000;"><strong>‚úó Vanishing gradients</strong></p>
                            </div>
                        </div>
                        <div>
                            <div class="definition">
                                <h5>Training Code Example</h5>
                                <pre style="font-size: 0.7em; background: #f5f5f5; padding: 8px; border-radius: 4px; overflow-x: auto;"><code># PyTorch training loop
import torch
import torch.nn.functional as F

# Forward pass
logits = model(x_batch)  # [32, 10]

# Compute loss
loss = F.cross_entropy(logits, y_batch)

# Backward pass
optimizer.zero_grad()
loss.backward()
optimizer.step()

# Evaluate
with torch.no_grad():
    preds = logits.argmax(dim=1)
    accuracy = (preds == y_batch).float().mean()
print(f"Loss: {loss:.4f}, Acc: {accuracy:.2%}")</code></pre>
                            </div>
                        </div>
                    </div>
                    <div class="highlight" style="margin-top: 12px; font-size: 0.85em;">
                        <p><strong>Rule of thumb:</strong> Use ReLU for hidden layers, Sigmoid for binary output, Softmax for multi-class output</p>
                    </div>
                `
            },
            // Slide 49: Forward Pass Flow
            {
                title: "Forward Propagation Flow",
                content: `
                    <p style="font-size: 0.9em; color: #555; margin-bottom: 12px;"><strong>How data flows through the network during training:</strong></p>
                    <div class="mermaid" style="background: white; padding: 15px; border-radius: 6px;">
graph TD
    A["Input: x (Training Data)"] --> B["Layer 1: z‚ÅΩ¬π‚Åæ = W‚ÅΩ¬π‚Åæ¬∑x + b‚ÅΩ¬π‚Åæ"]
    B --> C["Activation: a‚ÅΩ¬π‚Åæ = œÉ(z‚ÅΩ¬π‚Åæ)"]
    C --> D["Layer 2: z‚ÅΩ¬≤‚Åæ = W‚ÅΩ¬≤‚Åæ¬∑a‚ÅΩ¬π‚Åæ + b‚ÅΩ¬≤‚Åæ"]
    D --> E["Activation: a‚ÅΩ¬≤‚Åæ = œÉ(z‚ÅΩ¬≤‚Åæ)"]
    E --> F["Output Layer: ≈∑ = softmax(z‚ÅΩ¬≤‚Åæ)"]
    F --> G["Loss: L = CrossEntropy(y, ≈∑)"]
    G --> H["Backpropagation"]
    H --> I["Gradient Computation"]
    I --> J["Parameter Update: Œ∏ := Œ∏ - Œ±‚àáL"]

    style A fill:#FFD700,stroke:#1B4332,stroke-width:2px,color:#000
    style G fill:#FFEBEE,stroke:#8B0000,stroke-width:2px,color:#000
    style H fill:#E8F5E9,stroke:#1B4332,stroke-width:2px,color:#000
    style J fill:#E8F5E9,stroke:#1B4332,stroke-width:2px,color:#000
                    </div>
                    <div class="definition" style="margin-top: 12px; font-size: 0.85em;">
                        <p><strong>Key components:</strong></p>
                        <p>‚Ä¢ Forward pass: input ‚Üí predictions (deterministic)</p>
                        <p>‚Ä¢ Loss computation: how wrong are predictions?</p>
                        <p>‚Ä¢ Backward pass: compute gradients for all parameters</p>
                        <p>‚Ä¢ Update: adjust weights to minimize loss</p>
                    </div>
                `
            },
            // Slide 50: Gradient Descent Visualization
            {
                title: "Gradient Descent: Optimization Journey",
                content: `
                    <p style="font-size: 0.9em; color: #555; margin-bottom: 12px;"><strong>Visualizing the optimization landscape and gradient descent path:</strong></p>
                    <div class="mermaid" style="background: white; padding: 15px; border-radius: 6px;">
graph LR
    A["Start: Random Weights"] --> B["Epoch 1: Loss = 2.45"]
    B --> C["Epoch 5: Loss = 1.82"]
    C --> D["Epoch 10: Loss = 1.15"]
    D --> E["Epoch 20: Loss = 0.45"]
    E --> F["Epoch 50: Loss = 0.12"]
    F --> G["Converged: Loss ‚âà 0.05"]

    style A fill:#FFEBEE,stroke:#8B0000,stroke-width:2px,color:#000
    style B fill:#FFE082,stroke:#FF6F00,stroke-width:2px,color:#000
    style C fill:#FFF59D,stroke:#FBC02D,stroke-width:2px,color:#000
    style D fill:#F1F8E9,stroke:#558B2F,stroke-width:2px,color:#000
    style E fill:#E8F5E9,stroke:#2E7D32,stroke-width:2px,color:#000
    style G fill:#C8E6C9,stroke:#1B5E20,stroke-width:2px,color:#000
                    </div>
                    <div class="two-column" style="margin-top: 12px;">
                        <div class="highlight">
                            <h5>Key Concepts</h5>
                            <p style="font-size: 0.85em;"><strong>Learning Rate (Œ±):</strong> Controls step size</p>
                            <p style="font-size: 0.85em;"><strong>Gradient (‚àáL):</strong> Direction of steepest descent</p>
                            <p style="font-size: 0.85em;"><strong>Iteration:</strong> Repeat until convergence</p>
                        </div>
                        <div class="definition">
                            <h5>Challenges</h5>
                            <p style="font-size: 0.85em;"><strong>Local Minima:</strong> Get stuck in suboptimal solution</p>
                            <p style="font-size: 0.85em;"><strong>Plateau:</strong> Slow progress in flat regions</p>
                            <p style="font-size: 0.85em;"><strong>Overshooting:</strong> Learning rate too large</p>
                        </div>
                    </div>
                `
            },
            // Slide 51: Advanced Architectures
            {
                title: "Advanced Neural Network Architectures & Implementation",
                content: `
                    <p style="font-size: 0.9em; color: #555; margin-bottom: 12px;"><strong>Specialized architectures for different tasks with PyTorch implementations:</strong></p>
                    <div class="three-column">
                        <div class="definition">
                            <h5>CNN (Convolutional NN)</h5>
                            <p style="font-size: 0.85em;"><strong>Purpose:</strong> Image classification, detection</p>
                            <p style="font-size: 0.85em;"><strong>Key layers:</strong> Conv2d, Pooling, FC</p>
                            <p style="font-size: 0.85em;"><strong>Advantage:</strong> Spatial feature extraction</p>
                            <pre style="font-size: 0.7em; background: #f5f5f5; padding: 6px; border-radius: 4px; margin-top: 6px;"><code>class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(32*13*13, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x</code></pre>
                        </div>
                        <div class="definition">
                            <h5>RNN (Recurrent NN)</h5>
                            <p style="font-size: 0.85em;"><strong>Purpose:</strong> Sequence modeling, NLP</p>
                            <p style="font-size: 0.85em;"><strong>Key feature:</strong> Hidden state carries temporal info</p>
                            <p style="font-size: 0.85em;"><strong>Advantage:</strong> Processes sequences</p>
                            <pre style="font-size: 0.7em; background: #f5f5f5; padding: 6px; border-radius: 4px; margin-top: 6px;"><code>class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # x: [batch, seq_len, input_size]
        _, (h_n, _) = self.lstm(x)
        x = h_n[-1]  # Last hidden state
        x = self.fc(x)
        return x</code></pre>
                        </div>
                        <div class="definition">
                            <h5>Autoencoders</h5>
                            <p style="font-size: 0.85em;"><strong>Purpose:</strong> Dimensionality reduction</p>
                            <p style="font-size: 0.85em;"><strong>Structure:</strong> Encoder ‚Üí Code ‚Üí Decoder</p>
                            <p style="font-size: 0.85em;"><strong>Advantage:</strong> Unsupervised learning</p>
                            <pre style="font-size: 0.7em; background: #f5f5f5; padding: 6px; border-radius: 4px; margin-top: 6px;"><code>class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 32)  # Bottleneck
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, 256),
            nn.ReLU(),
            nn.Linear(256, 784)
        )

    def forward(self, x):
        code = self.encoder(x)
        recon = self.decoder(code)
        return recon</code></pre>
                        </div>
                    </div>
                `
            },
            // Slide 52: Closing
            {
                title: "Questions?",
                content: `
                    <div style="text-align: center; margin-top: 100px;">
                        <h2 style="font-size: 3em; color: #1B4332; border: none;">Questions?</h2>
                        <p style="font-size: 1.3em; color: #555; margin-top: 40px;">Thank you for your attention!</p>
                        <div style="margin-top: 60px;">
                            <p style="font-size: 1.1em; color: #8B0000;"><strong>CMSC 173: Machine Learning</strong></p>
                            <p style="font-size: 1em; color: #555; margin-top: 10px;">University of the Philippines - Cebu</p>
                        </div>
                    </div>
                `
            }
        ];

        let currentSlideIndex = 0;

        // Function to convert plain URLs in text to clickable links
        function makeURLsClickable(element) {
            if (!element) return;

            // URL regex pattern
            const urlRegex = /(https?:\/\/[^\s<>"']+)/gi;

            // Process all text nodes in the element
            const walker = document.createTreeWalker(
                element,
                NodeFilter.SHOW_TEXT,
                null,
                false
            );

            const nodesToReplace = [];
            let node;

            while (node = walker.nextNode()) {
                if (urlRegex.test(node.nodeValue)) {
                    nodesToReplace.push(node);
                }
            }

            // Replace text nodes with links
            nodesToReplace.forEach(textNode => {
                const span = document.createElement('span');
                span.innerHTML = textNode.nodeValue.replace(
                    /(https?:\/\/[^\s<>"']+)/gi,
                    '<a href="$1" target="_blank" rel="noopener noreferrer">$1</a>'
                );
                textNode.parentNode.replaceChild(span, textNode);
            });
        }

        function renderSlide() {
            const slide = slides[currentSlideIndex];
            const contentDiv = document.getElementById('slide-content');

            contentDiv.innerHTML = `
                <h2>${slide.title}</h2>
                ${slide.content}
            `;

            // Re-initialize Mermaid for any diagrams
            mermaid.contentLoaded();

            // Re-render MathJax equations for new content
            MathJax.typesetPromise([contentDiv]).catch(err => console.log(err));

            // Make URLs in content clickable
            makeURLsClickable(contentDiv);

            // Update slide input and counter
            document.getElementById('slide-input').value = currentSlideIndex + 1;
            document.getElementById('slide-input').max = slides.length;
            document.getElementById('total-slides').textContent = slides.length;

            // Update button states
            document.getElementById('prev-btn').disabled = currentSlideIndex === 0;
            document.getElementById('next-btn').disabled = currentSlideIndex === slides.length - 1;

            // Scroll to top of slide content
            contentDiv.scrollTop = 0;
        }

        function nextSlide() {
            if (currentSlideIndex < slides.length - 1) {
                currentSlideIndex++;
                renderSlide();
            }
        }

        function previousSlide() {
            if (currentSlideIndex > 0) {
                currentSlideIndex--;
                renderSlide();
            }
        }

        function goToSlide() {
            const input = document.getElementById('slide-input');
            let slideNum = parseInt(input.value);

            // Validate input
            if (isNaN(slideNum) || slideNum < 1) {
                slideNum = 1;
            } else if (slideNum > slides.length) {
                slideNum = slides.length;
            }

            currentSlideIndex = slideNum - 1;
            renderSlide();
        }

        function handleSlideInputKey(event) {
            if (event.key === 'Enter') {
                goToSlide();
                document.getElementById('slide-input').blur();
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight') nextSlide();
            if (e.key === 'ArrowLeft') previousSlide();
        });

        // Initialize
        document.addEventListener('DOMContentLoaded', () => {
            // Set total slides count immediately
            document.getElementById('total-slides').textContent = slides.length;
            // Update slide input max value
            document.getElementById('slide-input').max = slides.length;
            // Render first slide
            renderSlide();
        });
    </script>
</body>
</html>
